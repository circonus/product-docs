[{"title":"Graphite (Pickle)","logo":{"light":"/img/library/graphite.png"},"attributes":{"premium":false,"implementation":"broker","legacy":false,"deprecated":false},"tags":[],"module":"graphite_pickle","no_listing":true},{"title":"Graphite (Plain)","logo":{"light":"/img/library/graphite.png"},"attributes":{"premium":false,"implementation":"broker","legacy":false,"deprecated":false},"tags":[],"module":"graphite_plain","no_listing":true},{"title":"Graphite","logo":{"light":"/img/library/graphite.png"},"attributes":{"premium":false,"implementation":"broker","legacy":false,"deprecated":false},"tags":[],"module":"graphite_tls","no_listing":true},{"title":"StatsD UDP","logo":{"light":"/img/library/statsd.png"},"attributes":{"premium":false,"implementation":"broker","legacy":true,"deprecated":false},"tags":[],"module":"statsd","no_listing":true},{"title":"StatsD TCP","logo":{"light":"/img/library/statsd.png"},"attributes":{"premium":false,"implementation":"broker","legacy":true,"deprecated":false},"tags":[],"module":"statsd_tcp","no_listing":true},{"title":"Resmon","logo":{"light":"/img/library/resmon.png"},"attributes":{"premium":false,"implementation":"broker","legacy":true,"deprecated":false},"tags":[],"module":"resmon","no_listing":true},{"title":"MacOS/Darwin","attributes":{"premium":true,"implementation":"cua","legacy":false,"deprecated":false},"tags":[],"module":"httptrap:cua:host:darwin","no_listing":true},{"title":"FreeBSD","attributes":{"premium":true,"implementation":"cua","legacy":false,"deprecated":false},"tags":[],"module":"httptrap:cua:host:freebsd"},{"title":"Linux","attributes":{"premium":true,"implementation":"","legacy":false,"deprecated":false},"tags":[],"module":"httptrap:cua:host:linux"},{"title":"Windows","attributes":{"premium":true,"implementation":"","legacy":false,"deprecated":false},"tags":[],"module":"httptrap:cua:host:windows"},{"name":"aerospike","title":"Aerospike","content":"# Aerospike\n\n## Overview\n\nThe aerospike plugin queries aerospike server(s) and gets node statistics and stats for all the configured namespaces.\n\nFor what the measurements mean, please consult the [Aerospike Metrics Reference Docs](http://www.aerospike.com/docs/reference/metrics).\n\nThe metric names, to make it less complicated in querying, have replaced all `-` with `_` as Aerospike metrics come in both forms (no idea why).\n\nAll metrics are attempted to be cast to integers, then booleans, then strings.\n\n## Configuration\n\n```toml\n# Read stats from aerospike server(s)\n[[inputs.aerospike]]\n  ## Aerospike servers to connect to (with port)\n  ## This plugin will query all namespaces the aerospike\n  ## server has configured and get stats for them.\n  servers = [\"localhost:3000\"]\n\n  # username = \"circonus\"\n  # password = \"pa$$word\"\n\n  ## Optional TLS Config\n  # enable_tls = false\n  # tls_ca = \"/etc/circonus-unified-agent/ca.pem\"\n  # tls_cert = \"/etc/circonus-unified-agent/cert.pem\"\n  # tls_key = \"/etc/circonus-unified-agent/key.pem\"\n  ## If false, skip chain & host verification\n  # insecure_skip_verify = true\n\n  # Feature Options\n  # Add namespace variable to limit the namespaces executed on\n  # Leave blank to do all\n  # disable_query_namespaces = true # default false\n  # namespaces = [\"namespace1\", \"namespace2\"]\n\n  # Enable set level telmetry\n  # query_sets = true # default: false\n  # Add namespace set combinations to limit sets executed on\n  # Leave blank to do all\n  # sets = [\"namespace1/set1\", \"namespace1/set2\"]\n  # sets = [\"namespace1/set1\", \"namespace1/set2\", \"namespace3\"]\n\n  # Histograms\n  # enable_ttl_histogram = true # default: false\n  # enable_object_size_linear_histogram = true # default: false\n\n  # by default, aerospike produces a 100 bucket histogram\n  # this is not great for most graphing tools, this will allow\n  # the ability to squash this to a smaller number of buckets\n  # To have a balanced histogram, the number of buckets chosen\n  # should divide evenly into 100.\n  # num_histogram_buckets = 100 # default: 10\n```\n","logo":{"light":"/img/library/aerospike.svg"},"attributes":{}},{"name":"amazon-cloudwatch","title":"Amazon CloudWatch","content":"# Amazon CloudWatch\n\n## Overview\n\nThe CloudWatch Check monitors your Amazon Web Services (AWS) cloud infrastructure using CloudWatch, providing metrics related to your AWS resources and the AWS applications you run on AWS.\n\nFrom the CloudWatch Check you can set alerts within Circonus to send notifications allowing you to make changes to the resources within AWS. For example, you can monitor the CPU usage and disk reads and writes of your Amazon Elastic Compute Cloud (Amazon EC2) instances and then use this data to determine whether you should launch additional instances to handle increased load. You can also use this data to stop under-used instances. With the CloudWatch Check, you gain system-wide visibility into resource utilization, application performance, and operational health.\n\nCirconus takes the AWS Region, API Key, and API Secret, then polls the endpoint AWS for a list of all available Namespaces, Metrics, and Dimensions that are specific to the user (Region, API Key, and API Secret combination). Only those returned are displayed in the fields. The names that are displayed under each Dimension type (for example: Volume for EBS) are all instances running this Dimension type that have detailed monitoring enabled.\nMetrics\n\n## Advanced\n\n### CloudWatch Namespaces\n\nWithin AWS you can create CloudWatch namespaces. Namespaces are containers for metrics. Metrics in different namespaces are isolated from each other, so that metrics from different applications are not mistakenly aggregated into the same statistics.\n\nNamespace names are strings you define when you create a metric. The names must be valid XML characters, typically containing alphanumeric characters, plus the characters period (.), hyphen (-), underscore (\\_), slash (/), hash or pound (#), and colon (:). AWS namespaces all follow the naming convention \"`AWS/<service>`\", for example \"`AWS/EC2`\" and \"`AWS/ELB`\".\n\nThere is no default namespace; you must specify a namespace for each data element you put into CloudWatch.\n\nSelecting a specific namespace will reduce the amount of time and API queries needed to get a list of available metrics.\n\n### CloudWatch Metrics\n\nA metric represents a time-ordered set of data points and you retrieve statistics about those data points as an ordered set of time- series data. Think of a metric as a variable to monitor, and the data points represent the values of that variable over time. For example, the CPU usage of a particular Amazon EC2 instance is one metric, and the latency of an Elastic Load Balancing load balancer is another. The data points themselves can come from any application or business activity from which you collect data. Metrics are uniquely defined by a name, a namespace, and one or more dimensions.\n\nDepending on the Namespace selected, the Metrics displayed may be different when creating a new CloudWatch check.\n\nSelecting a specific metric name will reduce the amount of time and API queries needed to get a list of available metrics. To get all metrics, either enter \"All\" in to the \"Metric Name to List\" field, or leave the field blank.\n\n### CloudWatch Dimensions\n\nA dimension is a name/value pair that helps you to uniquely identify a CloudWatch metric. Every metric has specific characteristics that describe it, and you can think of dimensions as categories for those characteristics. Dimensions help you design a structure for your statistics plan. Because dimensions are part of the unique identifier for a metric, whenever you add a unique name/value pair to one of your metrics, you are creating a new metric.\n\nDepending on the Namespace and metric(s) selected, the Dimensions displayed may be different when creating a new CloudWatch check.\n\nCirconus will create one check for each unique combination of dimensions across categories. For example, if you select five dimensions in two different categories, Circonus will create six checks. Likewise, when editing a CloudWatch check, you will be limited to one dimension per category. If no dimensions are checked, the query will be made with no associated dimension data.\n","logo":{"light":"/img/library/amazon-cloudwatch.svg"},"attributes":{}},{"name":"amqp","title":"AMQP","content":"# AMQP\n\n## Overview\n\nThis plugin provides a consumer for use with AMQP 0-9-1, a prominent implementation of this protocol being [RabbitMQ](https://www.rabbitmq.com/).\n\nMetrics are read from a topic exchange using the configured queue and binding_key.\n\nMessage payload should be formatted in one of the [Data Formats](https://github.com/circonus-labs/circonus-unified-agent/blob/master/docs/DATA_FORMATS_INPUT.md).\n\nFor an introduction to AMQP see:\n\n- https://www.rabbitmq.com/tutorials/amqp-concepts.html\n- https://www.rabbitmq.com/getstarted.html\n\n## Configuration\n\nThe following defaults are known to work with RabbitMQ:\n\n```toml\n[[inputs.amqp_consumer]]\n  ## AMQP Brokers to consume from.  If multiple brokers are specified a random broker\n  ## will be selected anytime a connection is established.  This can be\n  ## helpful for load balancing when not using a dedicated load balancer.\n  brokers = [\"amqp://localhost:5672/circonus\"]\n\n  ## Authentication credentials for the PLAIN auth_method.\n  # username = \"\"\n  # password = \"\"\n\n  ## Name of the exchange to declare.  If unset, no exchange will be declared.\n  exchange = \"circonus\"\n\n  ## Exchange type; common types are \"direct\", \"fanout\", \"topic\", \"header\", \"x-consistent-hash\".\n  # exchange_type = \"topic\"\n\n  ## If true, exchange will be passively declared.\n  # exchange_passive = false\n\n  ## Exchange durability can be either \"transient\" or \"durable\".\n  # exchange_durability = \"durable\"\n\n  ## Additional exchange arguments.\n  # exchange_arguments = { }\n  # exchange_arguments = {\"hash_property\" = \"timestamp\"}\n\n  ## AMQP queue name\n  queue = \"circonus\"\n\n  ## AMQP queue durability can be \"transient\" or \"durable\".\n  queue_durability = \"durable\"\n\n  ## If true, queue will be passively declared.\n  # queue_passive = false\n\n  ## A binding between the exchange and queue using this binding key is\n  ## created.  If unset, no binding is created.\n  binding_key = \"#\"\n\n  ## Maximum number of messages server should give to the worker.\n  # prefetch_count = 50\n\n  ## Maximum messages to read from the broker that have not been written by an\n  ## output.  For best throughput set based on the number of metrics within\n  ## each message and the size of the output's metric_batch_size.\n  ##\n  ## For example, if each message from the queue contains 10 metrics and the\n  ## output metric_batch_size is 1000, setting this to 100 will ensure that a\n  ## full batch is collected and the write is triggered immediately without\n  ## waiting until the next flush_interval.\n  # max_undelivered_messages = 1000\n\n  ## Auth method. PLAIN and EXTERNAL are supported\n  ## Using EXTERNAL requires enabling the rabbitmq_auth_mechanism_ssl plugin as\n  ## described here: https://www.rabbitmq.com/plugins.html\n  # auth_method = \"PLAIN\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/circonus-unified-agent/ca.pem\"\n  # tls_cert = \"/etc/circonus-unified-agent/cert.pem\"\n  # tls_key = \"/etc/circonus-unified-agent/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n\n  ## Content encoding for message payloads, can be set to \"gzip\" to or\n  ## \"identity\" to apply no encoding.\n  # content_encoding = \"identity\"\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/circonus-labs/circonus-unified-agent/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"json\"\n```\n","logo":{},"attributes":{"implementation":"cua"},"module":"httptrap:cua:amqp_consumer"},{"name":"apache-activemq","title":"Apache ActiveMQ","content":"# Apache ActiveMQ\n\n## Overview\n\nThis plugin uses the ActiveMQ Console API to gather metrics for queues, topics, and subscribers.\n\n## Configuration\n\n```toml\n# Description\n[[inputs.activemq]]\n  ## ActiveMQ WebConsole URL\n  url = \"http://127.0.0.1:8161\"\n\n  ## Required ActiveMQ Endpoint\n  ##   deprecated in 1.11; use the url option\n  # server = \"192.168.50.10\"\n  # port = 8161\n\n  ## Credentials for basic HTTP authentication\n  # username = \"admin\"\n  # password = \"admin\"\n\n  ## Required ActiveMQ webadmin root path\n  # webadmin = \"admin\"\n\n  ## Maximum time to receive response.\n  # response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/circonus-unified-agent/ca.pem\"\n  # tls_cert = \"/etc/circonus-unified-agent/cert.pem\"\n  # tls_key = \"/etc/circonus-unified-agent/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n","logo":{"light":"/img/library/apache-activemq.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:activemq"},{"name":"apache-aurora","title":"Apache Aurora","content":"# Apache Aurora\n\n## Overview\n\nThe Aurora Input Plugin gathers metrics from [Apache Aurora](https://aurora.apache.org/) schedulers.\n\nFor monitoring recommendations reference [Monitoring your Aurora cluster](https://aurora.apache.org/documentation/latest/operations/monitoring/)\n\n## Configuration\n\n```toml\n[[inputs.aurora]]\n  ## Schedulers are the base addresses of your Aurora Schedulers\n  schedulers = [\"http://127.0.0.1:8081\"]\n\n  ## Set of role types to collect metrics from.\n  ##\n  ## The scheduler roles are checked each interval by contacting the\n  ## scheduler nodes; zookeeper is not contacted.\n  # roles = [\"leader\", \"follower\"]\n\n  ## Timeout is the max time for total network operations.\n  # timeout = \"5s\"\n\n  ## Username and password are sent using HTTP Basic Auth.\n  # username = \"username\"\n  # password = \"pa$$word\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/circonus-unified-agent/ca.pem\"\n  # tls_cert = \"/etc/circonus-unified-agent/cert.pem\"\n  # tls_key = \"/etc/circonus-unified-agent/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n\n## Troubleshooting\n\nCheck the Scheduler role, the leader will return a 200 status:\n\n```\ncurl -v http://127.0.0.1:8081/leaderhealth\n```\n\nGet available metrics:\n\n```\ncurl http://127.0.0.1:8081/vars\n```\n","logo":{"light":"/img/library/apache-aurora.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:aurora"},{"name":"apache-kafka","title":"Apache Kafka","content":"# Apache Kafka\n\n## Overview\n\nThe [Kafka](http://kafka.apache.org/) consumer plugin polls a specified Kafka\ntopic and adds messages to Circonus. The plugin assumes messages follow the\nline protocol. [Consumer Group](http://godoc.org/github.com/wvanbergen/kafka/consumergroup)\nis used to talk to the Kafka cluster so multiple instances of circonus-unified-agent can read\nfrom the same topic in parallel.\n\n## Configuration\n\n```toml\n# Read metrics from Kafka topic(s)\n[[inputs.kafka_consumer]]\n  ## topic(s) to consume\n  topics = [\"circonus\"]\n\n  ## an array of Zookeeper connection strings\n  zookeeper_peers = [\"localhost:2181\"]\n\n  ## Zookeeper Chroot\n  zookeeper_chroot = \"\"\n\n  ## the name of the consumer group\n  consumer_group = \"circonus_metrics_consumers\"\n\n  ## Offset (must be either \"oldest\" or \"newest\")\n  offset = \"oldest\"\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/circonus-labs/circonus-unified-agent/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"json\"\n\n  ## Maximum length of a message to consume, in bytes (default 0/unlimited);\n  ## larger messages are dropped\n  max_message_len = 65536\n```\n\n### Testing\n\nRunning integration tests requires running Zookeeper & Kafka. See Makefile\nfor kafka container command.\n","logo":{"light":"/img/library/apache-kafka.svg","dark":"/img/library/apache-kafka-dark.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:kafka_consumer"},{"name":"apache-legacy","title":"Apache","content":"# Apache\n\n## Overview\n\nThis check type extracts metrics from an Apache HTTP Server status page.\n\n## Configuration\n\nWith this check type, select from a variety of health and usage metrics, select the HTTP method to be used (GET, POST, HEAD), provide SSL and Server Authentication (username/password), add additional checks by parsing specific strings or patterns in BODY, or add any additional headers you may need.\n\n## Metrics\n\nAvailable metrics depend on the Apache server and relevant documentation can be found on the Apache HTTP Server project [website](https://httpd.apache.org/docs/).\n\nThe following metrics are typically available from Apache servers:\n\n- bytes\n- code\n- duration\n- truncated\n- tt_connect\n- tt_firstbyte\n","logo":{"light":"/img/library/apache.svg"},"attributes":{"implementation":"broker","legacy":true},"tags":["HTTP","server","status"],"module":"http:apache"},{"name":"apache-mesos","title":"Apache Mesos","content":"# Apache Mesos\n\n## Overview\n\nThis input plugin gathers metrics from Mesos. For more information, please check the [Mesos Observability Metrics](http://mesos.apache.org/documentation/latest/monitoring/) page.\n\n## Configuration\n\n```toml\n# plugin for gathering metrics from N Mesos masters\n[[inputs.mesos]]\n  ## Timeout, in ms.\n  timeout = 100\n\n  ## A list of Mesos masters.\n  masters = [\"http://localhost:5050\"]\n\n  ## Master metrics groups to be collected, by default, all enabled.\n  master_collections = [\n    \"resources\",\n    \"master\",\n    \"system\",\n    \"agents\",\n    \"frameworks\",\n    \"framework_offers\",\n    \"tasks\",\n    \"messages\",\n    \"evqueue\",\n    \"registrar\",\n    \"allocator\",\n  ]\n\n  ## A list of Mesos slaves, default is []\n  # slaves = []\n\n  ## Slave metrics groups to be collected, by default, all enabled.\n  # slave_collections = [\n  #   \"resources\",\n  #   \"agent\",\n  #   \"system\",\n  #   \"executors\",\n  #   \"tasks\",\n  #   \"messages\",\n  # ]\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/circonus-unified-agent/ca.pem\"\n  # tls_cert = \"/etc/circonus-unified-agent/cert.pem\"\n  # tls_key = \"/etc/circonus-unified-agent/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n\nBy default this plugin is not configured to gather metrics from mesos. Since a mesos cluster can be deployed in numerous ways it does not provide any default\nvalues. User needs to specify master/slave nodes this plugin will gather metrics from.\n","logo":{"light":"/img/library/apache-mesos.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:mesos"},{"name":"apache-tomcat","title":"Apache Tomcat","content":"# Apache Tomcat\n\n## Overview\n\nThe Tomcat plugin collects statistics available from the tomcat manager status page from the `http://<host>/manager/status/all?XML=true URL.` (`XML=true` will return only xml data).\n\nSee the [Tomcat documentation](https://tomcat.apache.org/tomcat-9.0-doc/manager-howto.html#Server_Status) for details of these statistics.\n\n## Configuration\n\n```toml\n# Gather metrics from the Tomcat server status page.\n[[inputs.tomcat]]\n  ## URL of the Tomcat server status\n  # url = \"http://127.0.0.1:8080/manager/status/all?XML=true\"\n\n  ## HTTP Basic Auth Credentials\n  # username = \"tomcat\"\n  # password = \"s3cret\"\n\n  ## Request timeout\n  # timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/circonus-unified-agent/ca.pem\"\n  # tls_cert = \"/etc/circonus-unified-agent/cert.pem\"\n  # tls_key = \"/etc/circonus-unified-agent/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n","logo":{"light":"/img/library/apache-tomcat.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:tomcat"},{"name":"apache-zookeeper","title":"Apache Zookeeper","content":"# Apache Zookeeper\n\n## Overview\n\nThe zookeeper plugin collects variables outputted from the 'mntr' command\n[Zookeeper Admin](https://zookeeper.apache.org/doc/current/zookeeperAdmin.html).\n\n## Configuration\n\n```toml\n# Reads 'mntr' stats from one or many zookeeper servers\n[[inputs.zookeeper]]\n  ## An array of address to gather stats about. Specify an ip or hostname\n  ## with port. ie localhost:2181, 10.0.0.1:2181, etc.\n\n  ## If no servers are specified, then localhost is used as the host.\n  ## If no port is specified, 2181 is used\n  servers = [\":2181\"]\n\n  ## Timeout for metric collections from all servers.  Minimum timeout is \"1s\".\n  # timeout = \"5s\"\n\n  ## Optional TLS Config\n  # enable_tls = true\n  # tls_ca = \"/etc/circonus-unified-agent/ca.pem\"\n  # tls_cert = \"/etc/circonus-unified-agent/cert.pem\"\n  # tls_key = \"/etc/circonus-unified-agent/key.pem\"\n  ## If false, skip chain & host verification\n  # insecure_skip_verify = true\n```\n\n## Troubleshooting\n\nIf you have any issues please check the direct Zookeeper output using netcat:\n\n```sh\n$ echo mntr | nc localhost 2181\nzk_version      3.4.9-3--1, built on Thu, 01 Jun 2017 16:26:44 -0700\nzk_avg_latency  0\nzk_max_latency  0\nzk_min_latency  0\nzk_packets_received     8\nzk_packets_sent 7\nzk_num_alive_connections        1\nzk_outstanding_requests 0\nzk_server_state standalone\nzk_znode_count  129\nzk_watch_count  0\nzk_ephemerals_count     0\nzk_approximate_data_size        10044\nzk_open_file_descriptor_count   44\nzk_max_file_descriptor_count    4096\n```\n","logo":{"light":"/img/library/apache-zookeeper.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:zookeeper"},{"name":"apache","title":"Apache","content":"# Apache\n\n## Overview\n\nThe Apache plugin collects server performance information using the [`mod_status`](https://httpd.apache.org/docs/2.4/mod/mod_status.html) module of the [Apache HTTP Server](https://httpd.apache.org/).\n\nTypically, the `mod_status` module is configured to expose a page at the `/server-status?auto` location of the Apache server. The [ExtendedStatus](https://httpd.apache.org/docs/2.4/mod/core.html#extendedstatus) option must be enabled in order to collect all available fields. For information about how to configure your server reference the [module documentation](https://httpd.apache.org/docs/2.4/mod/mod_status.html#enable).\n\n## Configuration\n\nThe following defaults are known to work with RabbitMQ:\n\n```toml\n# Read Apache status information (mod_status)\n[[inputs.apache]]\n  ## An array of URLs to gather from, must be directed at the machine\n  ## readable version of the mod_status page including the auto query string.\n  ## Default is \"http://localhost/server-status?auto\".\n  urls = [\"http://localhost/server-status?auto\"]\n\n  ## Credentials for basic HTTP authentication.\n  # username = \"myuser\"\n  # password = \"mypassword\"\n\n  ## Maximum time to receive response.\n  # response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/circonus-unified-agent/ca.pem\"\n  # tls_cert = \"/etc/circonus-unified-agent/cert.pem\"\n  # tls_key = \"/etc/circonus-unified-agent/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n","logo":{"light":"/img/library/apache.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:apache"},{"name":"apcupsd","title":"Apcupsd","content":"# Apcupsd\n\n## Overview\n\nThis plugin reads data from an apcupsd daemon over its NIS network protocol.\n\nApcupsd should be installed and it's daemon should be running.\n\n## Configuration\n\n```toml\n[[inputs.apcupsd]]\n  # A list of running apcupsd server to connect to.\n  # If not provided will default to tcp://127.0.0.1:3551\n  servers = [\"tcp://127.0.0.1:3551\"]\n\n  ## Timeout for dialing server.\n  timeout = \"5s\"\n```\n","logo":{"light":"/img/library/apc.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:apcupsd"},{"name":"arista-lanz","title":"Arista LANZ","content":"# Arista LANZ\n\n## Overview\n\nThis plugin provides a consumer for use with Arista Networks’ Latency Analyzer (LANZ)\n\nMetrics are read from a stream of data via TCP through port 50001 on the\nswitches management IP. The data is in Protobuffers format. For more information on Arista LANZ\n\n- https://www.arista.com/en/um-eos/eos-latency-analyzer-lanz\n\nThis plugin uses Arista's sdk.\n\n- https://github.com/aristanetworks/goarista\n\n## Configuration\n\nYou will need to configure LANZ and enable streaming LANZ data.\n\n- https://www.arista.com/en/um-eos/eos-section-44-3-configuring-lanz\n- https://www.arista.com/en/um-eos/eos-section-44-3-configuring-lanz#ww1149292\n\n```toml\n[[inputs.lanz]]\n  servers = [\n    \"tcp://switch1.int.example.com:50001\",\n    \"tcp://switch2.int.example.com:50001\",\n  ]\n```\n","logo":{"light":"/img/library/arista.svg","dark":"/img/library/arista-dark.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:lanz"},{"name":"bind","title":"BIND","content":"# BIND\n\n## Overview\n\nThis plugin decodes the JSON or XML statistics provided by BIND 9 nameservers.\n\n### XML Statistics Channel\n\nVersion 2 statistics (BIND 9.6 - 9.9) and version 3 statistics (BIND 9.9+) are supported. Note that\nfor BIND 9.9 to support version 3 statistics, it must be built with the `--enable-newstats` compile\nflag, and it must be specifically requested via the correct URL. Version 3 statistics are the\ndefault (and only) XML format in BIND 9.10+.\n\n### JSON Statistics Channel\n\nJSON statistics schema version 1 (BIND 9.10+) is supported. As of writing, some distros still do\nnot enable support for JSON statistics in their BIND packages.\n\n## Configuration\n\n- **urls** []string: List of BIND statistics channel URLs to collect from. Do not include a\n  trailing slash in the URL. Default is \"http://localhost:8053/xml/v3\".\n- **gather_memory_contexts** bool: Report per-context memory statistics.\n- **gather_views** bool: Report per-view query statistics.\n\nThe following table summarizes the URL formats which should be used, depending on your BIND\nversion and configured statistics channel.\n\n| BIND Version | Statistics Format | Example URL                   |\n| ------------ | ----------------- | ----------------------------- |\n| 9.6 - 9.8    | XML v2            | http://localhost:8053         |\n| 9.9          | XML v2            | http://localhost:8053/xml/v2  |\n| 9.9+         | XML v3            | http://localhost:8053/xml/v3  |\n| 9.10+        | JSON v1           | http://localhost:8053/json/v1 |\n\n### Configuration of BIND Daemon\n\nAdd the following to your named.conf if running the agent on the same host as the BIND daemon:\n\n```\nstatistics-channels {\n    inet 127.0.0.1 port 8053;\n};\n```\n\nAlternatively, specify a wildcard address (e.g., 0.0.0.0) or specific IP address of an interface to\nconfigure the BIND daemon to listen on that address. Note that you should secure the statistics\nchannel with an ACL if it is publicly reachable. Consult the BIND Administrator Reference Manual\nfor more information.\n","logo":{"light":"/img/library/bind.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:bind"},{"name":"burrow","title":"Burrow","content":"# Burrow\n\n## Overview\n\nCollect Kafka topic, consumer and partition status via [Burrow](https://github.com/linkedin/Burrow) HTTP [API](https://github.com/linkedin/Burrow/wiki/HTTP-Endpoint).\n\nSupported Burrow version: `1.x`\n\n## Configuration\n\n```toml\n[[inputs.burrow]]\n  ## Burrow API endpoints in format \"schema://host:port\".\n  ## Default is \"http://localhost:8000\".\n  servers = [\"http://localhost:8000\"]\n\n  ## Override Burrow API prefix.\n  ## Useful when Burrow is behind reverse-proxy.\n  # api_prefix = \"/v3/kafka\"\n\n  ## Maximum time to receive response.\n  # response_timeout = \"5s\"\n\n  ## Limit per-server concurrent connections.\n  ## Useful in case of large number of topics or consumer groups.\n  # concurrent_connections = 20\n\n  ## Filter clusters, default is no filtering.\n  ## Values can be specified as glob patterns.\n  # clusters_include = []\n  # clusters_exclude = []\n\n  ## Filter consumer groups, default is no filtering.\n  ## Values can be specified as glob patterns.\n  # groups_include = []\n  # groups_exclude = []\n\n  ## Filter topics, default is no filtering.\n  ## Values can be specified as glob patterns.\n  # topics_include = []\n  # topics_exclude = []\n\n  ## Credentials for basic HTTP authentication.\n  # username = \"\"\n  # password = \"\"\n\n  ## Optional SSL config\n  # ssl_ca = \"/etc/circonus-unified-agent/ca.pem\"\n  # ssl_cert = \"/etc/circonus-unified-agent/cert.pem\"\n  # ssl_key = \"/etc/circonus-unified-agent/key.pem\"\n  # insecure_skip_verify = false\n```\n","logo":{"light":"/img/library/apache-kafka.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:burrow"},{"name":"caql","title":"CAQL","content":"# CAQL\n\n## Overview\n\nThe CAQL Check allows users to create new metrics from combining or performing analytics on other metrics.\n\n## Metrics\n\nWithin Circonus, each CAQL metric is its own check which provides exactly one metric. The name of that metric is chosen by the creator of the check.\n\n## Notes\n\nFor full details, please see the [CAQL Documentation](/caql)\n","logo":{"light":"/img/library/caql.svg"},"attributes":{"implementation":"broker"},"tags":["caql"],"module":"caql"},{"name":"category","content":"{\n    \"label\": \"Library\",\n    \"position\": 1,\n    \"link\": {\n        \"type\": \"generated-index\"\n    }\n}","logo":{},"attributes":{}},{"name":"ceph","title":"Ceph","content":"# Ceph\n\n## Overview\n\nCollects performance metrics from the MON and OSD nodes in a Ceph storage cluster.\n\nThe Ceph CUA module is compatible with the agent and can send metrics with a socket_listener.\n\n### Admin Socket Stats\n\nThis gatherer works by scanning the configured SocketDir for OSD, MON, MDS and RGW socket files. When it finds\na MON socket, it runs **ceph --admin-daemon $file perfcounters_dump**. For OSDs it runs **ceph --admin-daemon $file perf dump**\n\nThe resulting JSON is parsed and grouped into collections, based on top-level key. Top-level keys are\nused as collection tags, and all sub-keys are flattened. For example:\n\n```json\n{\n  \"paxos\": {\n    \"refresh\": 9363435,\n    \"refresh_latency\": {\n      \"avgcount\": 9363435,\n      \"sum\": 5378.794002\n    }\n  }\n}\n```\n\nWould be parsed into the following metrics, all of which would be tagged with collection=paxos:\n\n- refresh = 9363435\n- refresh_latency.avgcount: 9363435\n- refresh_latency.sum: 5378.794002000\n\n### Cluster Stats\n\nThis gatherer works by invoking ceph commands against the cluster thus only requires the ceph client, valid\nceph configuration and an access key to function (the ceph_config and ceph_user configuration variables work\nin conjunction to specify these prerequisites). It may be run on any server you wish which has access to\nthe cluster. The currently supported commands are:\n\n- ceph status\n- ceph df\n- ceph osd pool stats\n\n## Configuration\n\n```toml\n# Collects performance metrics from the MON and OSD nodes in a Ceph storage cluster.\n[[inputs.ceph]]\n  ## This is the recommended interval to poll.  Too frequent and you will lose\n  ## data points due to timeouts during rebalancing and recovery\n  interval = '1m'\n\n  ## All configuration values are optional, defaults are shown below\n\n  ## location of ceph binary\n  ceph_binary = \"/usr/bin/ceph\"\n\n  ## directory in which to look for socket files\n  socket_dir = \"/var/run/ceph\"\n\n  ## prefix of MON and OSD socket files, used to determine socket type\n  mon_prefix = \"ceph-mon\"\n  osd_prefix = \"ceph-osd\"\n  mds_prefix = \"ceph-mds\"\n  rgw_prefix = \"ceph-client\"\n\n  ## suffix used to identify socket files\n  socket_suffix = \"asok\"\n\n  ## Ceph user to authenticate as, ceph will search for the corresponding keyring\n  ## e.g. client.admin.keyring in /etc/ceph, or the explicit path defined in the\n  ## client section of ceph.conf for example:\n  ##\n  ##     [client.circonus]\n  ##         keyring = /etc/ceph/client.circonus.keyring\n  ##\n  ## Consult the ceph documentation for more detail on keyring generation.\n  ceph_user = \"client.admin\"\n\n  ## Ceph configuration to use to locate the cluster\n  ceph_config = \"/etc/ceph/ceph.conf\"\n\n  ## Whether to gather statistics via the admin socket\n  gather_admin_socket_stats = true\n\n  ## Whether to gather statistics via ceph commands, requires ceph_user and ceph_config\n  ## to be specified\n  gather_cluster_stats = false\n```\n","logo":{"light":"/img/library/ceph.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:ceph"},{"name":"cim","title":"CIM","content":"# CIM\n\n## Overview\n\nThe CIM Check monitors your [Common Information Model](https://www.dmtf.org/standards/cim) compliant system. This will require server authorization (Username and Password) for the target host.\n\nThe Common Information Model (CIM) is a cross-platform, object-oriented data model containing information about different parts of an enterprise. It is commonly used on Microsoft platforms via [Windows Management Instrumentation](https://docs.microsoft.com/en-us/windows/desktop/wmisdk/common-information-model) (WMI) and in VMware deployments.\n\n## Configuration\n\nYou may specify fields to pull data from the CIM server classes (for example: \"HealthState\", or \"Operational Status\"). These fields are optional; if no fields are specified, all available fields will be returned.\n\nYou can also have the option to upgrade the TCP connection to use SSL.\n","logo":{"light":"/img/library/cim.svg"},"attributes":{"implementation":"broker"},"tags":["protocol","Common","Information","Model","system"],"module":"cim"},{"name":"cisco-mdt","title":"Cisco MDT","content":"# Cisco MDT\n\n## Overview\n\nCisco model-driven telemetry (MDT) is an input plugin that consumes\ntelemetry data from Cisco IOS XR, IOS XE and NX-OS platforms. It supports TCP & GRPC dialout transports.\nGRPC-based transport can utilize TLS for authentication and encryption.\nTelemetry data is expected to be GPB-KV (self-describing-gpb) encoded.\n\nThe GRPC dialout transport is supported on various IOS XR (64-bit) 6.1.x and later, IOS XE 16.10 and later, as well as NX-OS 7.x and later platforms.\n\nThe TCP dialout transport is supported on IOS XR (32-bit and 64-bit) 6.1.x and later.\n\n## Configuration\n\n```toml\n[[inputs.cisco_telemetry_mdt]]\n ## Telemetry transport can be \"tcp\" or \"grpc\".  TLS is only supported when\n ## using the grpc transport.\n transport = \"grpc\"\n\n ## Address and port to host telemetry listener\n service_address = \":57000\"\n\n ## Enable TLS; grpc transport only.\n # tls_cert = \"/etc/circonus-unified-agent/cert.pem\"\n # tls_key = \"/etc/circonus-unified-agent/key.pem\"\n\n ## Enable TLS client authentication and define allowed CA certificates; grpc\n ##  transport only.\n # tls_allowed_cacerts = [\"/etc/circonus-unified-agent/clientca.pem\"]\n\n ## Define (for certain nested telemetry measurements with embedded tags) which fields are tags\n # embedded_tags = [\"Cisco-IOS-XR-qos-ma-oper:qos/interface-table/interface/input/service-policy-names/service-policy-instance/statistics/class-stats/class-name\"]\n\n ## Define aliases to map telemetry encoding paths to simple measurement names\n [inputs.cisco_telemetry_mdt.aliases]\n   ifstats = \"ietf-interfaces:interfaces-state/interface/statistics\"\n```\n","logo":{"light":"/img/library/cisco.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:cisco_telemetry_mdt"},{"name":"collectd-tags","title":"collectd + tags","content":"# collectd + tags\n\n## Overview\n\nThis check type allows you to run [collectd](http://collectd.org/) on your hosts and send that data directly to Circonus. Collectd is a lightweight C-based tool that has a variety of plugins available for data collection.\n\nThe collectd + tags Check allows you to run [collectd](https://collectd.org/) in combination with the [write_tsdb](https://collectd.org/wiki/index.php/Plugin:Write_TSDB) plugin to submit tagged metrics directly to Circonus.\n\n## Configuration\n\nProvided metrics depend on what is submitted by the underlying collectd daemon, and how it’s configured.\n\nThis Check _requires_ the write_tsdb plugin be used in combination with collectd.\n\nFor additional instructions in using collectd to submit metrics to Circonus, see the [collectd documentation](https://docs.circonus.com/circonus/integrations/library/collectd/).\n\n## Metrics\n\nProvided metrics depend on what is submitted by the underlying collectd daemon, and how it’s configured.\n","logo":{"light":"/img/library/collectd+tags.svg","dark":"/img/library/collectd+tags-dark.svg"},"attributes":{"implementation":"broker"},"tags":["collectd","tags","push","write_tsdb"],"module":"opentsdb:collectd_write_tsdb"},{"name":"collectd","title":"collectd","content":"# collectd\n\n## Overview\n\nThis check type allows you to run [collectd](http://collectd.org/) on your hosts and send that data directly to Circonus. Collectd is a lightweight C-based tool that has a variety of plugins available for data collection.\n\n## Configuration\n\nThere are two main ways to use collectd with Circonus, either to push the information from your device over UDP (similar to statsd and HTTP Traps) or via the `write_http` plugin.\n\nBy default, collectd sends data over UDP to a broker on port 25826. Make sure your `collectd.conf` is properly configured to send to this port, or change the default port to the correct port.\n\nBecause of the way collectd sends data, **you cannot send data to any of the Circonus Public Brokers**. Circonus Enterprise Brokers should be used for collectd checks.\n\nYou may configure the check to specify Authorization information and set a Security Level. Security Level is an integer describing the security of allowed packets; 0 allows all packets, 1 allows signed and encrypted packets, 2 only allows encrypted packets.\n\n![Image: 'collectd_config3.png'](../../img/collectd_config3.png)\n\n### UDP submission\n\nThe target field for UDP can be either and IP address or a domain name that resolves to an IP address. When the packet is received by the broker, the source address is matched against the resolved IP address. If no check is found with that resolved IP, the information will be discarded. In this case, we strongly recommend that the IP address be used to identify the target host, rather than the domain name, to prevent potential DNS issues from causing data loss.\n\n### `write_http` submission\n\nWhen collectd submits data via HTTP, one of the attributes in the data payload is \"host\", which is the hostname of your machine. The broker will dissect the payload and attempt to find a collectd check in Circonus that has a target host exactly matching the host name specified. Do not use an IP address to identify the target host unless your collectd instance is reporting its \"host\" as that IP address.\n\nThe following should be added to your collectd configuration:\n\n```\n<Plugin write_http>\n   <URL \"https://<broker host>:43191/module/collectd/\">\n     User \"username\"\n     Password \"s3cr3t\"\n     VerifyPeer false\n     VerifyHost false\n     CACert \"/etc/circonus-ca.crt\"\n     Format \"JSON\"\n     StoreRates false\n   </URL>\n</Plugin>\n```\n\n> You can get a copy of the Circonus CA certificate here: https://login.circonus.com/pki/ca.crt\n\n> Target host is name matched (not IP matched) for the `write_http` check.\n\nThe User and Password must match the Username and Password in the \"Advanced Configuration\" check. For `write_http`, Username and Password are not optional. Both must be specified and not blank.\n\n## Metrics\n\nProvided metrics depend on what is submitted by the underlying collectd daemon, and how it’s configured.\n","logo":{"light":"../../img/s/circonus/library/collectd.svg","dark":"../../img/s/circonus/library/collectd-dark.svg"},"attributes":{"implementation":"broker"},"tags":["push"],"module":"collectd"},{"name":"couchbase","title":"Couchbase","content":"# Couchbase\n\n## Configuration\n\n```toml\n# Read per-node and per-bucket metrics from Couchbase\n[[inputs.couchbase]]\n  ## specify servers via a url matching:\n  ##  [protocol://][:password]@address[:port]\n  ##  e.g.\n  ##    http://couchbase-0.example.com/\n  ##    http://admin:secret@couchbase-0.example.com:8091/\n  ##\n  ## If no servers are specified, then localhost is used as the host.\n  ## If no protocol is specified, HTTP is used.\n  ## If no port is specified, 8091 is used.\n  servers = [\"http://localhost:8091\"]\n```\n","logo":{"light":"/img/library/couchbase.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:couchbase"},{"name":"couchdb-legacy","title":"CouchDB","content":"# CouchDB\n\n## Overview\n\nApache CouchDB is a NoSQL database that uses JSON for documents, JavaScript for MapReduce indexes, and HTTP for its API.\n\nThe CouchDB Check pulls statistics from CouchDB in JSON format via the database's native interface.\n\n## Metrics\n\nTypical metrics include request times, requests by type, continuous changes feed listeners, response status codes, authentication cache, database I/O, and many more.\n","logo":{"light":"/img/library/couchdb.svg"},"attributes":{"implementation":"broker","legacy":true},"tags":["apache","nosql","database"],"module":"json:couchdb"},{"name":"couchdb","title":"CouchDB","content":"# CouchDB\n\n## Overview\n\nThe CouchDB plugin gathers metrics of CouchDB using [\\_stats](http://docs.couchdb.org/en/1.6.1/api/server/common.html?highlight=stats#get--_stats) endpoint.\n\n## Configuration\n\n```toml\n[[inputs.couchdb]]\n  ## Works with CouchDB stats endpoints out of the box\n  ## Multiple Hosts from which to read CouchDB stats:\n  hosts = [\"http://localhost:8086/_stats\"]\n\n  ## Use HTTP Basic Authentication.\n  # basic_username = \"circonus\"\n  # basic_password = \"p@ssw0rd\"\n```\n","logo":{"light":"/img/library/couchdb.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:couchdb"},{"name":"custom","title":"Custom","content":"# Custom\n\n## Overview\n\nCustom checks are created by the user and configured to run on an Enterprise Broker. This check will only operate on Enterprise Brokers.\n\n## Metrics\n\nProvided metrics depend on what is submitted to the Custom check interface.\n","logo":{"light":"/img/library/custom.svg"},"attributes":{"implementation":"broker"},"module":"custom"},{"name":"dhcp","title":"DHCP","content":"# DHCP\n\n## Overview\n\nThis check type monitors your DHCP system.\n\n## Configuration\n\nYou will need to provide the hardware address of the DHCP client, in \"`00:00:00:00:00:00`\" format.\n\nYou further have the option to specify the port to which DHCP requests are sent, the IP address of the agent running the DHCP check, and to select a request type, such as DHCPDiscover or DHCPInform.\n\n> This check will only operate on Enterprise Brokers.\n","logo":{"light":"/img/library/dhcp.svg"},"attributes":{"implementation":"broker"},"tags":["network","Dynamic","Host","Configuration","Protocol"],"module":"dhcp"},{"name":"dns-legacy","title":"DNS","content":"# DNS\n\n## Overview\n\nThis check type monitors your DNS server responses.\n\n## Configuration\n\nThe DNS check offers support for a variety of DNS record types. The following record types are supported:\n\n| Record | Description                                                   |\n| ------ | ------------------------------------------------------------- |\n| A      | Name to number for IPv4 (i.e. circonus.com -> 66.225.209.241) |\n| AAAA   | Name to number for IPv6                                       |\n| PTR    | Number to name (i.e. 66.225.209.241 -> circonus.com)          |\n| TXT    | Text record                                                   |\n| MX     | Mail exchange                                                 |\n| CNAME  | Canonical name                                                |\n| NS     | Nameserver                                                    |\n\nAfter choosing the record type, enter the record you wish to look up. For example, if you chose the A record type, enter a Fully Qualified Domain Name (FQDN) as the \"Record to look up\".\n\n## Metrics\n\n| Name   | Type    | Description                                                                                                     |\n| ------ | ------- | --------------------------------------------------------------------------------------------------------------- |\n| answer | text    | The value returned by the DNS server for the query                                                              |\n| rtt    | numeric | The round trip time, in milliseconds, for the DNS request, from the perspective of the broker running the check |\n| ttl    | numeric | The time-to-live value of the requested record                                                                  |\n","logo":{"light":"/img/library/dns.svg"},"attributes":{"implementation":"broker","legacy":true},"tags":["protocol","server","IP","address","Domain","Name","Service"],"module":"dns"},{"name":"dns","title":"DNS","content":"# DNS\n\n## Overview\n\nThe DNS plugin gathers dns query times in miliseconds - like [Dig](<https://en.wikipedia.org/wiki/Dig_(command)>)\n\n## Configuration\n\n```toml\n# Query given DNS server and gives statistics\n[[inputs.dns_query]]\n  ## servers to query\n  servers = [\"8.8.8.8\"]\n\n  ## Network is the network protocol name.\n  # network = \"udp\"\n\n  ## Domains or subdomains to query.\n  # domains = [\".\"]\n\n  ## Query record type.\n  ## Possible values: A, AAAA, CNAME, MX, NS, PTR, TXT, SOA, SPF, SRV.\n  # record_type = \"A\"\n\n  ## Dns server port.\n  # port = 53\n\n  ## Query timeout in seconds.\n  # timeout = 2\n```\n","logo":{"light":"/img/library/dns.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:dns_query"},{"name":"docker","title":"Docker","content":"# Docker\n\n## Overview\n\nThe docker plugin uses the Docker Engine API to gather metrics on running docker containers and the [Official Docker Client](https://github.com/moby/moby/tree/master/client) to gather stats from the [Engine API](https://docs.docker.com/engine/api/v1.24/).\n\n## Configuration\n\n```toml\n# Read metrics about docker containers\n[[inputs.docker]]\n  ## Docker Endpoint\n  ##   To use TCP, set endpoint = \"tcp://[ip]:[port]\"\n  ##   To use environment variables (ie, docker-machine), set endpoint = \"ENV\"\n  endpoint = \"unix:///var/run/docker.sock\"\n\n  ## Set to true to collect Swarm metrics(desired_replicas, running_replicas)\n  ## Note: configure this in one of the manager nodes in a Swarm cluster.\n  ## configuring in multiple Swarm managers results in duplication of metrics.\n  gather_services = false\n\n  ## Only collect metrics for these containers. Values will be appended to\n  ## container_name_include.\n  ## Deprecated (1.4.0), use container_name_include\n  container_names = []\n\n  ## Set the source tag for the metrics to the container ID hostname, eg first 12 chars\n  source_tag = false\n\n  ## Containers to include and exclude. Collect all if empty. Globs accepted.\n  container_name_include = []\n  container_name_exclude = []\n\n  ## Container states to include and exclude. Globs accepted.\n  ## When empty only containers in the \"running\" state will be captured.\n  ## example: container_state_include = [\"created\", \"restarting\", \"running\", \"removing\", \"paused\", \"exited\", \"dead\"]\n  ## example: container_state_exclude = [\"created\", \"restarting\", \"running\", \"removing\", \"paused\", \"exited\", \"dead\"]\n  # container_state_include = []\n  # container_state_exclude = []\n\n  ## Timeout for docker list, info, and stats commands\n  timeout = \"5s\"\n\n  ## Whether to report for each container per-device blkio (8:0, 8:1...) and\n  ## network (eth0, eth1, ...) stats or not\n  perdevice = true\n\n  ## Whether to report for each container total blkio and network stats or not\n  total = false\n\n  ## docker labels to include and exclude as tags.  Globs accepted.\n  ## Note that an empty array for both will include all labels as tags\n  docker_label_include = []\n  docker_label_exclude = []\n\n  ## Which environment variables should we use as a tag\n  tag_env = [\"JAVA_HOME\", \"HEAP_SIZE\"]\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/circonus-unified-agent/ca.pem\"\n  # tls_cert = \"/etc/circonus-unified-agent/cert.pem\"\n  # tls_key = \"/etc/circonus-unified-agent/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n\n### Environment Configuration\n\nWhen using the `\"ENV\"` endpoint, the connection is configured using the\n[cli Docker environment variables](https://godoc.org/github.com/moby/moby/client#NewEnvClient).\n\n### Security\n\nGiving the agent access to the Docker daemon expands the [attack surface](https://docs.docker.com/engine/security/security/#docker-daemon-attack-surface) that could result in an attacker gaining root access to a machine. This is especially relevant if the agent configuration can be changed by untrusted users.\n\n### Docker Daemon Permissions\n\nTypically, the agent must be given permission to access the docker daemon unix\nsocket when using the default endpoint. This can be done by adding the\n`cua` unix user (created when installing the circonus-unified-agent package) to the\n`docker` unix group with the following command:\n\n```\nsudo usermod -aG docker cua\n```\n\nIf the agent is run within a container, the unix socket will need to be exposed\nwithin the container. This can be done in the docker CLI by add the\noption `-v /var/run/docker.sock:/var/run/docker.sock` or adding the following\nlines to the circonus-unified-agent container definition in a docker compose file:\n\n```\nvolumes:\n  - /var/run/docker.sock:/var/run/docker.sock\n```\n\n### source tag\n\nSelecting the containers measurements can be tricky if you have many containers with the same name.\nTo alleviate this issue you can set the below value to `true`\n\n```toml\nsource_tag = true\n```\n\nThis will cause all measurements to have the `source` tag be set to the first 12 characters of the container id. The first 12 characters is the common hostname for containers that have no explicit hostname set, as defined by docker.\n\n### Kubernetes Labels\n\nKubernetes may add many labels to your containers, if they are not needed you\nmay prefer to exclude them:\n\n```\n  docker_label_exclude = [\"annotation.kubernetes*\"]\n```\n","logo":{"light":"/img/library/docker.svg"},"attributes":{"implementation":"cua"},"tags":["containers","management","virtual"],"module":"httptrap:cua:docker"},{"name":"dovecot","title":"Dovecot","content":"# Dovecot\n\n## Overview\n\nThe dovecot plugin uses the Dovecot [v2.1 stats protocol](http://wiki2.dovecot.org/Statistics/Old) to gather\nmetrics on configured domains.\n\nWhen using Dovecot v2.3 you are still able to use this protocol by following\nthe [upgrading steps](https://wiki2.dovecot.org/Upgrading/2.3#Statistics_Redesign).\n\n## Configuration\n\n```toml\n# Read metrics about dovecot servers\n[[inputs.dovecot]]\n  ## specify dovecot servers via an address:port list\n  ##  e.g.\n  ##    localhost:24242\n  ##\n  ## If no servers are specified, then localhost is used as the host.\n  servers = [\"localhost:24242\"]\n\n  ## Type is one of \"user\", \"domain\", \"ip\", or \"global\"\n  type = \"global\"\n\n  ## Wildcard matches like \"*.com\". An empty string \"\" is same as \"*\"\n  ## If type = \"ip\" filters should be <IP/network>\n  filters = [\"\"]\n```\n","logo":{"light":"/img/library/dovecot.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:dovecot"},{"name":"elasticsearch-legacy","title":"Elasticsearch","content":"# Elasticsearch\n\n## Overview\n\n[Elasticsearch](http://www.elasticsearch.org/) is a flexible and powerful open source, distributed, real-time search and analytics engine because of its robust set of APIs and query DSLs, with clients for the most popular programming languages. Almost any action can be performed using a simple RESTful API using JSON over HTTP.\n\nWith an elasticsearch check, you can set alerts within Circonus to send notifications allowing you to make any necessary changes to your resources.\n\n## Metrics\n\nCirconus collects information from elasticsearch in JSON format via the elasticsearch NoSQL database's native interface. Circonus turns these JSON documents into metrics for trending and alerting. Circonus uses this to track the inserts, deletes, and the searches performed on each node.\n","logo":{"light":"/img/library/elasticsearch.svg","dark":"/img/library/elasticsearch-dark.svg"},"attributes":{"implementation":"broker","legacy":true},"tags":["nosql"],"module":"elasticsearch"},{"name":"elasticsearch","title":"Elasticsearch","content":"# Elasticsearch\n\n## Overview\n\nThe [elasticsearch](https://www.elastic.co/) plugin queries endpoints to obtain\n[Node Stats](https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-nodes-stats.html)\nand optionally\n[Cluster-Health](https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-health.html)\nmetrics.\n\nIn addition, the following optional queries are only made by the master node:\n[Cluster Stats](https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-stats.html)\n[Indices Stats](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-stats.html)\n[Shard Stats](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-stats.html)\n\nSpecific Elasticsearch endpoints that are queried:\n\n- Node: either /\\_nodes/stats or /\\_nodes/\\_local/stats depending on 'local' configuration setting\n- Cluster Heath: /\\_cluster/health?level=indices\n- Cluster Stats: /\\_cluster/stats\n- Indices Stats: /\\_all/\\_stats\n- Shard Stats: /\\_all/\\_stats?level=shards\n\nNote that specific statistics information can change between Elasticsearch versions. In general, this plugin attempts to stay as version-generic as possible by tagging high-level categories only and using a generic json parser to make unique field names of whatever statistics names are provided at the mid-low level.\n\n## Configuration\n\n```toml\n[[inputs.elasticsearch]]\n  ## specify a list of one or more Elasticsearch servers\n  ## you can add username and password to your url to use basic authentication:\n  ## servers = [\"http://user:pass@localhost:9200\"]\n  servers = [\"http://localhost:9200\"]\n\n  ## Timeout for HTTP requests to the elastic search server(s)\n  http_timeout = \"5s\"\n\n  ## When local is true (the default), the node will read only its own stats.\n  ## Set local to false when you want to read the node stats from all nodes\n  ## of the cluster.\n  local = true\n\n  ## Set cluster_health to true when you want to obtain cluster health stats\n  cluster_health = false\n\n  ## Adjust cluster_health_level when you want to obtain detailed health stats\n  ## The options are\n  ##  - indices (default)\n  ##  - cluster\n  # cluster_health_level = \"indices\"\n\n  ## Set cluster_stats to true when you want to obtain cluster stats.\n  cluster_stats = false\n\n  ## Only gather cluster_stats from the master node. To work this require local = true\n  cluster_stats_only_from_master = true\n\n  ## Indices to collect; can be one or more indices names or _all\n  indices_include = [\"_all\"]\n\n  ## One of \"shards\", \"cluster\", \"indices\"\n  ## Currently only \"shards\" is implemented\n  indices_level = \"shards\"\n\n  ## node_stats is a list of sub-stats that you want to have gathered. Valid options\n  ## are \"indices\", \"os\", \"process\", \"jvm\", \"thread_pool\", \"fs\", \"transport\", \"http\",\n  ## \"breaker\". Per default, all stats are gathered.\n  # node_stats = [\"jvm\", \"http\"]\n\n  ## HTTP Basic Authentication username and password.\n  # username = \"\"\n  # password = \"\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/circonus-unified-agent/ca.pem\"\n  # tls_cert = \"/etc/circonus-unified-agent/cert.pem\"\n  # tls_key = \"/etc/circonus-unified-agent/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n","logo":{"light":"/img/library/elasticsearch.svg","dark":"/img/library/elasticsearch-dark.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:elasticsearch"},{"name":"ethtool","title":"Ethtool","content":"# Ethtool\n\n## Overview\n\nThe ethtool input plugin pulls ethernet device stats. Fields pulled will depend on the network device and driver.\n\n## Configuration\n\n```toml\n# Returns ethtool statistics for given interfaces\n[[inputs.ethtool]]\n  ## List of interfaces to pull metrics for\n  # interface_include = [\"eth0\"]\n\n  ## List of interfaces to ignore when pulling metrics.\n  # interface_exclude = [\"eth1\"]\n```\n\nInterfaces can be included or ignored using:\n\n- `interface_include`\n- `interface_exclude`\n\nNote that loopback interfaces will be automatically ignored.\n","logo":{"light":"/img/library/ethtool.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:ethtool"},{"name":"exec","title":"Exec","content":"# Exec\n\n## Overview\n\nThe `exec` plugin executes all the `commands` in parallel on every interval and parses metrics from\ntheir output in any one of the accepted [Input Data Formats](https://github.com/circonus-labs/circonus-unified-agent/blob/master/docs/DATA_FORMATS_INPUT.md).\n\nThis plugin can be used to poll for custom metrics from any source.\n\n## Configuration\n\n```toml\n[[inputs.exec]]\n  ## Commands array\n  commands = [\n    \"/tmp/test.sh\",\n    \"/usr/bin/mycollector --foo=bar\",\n    \"/tmp/collect_*.sh\"\n  ]\n\n  ## Timeout for each command to complete.\n  timeout = \"5s\"\n\n  ## measurement name suffix (for separating different commands)\n  name_suffix = \"_mycollector\"\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/circonus-labs/circonus-unified-agent/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n```\n\nGlob patterns in the `command` option are matched on every run, so adding new\nscripts that match the pattern will cause them to be picked up immediately.\n\n### Example\n\nThis script produces static values, since no timestamp is specified the values are at the current time.\n\n```sh\n#!/bin/sh\necho 'example,tag1=a,tag2=b i=42i,j=43i,k=44i'\n```\n\nIt can be paired with the following configuration and will be run at the `interval` of the agent.\n\n```toml\n[[inputs.exec]]\n  instance_id = \"\" # unique instance identifier (REQUIRED)\n\n  commands = [\"sh /tmp/test.sh\"]\n  timeout = \"5s\"\n  data_format = \"influx\"\n```\n\n## Troubleshooting\n\n_This script works when you run it by hand, but not when the agent is running as a service._\n\nThis may be related to the agent service running as a different user. The\nofficial packages run the agent as the `cua` user and group on Linux\nsystems.\n\n_With a PowerShell on Windows, the output of the script appears to be truncated._\n\nYou may need to set a variable in your script to increase the number of columns\navailable for output:\n\n```\n$host.UI.RawUI.BufferSize = new-object System.Management.Automation.Host.Size(1024,50)\n```\n","logo":{"light":"/img/library/exec.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:exec"},{"name":"external","title":"External","content":"# External\n\n## Overview\n\nThe External Check allows you to run an executable on the same box as your Enterprise Broker and turn the output into metrics.\n\nAn executable is defined as anything that runs for a short time (starts and ends) and creates output. Most often, it is a script in any language that your environment understands and can execute. A good rule of thumb is that if you can execute it on the command line, then it would work in this check type.\nMetrics\n\n## Configuration\n\nBy default, executables must be located on your Enterprise Broker under `/opt/noit/prod/libexec/external-plugins/`. These executables can be under either \"`/site`\" (for site-wide executables) or \"`/local`\" (for local executables) and we'll execute them and parse the results.\n\nIn the creation of the output, either a regular expression or Nagios is defined in the Output Extract field in the advanced options. If Nagios is defined, then Circonus takes the output of the executable and parses it to create the metrics. This is the default and is the preferred method.\n\n**Note:**\n\n> This check type only appears as an option if you have an Enterprise Broker defined.\n> The Broker that is selected must be the Enterprise Broker where the executable has been placed.\n\n### Defining the Host and the Executable\n\n![Image: 'ext_chk_configure3.png'](../../img/ext_chk_configure3.png)\n\n1.  Enter the **Host**. You must enter either a hostname or an IPv4 address (ex. `1.2.3.4`). If the program you are running does not require an external host, enter \"`127.0.0.1`\".\n1.  Enter the **Command** to run on your broker, including the full path. They can be in \"`/opt/noit/prod/libexec/external-plugins`\" under either \"`/site`\" (for site-wide) or \"`/local`\" (for local). Do not include command line arguments here.\n1.  If required, enter the **Command Line Arguments** to pass to the external program. Make sure you enter these in the correct order, one parameter per box. You may use the check target host as a value here by entering \"`%[target]`\". If you need more than one parameter, click the Add New Parameter link and a new entry area is displayed. They are numbered (Parameter 1, Parameter 2, Parameter 3, etc.).\n1.  If required, enter any **Environment Variables** that need to be set when running the external program, where \"Name\" is the environment variable to be set and \"Value\" is the value to which the environment variable is set. If you need more than one environment variable, click the \"Add New Environment Variable\" link and a new entry area will be displayed.\n\n### Using the Output Extract\n\n![Image: 'ext_chk_adv_configure3.png'](../../img/ext_chk_adv_configure3.png)\n\nThe Output Extract field determines how the output data from the executable is parsed. You can use either a regular expression, JSON, or Nagios as possible values in this field. Unless you are very familiar with creating regular expressions, in most cases you should use Nagios as the Output Extract.\n\nOptions for Output Extract values include:\n\n- Nagios\n- JSON\n- All other values are treated as a regular expression.\n\nIf a regular expression is used, then it is globally applied to the standard output of the command. Each match is turned into a metric. It is a requirement to use named capturing in the regular expression, where \"key\" is the named match of the metric name and \"value\" is the named match for the metric value. For example, extracting performance data in the form of \"x=5\", \"y=6\", etc. would look like this:\n\n```\n(?<key>\\S+)=(?<value>[^;\\s]+)(?=[;\\s])\n```\n\nIf Nagios is defined, then Circonus takes the output of the executable and parses it to create the metrics. This is the default and is the preferred method.\n\n## Metrics\n\nProvided metrics depend on the metrics made available by the specified executable.\n","logo":{"light":"../../img/library/external.svg"},"attributes":{"implementation":"broker"},"tags":["script","executable"],"module":"external"},{"name":"fibaro","title":"Fibaro","content":"# Fibaro\n\n## Overview\n\nThe Fibaro plugin makes HTTP calls to the Fibaro controller API to gather values of hooked devices.\nThose values could be true (1) or false (0) for switches, percentage for dimmers, temperature, etc.\n\n## Configuration\n\n```toml\n# Read devices value(s) from a Fibaro controller\n[[inputs.fibaro]]\n  ## Required Fibaro controller address/hostname.\n  ## Note: at the time of writing this plugin, Fibaro only implemented http - no https available\n  url = \"http://<controller>:80\"\n\n  ## Required credentials to access the API (http://<controller/api/<component>)\n  username = \"<username>\"\n  password = \"<password>\"\n\n  ## Amount of time allowed to complete the HTTP request\n  # timeout = \"5s\"\n```\n","logo":{"light":"/img/library/fibaro.svg","dark":"/img/library/fibaro-dark.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:fibaro"},{"name":"fireboard","title":"Fireboard","content":"# Fireboard\n\n## Overview\n\nThe fireboard plugin gathers the real time temperature data from fireboard\nthermometers. In order to use this input plugin, you'll need to sign up to use\nthe [Fireboard REST API](https://docs.fireboard.io/reference/restapi.html).\n\n## Configuration\n\n```toml\n[[inputs.fireboard]]\n  ## Specify auth token for your account\n  auth_token = \"invalidAuthToken\"\n  ## You can override the fireboard server URL if necessary\n  # url = https://fireboard.io/api/v1/devices.json\n  ## You can set a different http_timeout if you need to\n  # http_timeout = 4\n```\n\n### auth_token\n\nIn lieu of requiring a username and password, this plugin requires an\nauthentication token that you can generate using the [Fireboard REST API](https://docs.fireboard.io/reference/restapi.html#Authentication).\n\n### url\n\nWhile there should be no reason to override the URL, the option is available\nin case Fireboard changes their site, etc.\n\n### http_timeout\n\nIf you need to increase the HTTP timeout, you can do so here. You can set this\nvalue in seconds. The default value is four (4) seconds.\n","logo":{"light":"/img/library/fireboard.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:fireboard"},{"name":"fluentd","title":"Fluentd","content":"# Fluentd\n\n## Overview\n\nThe fluentd plugin gathers metrics from plugin endpoint provided by [in_monitor plugin](https://docs.fluentd.org/input/monitor_agent).\nThis plugin understands data provided by /api/plugin.json resource (/api/config.json is not covered).\n\nYou might need to adjust your fluentd configuration, in order to reduce series cardinality in case your fluentd restarts frequently. Every time fluentd starts, `plugin_id` value is given a new random value.\nAccording to [fluentd documentation](https://docs.fluentd.org/configuration/config-file#common-plugin-parameter), you are able to add `@id` parameter for each plugin to avoid this behaviour and define custom `plugin_id`.\n\nexample configuration with `@id` parameter for http plugin:\n\n```\n<source>\n  @type http\n  @id http\n  port 8888\n</source>\n```\n\n## Configuration\n\n```toml\n# Read metrics exposed by fluentd in_monitor plugin\n[[inputs.fluentd]]\n  ## This plugin reads information exposed by fluentd (using /api/plugins.json endpoint).\n  ##\n  ## Endpoint:\n  ## - only one URI is allowed\n  ## - https is not supported\n  endpoint = \"http://localhost:24220/api/plugins.json\"\n\n  ## Define which plugins have to be excluded (based on \"type\" field - e.g. monitor_agent)\n  exclude = [\n\t  \"monitor_agent\",\n\t  \"dummy\",\n  ]\n```\n","logo":{"light":"/img/library/fluentd.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:fluentd"},{"name":"ganglia","title":"Ganglia","content":"# Ganglia\n\n## Overview\n\n[Ganglia](http://sourceforge.net/projects/ganglia/) is a scalable distributed monitoring system for high-performance computing systems such as clusters and Grids. It is based on a hierarchical design targeted at federations of clusters.\n\nThe Ganglia Check is a push-style check which allows you to run [Ganglia](http://ganglia.info/)'s gmond on your hosts and send count and timer metrics directly to an Enterprise Broker.\n\n## Metrics\n\nThis check type pushes count and timer metrics to your Circonus Enterprise Broker user .\n","logo":{"light":"/img/library/ganglia.svg"},"attributes":{"implementation":"broker"},"tags":["push","gmond","custom"],"module":"ganglia"},{"name":"gcp-bigquery","title":"GCP BigQuery","content":"# GCP BigQuery\n\n## Overview\n\nThe Circonus GCP BigQuery plugin depends on installation of the Circonus Unified Agent (CUA) for Google Cloud Platform (GCP).\n\nCUA uses the [Cloud Monitoring API v3](https://cloud.google.com/monitoring/api/v3/) to query Google Cloud Monitoring (formerly Stackdriver).\n\nPlease note that this integration accesses APIs which are [chargeable](https://cloud.google.com/stackdriver/pricing#stackdriver_monitoring_services); you might incur costs.\n\n## Configuration\n\n```toml\n[[inputs.stackdriver]]\n  ## Instance ID is required\n  instance_id = \"gcp\"\n\n  ## GCP Project\n  project = \"erudite-bloom-151019\"\n\n  ## Most metrics are updated no more than once per minute; it is recommended\n  ## to override the agent level interval with a value of 1m or greater.\n  interval = \"1m\"\n\n  ## Maximum number of API calls to make per second.  The quota for accounts\n  ## varies, it can be viewed on the API dashboard:\n  ##   https://cloud.google.com/monitoring/quotas#quotas_and_limits\n  # rate_limit = 14\n\n  ## The delay and window options control the number of points selected on\n  ## each gather.  When set, metrics are gathered between:\n  ##   start: now() - delay - window\n  ##   end:   now() - delay\n  #\n  ## Collection delay; if set too low metrics may not yet be available.\n  # delay = \"5m\"\n  #\n  ## If unset, the window will start at 1m and be updated dynamically to span\n  ## the time between calls (approximately the length of the plugin interval).\n  # window = \"1m\"\n\n  ## TTL for cached list of metric types.  This is the maximum amount of time\n  ## it may take to discover new metrics.\n  # cache_ttl = \"1h\"\n\n  ## If true, raw bucket counts are collected for distribution value types.\n  ## For a more lightweight collection, you may wish to disable and use\n  ## distribution_aggregation_aligners instead.\n  # gather_raw_distribution_buckets = true\n\n  ## Aggregate functions to be used for metrics whose value type is\n  ## distribution.  These aggregate values are recorded in in addition to raw\n  ## bucket counts; if they are enabled.\n  ##\n  ## For a list of aligner strings see:\n  ##   https://cloud.google.com/monitoring/api/ref_v3/rpc/google.monitoring.v3#aligner\n  # distribution_aggregation_aligners = [\n  #   \"ALIGN_PERCENTILE_99\",\n  #   \"ALIGN_PERCENTILE_95\",\n  #   \"ALIGN_PERCENTILE_50\",\n  # ]\n\n  ## Filters can be added to reduce the number of time series matched.  All\n  ## functions are supported: starts_with, ends_with, has_substring, and\n  ## one_of.  Only the '=' operator is supported.\n  ##\n  ## The logical operators when combining filters are defined statically using\n  ## the following values:\n  ##   filter ::= <resource_labels> {AND <metric_labels>}\n  ##   resource_labels ::= <resource_labels> {OR <resource_label>}\n  ##   metric_labels ::= <metric_labels> {OR <metric_label>}\n  ##\n  ## For more details, see https://cloud.google.com/monitoring/api/v3/filters\n  #\n  ## Resource labels refine the time series selection with the following expression:\n  ##   resource.labels.<key> = <value>\n  # [[inputs.stackdriver.filter.resource_labels]]\n  #   key = \"instance_name\"\n  #   value = 'starts_with(\"localhost\")'\n  #\n  ## Metric labels refine the time series selection with the following expression:\n  ##   metric.labels.<key> = <value>\n  #  [[inputs.stackdriver.filter.metric_labels]]\n  #    key = \"device_name\"\n  #    value = 'one_of(\"sda\", \"sdb\")'\n```\n\n### Authentication\n\nIt is recommended to use a service account to authenticate with the\nStackdriver Monitoring API. [Getting Started with Authentication](https://cloud.google.com/docs/authentication/getting-started).\n\n## Troubleshooting\n\nWhen agent is ran with `--debug`, detailed information about the performed\nqueries will be logged.\n","logo":{"light":"/img/library/gcp-bigquery.svg"},"attributes":{"implementation":"cua"},"tags":["cloud","agent","google","cloud","platform"],"module":"httptrap:cua:stackdriver_circonus:bigquery.googleapis.com"},{"name":"gcp-cloud-apis","title":"GCP Cloud APIs","content":"# GCP Cloud APIs\n\n## Overview\n\nThe Circonus GCP Cloud APIs plugin depends on installation of the Circonus Unified Agent (CUA) for Google Cloud Platform (GCP).\n\nCUA uses the [Cloud Monitoring API v3](https://cloud.google.com/monitoring/api/v3/) to query Google Cloud Monitoring (formerly Stackdriver).\n\nPlease note that this integration accesses APIs which are [chargeable](https://cloud.google.com/stackdriver/pricing#stackdriver_monitoring_services); you might incur costs.\n\n## Configuration\n\n```toml\n[[inputs.stackdriver]]\n  ## Instance ID is required\n  instance_id = \"gcp\"\n\n  ## GCP Project\n  project = \"erudite-bloom-151019\"\n\n  ## Most metrics are updated no more than once per minute; it is recommended\n  ## to override the agent level interval with a value of 1m or greater.\n  interval = \"1m\"\n\n  ## Maximum number of API calls to make per second.  The quota for accounts\n  ## varies, it can be viewed on the API dashboard:\n  ##   https://cloud.google.com/monitoring/quotas#quotas_and_limits\n  # rate_limit = 14\n\n  ## The delay and window options control the number of points selected on\n  ## each gather.  When set, metrics are gathered between:\n  ##   start: now() - delay - window\n  ##   end:   now() - delay\n  #\n  ## Collection delay; if set too low metrics may not yet be available.\n  # delay = \"5m\"\n  #\n  ## If unset, the window will start at 1m and be updated dynamically to span\n  ## the time between calls (approximately the length of the plugin interval).\n  # window = \"1m\"\n\n  ## TTL for cached list of metric types.  This is the maximum amount of time\n  ## it may take to discover new metrics.\n  # cache_ttl = \"1h\"\n\n  ## If true, raw bucket counts are collected for distribution value types.\n  ## For a more lightweight collection, you may wish to disable and use\n  ## distribution_aggregation_aligners instead.\n  # gather_raw_distribution_buckets = true\n\n  ## Aggregate functions to be used for metrics whose value type is\n  ## distribution.  These aggregate values are recorded in in addition to raw\n  ## bucket counts; if they are enabled.\n  ##\n  ## For a list of aligner strings see:\n  ##   https://cloud.google.com/monitoring/api/ref_v3/rpc/google.monitoring.v3#aligner\n  # distribution_aggregation_aligners = [\n  #   \"ALIGN_PERCENTILE_99\",\n  #   \"ALIGN_PERCENTILE_95\",\n  #   \"ALIGN_PERCENTILE_50\",\n  # ]\n\n  ## Filters can be added to reduce the number of time series matched.  All\n  ## functions are supported: starts_with, ends_with, has_substring, and\n  ## one_of.  Only the '=' operator is supported.\n  ##\n  ## The logical operators when combining filters are defined statically using\n  ## the following values:\n  ##   filter ::= <resource_labels> {AND <metric_labels>}\n  ##   resource_labels ::= <resource_labels> {OR <resource_label>}\n  ##   metric_labels ::= <metric_labels> {OR <metric_label>}\n  ##\n  ## For more details, see https://cloud.google.com/monitoring/api/v3/filters\n  #\n  ## Resource labels refine the time series selection with the following expression:\n  ##   resource.labels.<key> = <value>\n  # [[inputs.stackdriver.filter.resource_labels]]\n  #   key = \"instance_name\"\n  #   value = 'starts_with(\"localhost\")'\n  #\n  ## Metric labels refine the time series selection with the following expression:\n  ##   metric.labels.<key> = <value>\n  #  [[inputs.stackdriver.filter.metric_labels]]\n  #    key = \"device_name\"\n  #    value = 'one_of(\"sda\", \"sdb\")'\n```\n\n### Authentication\n\nIt is recommended to use a service account to authenticate with the\nStackdriver Monitoring API. [Getting Started with Authentication](https://cloud.google.com/docs/authentication/getting-started).\n\n## Troubleshooting\n\nWhen agent is ran with `--debug`, detailed information about the performed\nqueries will be logged.\n","logo":{"light":"/img/library/gcp-cloud-apis.svg"},"attributes":{"implementation":"cua"},"tags":["cloud","agent","google","cloud","platform"],"module":"httptrap:cua:stackdriver_circonus:serviceruntime.googleapis.com"},{"name":"gcp-cloud-armor","title":"GCP Cloud Armor","content":"# GCP Cloud Armor\n\n## Overview\n\nThe Circonus GCP Cloud Armor plugin depends on installation of the Circonus Unified Agent (CUA) for Google Cloud Platform (GCP).\n\nCUA uses the [Cloud Monitoring API v3](https://cloud.google.com/monitoring/api/v3/) to query Google Cloud Monitoring (formerly Stackdriver).\n\nPlease note that this integration accesses APIs which are [chargeable](https://cloud.google.com/stackdriver/pricing#stackdriver_monitoring_services); you might incur costs.\n\n## Configuration\n\n```toml\n[[inputs.stackdriver]]\n  ## Instance ID is required\n  instance_id = \"gcp\"\n\n  ## GCP Project\n  project = \"erudite-bloom-151019\"\n\n  ## Most metrics are updated no more than once per minute; it is recommended\n  ## to override the agent level interval with a value of 1m or greater.\n  interval = \"1m\"\n\n  ## Maximum number of API calls to make per second.  The quota for accounts\n  ## varies, it can be viewed on the API dashboard:\n  ##   https://cloud.google.com/monitoring/quotas#quotas_and_limits\n  # rate_limit = 14\n\n  ## The delay and window options control the number of points selected on\n  ## each gather.  When set, metrics are gathered between:\n  ##   start: now() - delay - window\n  ##   end:   now() - delay\n  #\n  ## Collection delay; if set too low metrics may not yet be available.\n  # delay = \"5m\"\n  #\n  ## If unset, the window will start at 1m and be updated dynamically to span\n  ## the time between calls (approximately the length of the plugin interval).\n  # window = \"1m\"\n\n  ## TTL for cached list of metric types.  This is the maximum amount of time\n  ## it may take to discover new metrics.\n  # cache_ttl = \"1h\"\n\n  ## If true, raw bucket counts are collected for distribution value types.\n  ## For a more lightweight collection, you may wish to disable and use\n  ## distribution_aggregation_aligners instead.\n  # gather_raw_distribution_buckets = true\n\n  ## Aggregate functions to be used for metrics whose value type is\n  ## distribution.  These aggregate values are recorded in in addition to raw\n  ## bucket counts; if they are enabled.\n  ##\n  ## For a list of aligner strings see:\n  ##   https://cloud.google.com/monitoring/api/ref_v3/rpc/google.monitoring.v3#aligner\n  # distribution_aggregation_aligners = [\n  #   \"ALIGN_PERCENTILE_99\",\n  #   \"ALIGN_PERCENTILE_95\",\n  #   \"ALIGN_PERCENTILE_50\",\n  # ]\n\n  ## Filters can be added to reduce the number of time series matched.  All\n  ## functions are supported: starts_with, ends_with, has_substring, and\n  ## one_of.  Only the '=' operator is supported.\n  ##\n  ## The logical operators when combining filters are defined statically using\n  ## the following values:\n  ##   filter ::= <resource_labels> {AND <metric_labels>}\n  ##   resource_labels ::= <resource_labels> {OR <resource_label>}\n  ##   metric_labels ::= <metric_labels> {OR <metric_label>}\n  ##\n  ## For more details, see https://cloud.google.com/monitoring/api/v3/filters\n  #\n  ## Resource labels refine the time series selection with the following expression:\n  ##   resource.labels.<key> = <value>\n  # [[inputs.stackdriver.filter.resource_labels]]\n  #   key = \"instance_name\"\n  #   value = 'starts_with(\"localhost\")'\n  #\n  ## Metric labels refine the time series selection with the following expression:\n  ##   metric.labels.<key> = <value>\n  #  [[inputs.stackdriver.filter.metric_labels]]\n  #    key = \"device_name\"\n  #    value = 'one_of(\"sda\", \"sdb\")'\n```\n\n### Authentication\n\nIt is recommended to use a service account to authenticate with the\nStackdriver Monitoring API. [Getting Started with Authentication](https://cloud.google.com/docs/authentication/getting-started).\n\n## Troubleshooting\n\nWhen agent is ran with `--debug`, detailed information about the performed\nqueries will be logged.\n","logo":{"light":"/img/library/gcp-cloud-armor.svg"},"attributes":{"implementation":"cua"},"tags":["cloud","agent","google","cloud","platform"],"module":"httptrap:cua:stackdriver_circonus:networksecurity.googleapis.com"},{"name":"gcp-cloud-router","title":"GCP Cloud Router","content":"# GCP Cloud Router\n\n## Overview\n\nThe Circonus GCP Cloud Router plugin depends on installation of the Circonus Unified Agent (CUA) for Google Cloud Platform (GCP).\n\nCUA uses the [Cloud Monitoring API v3](https://cloud.google.com/monitoring/api/v3/) to query Google Cloud Monitoring (formerly Stackdriver).\n\nPlease note that this integration accesses APIs which are [chargeable](https://cloud.google.com/stackdriver/pricing#stackdriver_monitoring_services); you might incur costs.\n\n## Configuration\n\n```toml\n[[inputs.stackdriver]]\n  ## Instance ID is required\n  instance_id = \"gcp\"\n\n  ## GCP Project\n  project = \"erudite-bloom-151019\"\n\n  ## Most metrics are updated no more than once per minute; it is recommended\n  ## to override the agent level interval with a value of 1m or greater.\n  interval = \"1m\"\n\n  ## Maximum number of API calls to make per second.  The quota for accounts\n  ## varies, it can be viewed on the API dashboard:\n  ##   https://cloud.google.com/monitoring/quotas#quotas_and_limits\n  # rate_limit = 14\n\n  ## The delay and window options control the number of points selected on\n  ## each gather.  When set, metrics are gathered between:\n  ##   start: now() - delay - window\n  ##   end:   now() - delay\n  #\n  ## Collection delay; if set too low metrics may not yet be available.\n  # delay = \"5m\"\n  #\n  ## If unset, the window will start at 1m and be updated dynamically to span\n  ## the time between calls (approximately the length of the plugin interval).\n  # window = \"1m\"\n\n  ## TTL for cached list of metric types.  This is the maximum amount of time\n  ## it may take to discover new metrics.\n  # cache_ttl = \"1h\"\n\n  ## If true, raw bucket counts are collected for distribution value types.\n  ## For a more lightweight collection, you may wish to disable and use\n  ## distribution_aggregation_aligners instead.\n  # gather_raw_distribution_buckets = true\n\n  ## Aggregate functions to be used for metrics whose value type is\n  ## distribution.  These aggregate values are recorded in in addition to raw\n  ## bucket counts; if they are enabled.\n  ##\n  ## For a list of aligner strings see:\n  ##   https://cloud.google.com/monitoring/api/ref_v3/rpc/google.monitoring.v3#aligner\n  # distribution_aggregation_aligners = [\n  #   \"ALIGN_PERCENTILE_99\",\n  #   \"ALIGN_PERCENTILE_95\",\n  #   \"ALIGN_PERCENTILE_50\",\n  # ]\n\n  ## Filters can be added to reduce the number of time series matched.  All\n  ## functions are supported: starts_with, ends_with, has_substring, and\n  ## one_of.  Only the '=' operator is supported.\n  ##\n  ## The logical operators when combining filters are defined statically using\n  ## the following values:\n  ##   filter ::= <resource_labels> {AND <metric_labels>}\n  ##   resource_labels ::= <resource_labels> {OR <resource_label>}\n  ##   metric_labels ::= <metric_labels> {OR <metric_label>}\n  ##\n  ## For more details, see https://cloud.google.com/monitoring/api/v3/filters\n  #\n  ## Resource labels refine the time series selection with the following expression:\n  ##   resource.labels.<key> = <value>\n  # [[inputs.stackdriver.filter.resource_labels]]\n  #   key = \"instance_name\"\n  #   value = 'starts_with(\"localhost\")'\n  #\n  ## Metric labels refine the time series selection with the following expression:\n  ##   metric.labels.<key> = <value>\n  #  [[inputs.stackdriver.filter.metric_labels]]\n  #    key = \"device_name\"\n  #    value = 'one_of(\"sda\", \"sdb\")'\n```\n\n### Authentication\n\nIt is recommended to use a service account to authenticate with the\nStackdriver Monitoring API. [Getting Started with Authentication](https://cloud.google.com/docs/authentication/getting-started).\n\n## Troubleshooting\n\nWhen agent is ran with `--debug`, detailed information about the performed\nqueries will be logged.\n","logo":{"light":"/img/library/gcp-cloud-router.svg"},"attributes":{"implementation":"cua"},"tags":["cloud","agent","google","cloud","platform"],"module":"httptrap:cua:stackdriver_circonus:router.googleapis.com"},{"name":"gcp-cloud-storage","title":"GCP Cloud Storage","content":"# GCP Cloud Storage\n\n## Overview\n\nThe Circonus GCP Cloud Storage plugin depends on installation of the Circonus Unified Agent (CUA) for Google Cloud Platform (GCP).\n\nCUA uses the [Cloud Monitoring API v3](https://cloud.google.com/monitoring/api/v3/) to query Google Cloud Monitoring (formerly Stackdriver).\n\nPlease note that this integration accesses APIs which are [chargeable](https://cloud.google.com/stackdriver/pricing#stackdriver_monitoring_services); you might incur costs.\n\n## Configuration\n\n```toml\n[[inputs.stackdriver]]\n  ## Instance ID is required\n  instance_id = \"gcp\"\n\n  ## GCP Project\n  project = \"erudite-bloom-151019\"\n\n  ## Most metrics are updated no more than once per minute; it is recommended\n  ## to override the agent level interval with a value of 1m or greater.\n  interval = \"1m\"\n\n  ## Maximum number of API calls to make per second.  The quota for accounts\n  ## varies, it can be viewed on the API dashboard:\n  ##   https://cloud.google.com/monitoring/quotas#quotas_and_limits\n  # rate_limit = 14\n\n  ## The delay and window options control the number of points selected on\n  ## each gather.  When set, metrics are gathered between:\n  ##   start: now() - delay - window\n  ##   end:   now() - delay\n  #\n  ## Collection delay; if set too low metrics may not yet be available.\n  # delay = \"5m\"\n  #\n  ## If unset, the window will start at 1m and be updated dynamically to span\n  ## the time between calls (approximately the length of the plugin interval).\n  # window = \"1m\"\n\n  ## TTL for cached list of metric types.  This is the maximum amount of time\n  ## it may take to discover new metrics.\n  # cache_ttl = \"1h\"\n\n  ## If true, raw bucket counts are collected for distribution value types.\n  ## For a more lightweight collection, you may wish to disable and use\n  ## distribution_aggregation_aligners instead.\n  # gather_raw_distribution_buckets = true\n\n  ## Aggregate functions to be used for metrics whose value type is\n  ## distribution.  These aggregate values are recorded in in addition to raw\n  ## bucket counts; if they are enabled.\n  ##\n  ## For a list of aligner strings see:\n  ##   https://cloud.google.com/monitoring/api/ref_v3/rpc/google.monitoring.v3#aligner\n  # distribution_aggregation_aligners = [\n  #   \"ALIGN_PERCENTILE_99\",\n  #   \"ALIGN_PERCENTILE_95\",\n  #   \"ALIGN_PERCENTILE_50\",\n  # ]\n\n  ## Filters can be added to reduce the number of time series matched.  All\n  ## functions are supported: starts_with, ends_with, has_substring, and\n  ## one_of.  Only the '=' operator is supported.\n  ##\n  ## The logical operators when combining filters are defined statically using\n  ## the following values:\n  ##   filter ::= <resource_labels> {AND <metric_labels>}\n  ##   resource_labels ::= <resource_labels> {OR <resource_label>}\n  ##   metric_labels ::= <metric_labels> {OR <metric_label>}\n  ##\n  ## For more details, see https://cloud.google.com/monitoring/api/v3/filters\n  #\n  ## Resource labels refine the time series selection with the following expression:\n  ##   resource.labels.<key> = <value>\n  # [[inputs.stackdriver.filter.resource_labels]]\n  #   key = \"instance_name\"\n  #   value = 'starts_with(\"localhost\")'\n  #\n  ## Metric labels refine the time series selection with the following expression:\n  ##   metric.labels.<key> = <value>\n  #  [[inputs.stackdriver.filter.metric_labels]]\n  #    key = \"device_name\"\n  #    value = 'one_of(\"sda\", \"sdb\")'\n```\n\n### Authentication\n\nIt is recommended to use a service account to authenticate with the\nStackdriver Monitoring API. [Getting Started with Authentication](https://cloud.google.com/docs/authentication/getting-started).\n\n## Troubleshooting\n\nWhen agent is ran with `--debug`, detailed information about the performed\nqueries will be logged.\n","logo":{"light":"/img/library/gcp-cloud-storage.svg"},"attributes":{"implementation":"cua"},"tags":["cloud","agent","google","cloud","platform"],"module":"httptrap:cua:stackdriver_circonus:storage.googleapis.com"},{"name":"gcp-cloud-vpn","title":"GCP Cloud VPN","content":"# GCP Cloud VPN\n\n## Overview\n\nThe Circonus GCP Cloud VPN plugin depends on installation of the Circonus Unified Agent (CUA) for Google Cloud Platform (GCP).\n\nCUA uses the [Cloud Monitoring API v3](https://cloud.google.com/monitoring/api/v3/) to query Google Cloud Monitoring (formerly Stackdriver).\n\nPlease note that this integration accesses APIs which are [chargeable](https://cloud.google.com/stackdriver/pricing#stackdriver_monitoring_services); you might incur costs.\n\n## Configuration\n\n```toml\n[[inputs.stackdriver]]\n  ## Instance ID is required\n  instance_id = \"gcp\"\n\n  ## GCP Project\n  project = \"erudite-bloom-151019\"\n\n  ## Most metrics are updated no more than once per minute; it is recommended\n  ## to override the agent level interval with a value of 1m or greater.\n  interval = \"1m\"\n\n  ## Maximum number of API calls to make per second.  The quota for accounts\n  ## varies, it can be viewed on the API dashboard:\n  ##   https://cloud.google.com/monitoring/quotas#quotas_and_limits\n  # rate_limit = 14\n\n  ## The delay and window options control the number of points selected on\n  ## each gather.  When set, metrics are gathered between:\n  ##   start: now() - delay - window\n  ##   end:   now() - delay\n  #\n  ## Collection delay; if set too low metrics may not yet be available.\n  # delay = \"5m\"\n  #\n  ## If unset, the window will start at 1m and be updated dynamically to span\n  ## the time between calls (approximately the length of the plugin interval).\n  # window = \"1m\"\n\n  ## TTL for cached list of metric types.  This is the maximum amount of time\n  ## it may take to discover new metrics.\n  # cache_ttl = \"1h\"\n\n  ## If true, raw bucket counts are collected for distribution value types.\n  ## For a more lightweight collection, you may wish to disable and use\n  ## distribution_aggregation_aligners instead.\n  # gather_raw_distribution_buckets = true\n\n  ## Aggregate functions to be used for metrics whose value type is\n  ## distribution.  These aggregate values are recorded in in addition to raw\n  ## bucket counts; if they are enabled.\n  ##\n  ## For a list of aligner strings see:\n  ##   https://cloud.google.com/monitoring/api/ref_v3/rpc/google.monitoring.v3#aligner\n  # distribution_aggregation_aligners = [\n  #   \"ALIGN_PERCENTILE_99\",\n  #   \"ALIGN_PERCENTILE_95\",\n  #   \"ALIGN_PERCENTILE_50\",\n  # ]\n\n  ## Filters can be added to reduce the number of time series matched.  All\n  ## functions are supported: starts_with, ends_with, has_substring, and\n  ## one_of.  Only the '=' operator is supported.\n  ##\n  ## The logical operators when combining filters are defined statically using\n  ## the following values:\n  ##   filter ::= <resource_labels> {AND <metric_labels>}\n  ##   resource_labels ::= <resource_labels> {OR <resource_label>}\n  ##   metric_labels ::= <metric_labels> {OR <metric_label>}\n  ##\n  ## For more details, see https://cloud.google.com/monitoring/api/v3/filters\n  #\n  ## Resource labels refine the time series selection with the following expression:\n  ##   resource.labels.<key> = <value>\n  # [[inputs.stackdriver.filter.resource_labels]]\n  #   key = \"instance_name\"\n  #   value = 'starts_with(\"localhost\")'\n  #\n  ## Metric labels refine the time series selection with the following expression:\n  ##   metric.labels.<key> = <value>\n  #  [[inputs.stackdriver.filter.metric_labels]]\n  #    key = \"device_name\"\n  #    value = 'one_of(\"sda\", \"sdb\")'\n```\n\n### Authentication\n\nIt is recommended to use a service account to authenticate with the\nStackdriver Monitoring API. [Getting Started with Authentication](https://cloud.google.com/docs/authentication/getting-started).\n\n## Troubleshooting\n\nWhen agent is ran with `--debug`, detailed information about the performed\nqueries will be logged.\n","logo":{"light":"/img/library/gcp-cloud-vpn.svg"},"attributes":{"implementation":"cua"},"tags":["cloud","agent","google","cloud","platform"],"module":"httptrap:cua:stackdriver_circonus:vpn.googleapis.com"},{"name":"gcp-compute-engine","title":"GCP Compute Engine","content":"# GCP Compute Engine\n\n## Overview\n\nThe Circonus GCP Compute Engine plugin depends on installation of the Circonus Unified Agent (CUA) for Google Cloud Platform (GCP).\n\nCUA uses the [Cloud Monitoring API v3](https://cloud.google.com/monitoring/api/v3/) to query Google Cloud Monitoring (formerly Stackdriver).\n\nPlease note that this integration accesses APIs which are [chargeable](https://cloud.google.com/stackdriver/pricing#stackdriver_monitoring_services); you might incur costs.\n\n## Configuration\n\n```toml\n[[inputs.stackdriver]]\n  ## Instance ID is required\n  instance_id = \"gcp\"\n\n  ## GCP Project\n  project = \"erudite-bloom-151019\"\n\n  ## Most metrics are updated no more than once per minute; it is recommended\n  ## to override the agent level interval with a value of 1m or greater.\n  interval = \"1m\"\n\n  ## Maximum number of API calls to make per second.  The quota for accounts\n  ## varies, it can be viewed on the API dashboard:\n  ##   https://cloud.google.com/monitoring/quotas#quotas_and_limits\n  # rate_limit = 14\n\n  ## The delay and window options control the number of points selected on\n  ## each gather.  When set, metrics are gathered between:\n  ##   start: now() - delay - window\n  ##   end:   now() - delay\n  #\n  ## Collection delay; if set too low metrics may not yet be available.\n  # delay = \"5m\"\n  #\n  ## If unset, the window will start at 1m and be updated dynamically to span\n  ## the time between calls (approximately the length of the plugin interval).\n  # window = \"1m\"\n\n  ## TTL for cached list of metric types.  This is the maximum amount of time\n  ## it may take to discover new metrics.\n  # cache_ttl = \"1h\"\n\n  ## If true, raw bucket counts are collected for distribution value types.\n  ## For a more lightweight collection, you may wish to disable and use\n  ## distribution_aggregation_aligners instead.\n  # gather_raw_distribution_buckets = true\n\n  ## Aggregate functions to be used for metrics whose value type is\n  ## distribution.  These aggregate values are recorded in in addition to raw\n  ## bucket counts; if they are enabled.\n  ##\n  ## For a list of aligner strings see:\n  ##   https://cloud.google.com/monitoring/api/ref_v3/rpc/google.monitoring.v3#aligner\n  # distribution_aggregation_aligners = [\n  #   \"ALIGN_PERCENTILE_99\",\n  #   \"ALIGN_PERCENTILE_95\",\n  #   \"ALIGN_PERCENTILE_50\",\n  # ]\n\n  ## Filters can be added to reduce the number of time series matched.  All\n  ## functions are supported: starts_with, ends_with, has_substring, and\n  ## one_of.  Only the '=' operator is supported.\n  ##\n  ## The logical operators when combining filters are defined statically using\n  ## the following values:\n  ##   filter ::= <resource_labels> {AND <metric_labels>}\n  ##   resource_labels ::= <resource_labels> {OR <resource_label>}\n  ##   metric_labels ::= <metric_labels> {OR <metric_label>}\n  ##\n  ## For more details, see https://cloud.google.com/monitoring/api/v3/filters\n  #\n  ## Resource labels refine the time series selection with the following expression:\n  ##   resource.labels.<key> = <value>\n  # [[inputs.stackdriver.filter.resource_labels]]\n  #   key = \"instance_name\"\n  #   value = 'starts_with(\"localhost\")'\n  #\n  ## Metric labels refine the time series selection with the following expression:\n  ##   metric.labels.<key> = <value>\n  #  [[inputs.stackdriver.filter.metric_labels]]\n  #    key = \"device_name\"\n  #    value = 'one_of(\"sda\", \"sdb\")'\n```\n\n### Authentication\n\nIt is recommended to use a service account to authenticate with the\nStackdriver Monitoring API. [Getting Started with Authentication](https://cloud.google.com/docs/authentication/getting-started).\n\n## Troubleshooting\n\nWhen agent is ran with `--debug`, detailed information about the performed\nqueries will be logged.\n","logo":{"light":"/img/library/gcp-compute-engine.svg"},"attributes":{"implementation":"cua"},"tags":["cloud","agent","google","cloud","platform"],"module":"httptrap:cua:stackdriver_circonus:compute.googleapis.com"},{"name":"gcp-firewall-insights","title":"GCP Firewall Insights","content":"# GCP Firewall Insights\n\n## Overview\n\nThe Circonus GCP Firewall Insights plugin depends on installation of the Circonus Unified Agent (CUA) for Google Cloud Platform (GCP).\n\nCUA uses the [Cloud Monitoring API v3](https://cloud.google.com/monitoring/api/v3/) to query Google Cloud Monitoring (formerly Stackdriver).\n\nPlease note that this integration accesses APIs which are [chargeable](https://cloud.google.com/stackdriver/pricing#stackdriver_monitoring_services); you might incur costs.\n\n## Configuration\n\n```toml\n[[inputs.stackdriver]]\n  ## Instance ID is required\n  instance_id = \"gcp\"\n\n  ## GCP Project\n  project = \"erudite-bloom-151019\"\n\n  ## Most metrics are updated no more than once per minute; it is recommended\n  ## to override the agent level interval with a value of 1m or greater.\n  interval = \"1m\"\n\n  ## Maximum number of API calls to make per second.  The quota for accounts\n  ## varies, it can be viewed on the API dashboard:\n  ##   https://cloud.google.com/monitoring/quotas#quotas_and_limits\n  # rate_limit = 14\n\n  ## The delay and window options control the number of points selected on\n  ## each gather.  When set, metrics are gathered between:\n  ##   start: now() - delay - window\n  ##   end:   now() - delay\n  #\n  ## Collection delay; if set too low metrics may not yet be available.\n  # delay = \"5m\"\n  #\n  ## If unset, the window will start at 1m and be updated dynamically to span\n  ## the time between calls (approximately the length of the plugin interval).\n  # window = \"1m\"\n\n  ## TTL for cached list of metric types.  This is the maximum amount of time\n  ## it may take to discover new metrics.\n  # cache_ttl = \"1h\"\n\n  ## If true, raw bucket counts are collected for distribution value types.\n  ## For a more lightweight collection, you may wish to disable and use\n  ## distribution_aggregation_aligners instead.\n  # gather_raw_distribution_buckets = true\n\n  ## Aggregate functions to be used for metrics whose value type is\n  ## distribution.  These aggregate values are recorded in in addition to raw\n  ## bucket counts; if they are enabled.\n  ##\n  ## For a list of aligner strings see:\n  ##   https://cloud.google.com/monitoring/api/ref_v3/rpc/google.monitoring.v3#aligner\n  # distribution_aggregation_aligners = [\n  #   \"ALIGN_PERCENTILE_99\",\n  #   \"ALIGN_PERCENTILE_95\",\n  #   \"ALIGN_PERCENTILE_50\",\n  # ]\n\n  ## Filters can be added to reduce the number of time series matched.  All\n  ## functions are supported: starts_with, ends_with, has_substring, and\n  ## one_of.  Only the '=' operator is supported.\n  ##\n  ## The logical operators when combining filters are defined statically using\n  ## the following values:\n  ##   filter ::= <resource_labels> {AND <metric_labels>}\n  ##   resource_labels ::= <resource_labels> {OR <resource_label>}\n  ##   metric_labels ::= <metric_labels> {OR <metric_label>}\n  ##\n  ## For more details, see https://cloud.google.com/monitoring/api/v3/filters\n  #\n  ## Resource labels refine the time series selection with the following expression:\n  ##   resource.labels.<key> = <value>\n  # [[inputs.stackdriver.filter.resource_labels]]\n  #   key = \"instance_name\"\n  #   value = 'starts_with(\"localhost\")'\n  #\n  ## Metric labels refine the time series selection with the following expression:\n  ##   metric.labels.<key> = <value>\n  #  [[inputs.stackdriver.filter.metric_labels]]\n  #    key = \"device_name\"\n  #    value = 'one_of(\"sda\", \"sdb\")'\n```\n\n### Authentication\n\nIt is recommended to use a service account to authenticate with the\nStackdriver Monitoring API. [Getting Started with Authentication](https://cloud.google.com/docs/authentication/getting-started).\n\n## Troubleshooting\n\nWhen agent is ran with `--debug`, detailed information about the performed\nqueries will be logged.\n","logo":{"light":"/img/library/gcp-firewall-insights.svg"},"attributes":{"implementation":"cua"},"tags":["cloud","agent","google","cloud","platform"],"module":"httptrap:cua:stackdriver_circonus:firewallinsights.googleapis.com"},{"name":"gcp-load-balancing","title":"GCP Load Balancing","content":"# GCP Load Balancing\n\n## Overview\n\nThe Circonus GCP Load Balancing plugin depends on installation of the Circonus Unified Agent (CUA) for Google Cloud Platform (GCP).\n\nCUA uses the [Cloud Monitoring API v3](https://cloud.google.com/monitoring/api/v3/) to query Google Cloud Monitoring (formerly Stackdriver).\n\nPlease note that this integration accesses APIs which are [chargeable](https://cloud.google.com/stackdriver/pricing#stackdriver_monitoring_services); you might incur costs.\n\n## Configuration\n\n```toml\n[[inputs.stackdriver]]\n  ## Instance ID is required\n  instance_id = \"gcp\"\n\n  ## GCP Project\n  project = \"erudite-bloom-151019\"\n\n  ## Most metrics are updated no more than once per minute; it is recommended\n  ## to override the agent level interval with a value of 1m or greater.\n  interval = \"1m\"\n\n  ## Maximum number of API calls to make per second.  The quota for accounts\n  ## varies, it can be viewed on the API dashboard:\n  ##   https://cloud.google.com/monitoring/quotas#quotas_and_limits\n  # rate_limit = 14\n\n  ## The delay and window options control the number of points selected on\n  ## each gather.  When set, metrics are gathered between:\n  ##   start: now() - delay - window\n  ##   end:   now() - delay\n  #\n  ## Collection delay; if set too low metrics may not yet be available.\n  # delay = \"5m\"\n  #\n  ## If unset, the window will start at 1m and be updated dynamically to span\n  ## the time between calls (approximately the length of the plugin interval).\n  # window = \"1m\"\n\n  ## TTL for cached list of metric types.  This is the maximum amount of time\n  ## it may take to discover new metrics.\n  # cache_ttl = \"1h\"\n\n  ## If true, raw bucket counts are collected for distribution value types.\n  ## For a more lightweight collection, you may wish to disable and use\n  ## distribution_aggregation_aligners instead.\n  # gather_raw_distribution_buckets = true\n\n  ## Aggregate functions to be used for metrics whose value type is\n  ## distribution.  These aggregate values are recorded in in addition to raw\n  ## bucket counts; if they are enabled.\n  ##\n  ## For a list of aligner strings see:\n  ##   https://cloud.google.com/monitoring/api/ref_v3/rpc/google.monitoring.v3#aligner\n  # distribution_aggregation_aligners = [\n  #   \"ALIGN_PERCENTILE_99\",\n  #   \"ALIGN_PERCENTILE_95\",\n  #   \"ALIGN_PERCENTILE_50\",\n  # ]\n\n  ## Filters can be added to reduce the number of time series matched.  All\n  ## functions are supported: starts_with, ends_with, has_substring, and\n  ## one_of.  Only the '=' operator is supported.\n  ##\n  ## The logical operators when combining filters are defined statically using\n  ## the following values:\n  ##   filter ::= <resource_labels> {AND <metric_labels>}\n  ##   resource_labels ::= <resource_labels> {OR <resource_label>}\n  ##   metric_labels ::= <metric_labels> {OR <metric_label>}\n  ##\n  ## For more details, see https://cloud.google.com/monitoring/api/v3/filters\n  #\n  ## Resource labels refine the time series selection with the following expression:\n  ##   resource.labels.<key> = <value>\n  # [[inputs.stackdriver.filter.resource_labels]]\n  #   key = \"instance_name\"\n  #   value = 'starts_with(\"localhost\")'\n  #\n  ## Metric labels refine the time series selection with the following expression:\n  ##   metric.labels.<key> = <value>\n  #  [[inputs.stackdriver.filter.metric_labels]]\n  #    key = \"device_name\"\n  #    value = 'one_of(\"sda\", \"sdb\")'\n```\n\n### Authentication\n\nIt is recommended to use a service account to authenticate with the\nStackdriver Monitoring API. [Getting Started with Authentication](https://cloud.google.com/docs/authentication/getting-started).\n\n## Troubleshooting\n\nWhen agent is ran with `--debug`, detailed information about the performed\nqueries will be logged.\n","logo":{"light":"/img/library/gcp-load-balancing.svg"},"attributes":{"implementation":"cua"},"tags":["cloud","agent","google","cloud","platform"],"module":"httptrap:cua:stackdriver_circonus:loadbalancing.googleapis.com"},{"name":"gcp-logging","title":"GCP Logging","content":"# GCP Logging\n\n## Overview\n\nThe Circonus GCP Logging plugin depends on installation of the Circonus Unified Agent (CUA) for Google Cloud Platform (GCP).\n\nCUA uses the [Cloud Monitoring API v3](https://cloud.google.com/monitoring/api/v3/) to query Google Cloud Monitoring (formerly Stackdriver).\n\nPlease note that this integration accesses APIs which are [chargeable](https://cloud.google.com/stackdriver/pricing#stackdriver_monitoring_services); you might incur costs.\n\n## Configuration\n\n```toml\n[[inputs.stackdriver]]\n  ## Instance ID is required\n  instance_id = \"gcp\"\n\n  ## GCP Project\n  project = \"erudite-bloom-151019\"\n\n  ## Most metrics are updated no more than once per minute; it is recommended\n  ## to override the agent level interval with a value of 1m or greater.\n  interval = \"1m\"\n\n  ## Maximum number of API calls to make per second.  The quota for accounts\n  ## varies, it can be viewed on the API dashboard:\n  ##   https://cloud.google.com/monitoring/quotas#quotas_and_limits\n  # rate_limit = 14\n\n  ## The delay and window options control the number of points selected on\n  ## each gather.  When set, metrics are gathered between:\n  ##   start: now() - delay - window\n  ##   end:   now() - delay\n  #\n  ## Collection delay; if set too low metrics may not yet be available.\n  # delay = \"5m\"\n  #\n  ## If unset, the window will start at 1m and be updated dynamically to span\n  ## the time between calls (approximately the length of the plugin interval).\n  # window = \"1m\"\n\n  ## TTL for cached list of metric types.  This is the maximum amount of time\n  ## it may take to discover new metrics.\n  # cache_ttl = \"1h\"\n\n  ## If true, raw bucket counts are collected for distribution value types.\n  ## For a more lightweight collection, you may wish to disable and use\n  ## distribution_aggregation_aligners instead.\n  # gather_raw_distribution_buckets = true\n\n  ## Aggregate functions to be used for metrics whose value type is\n  ## distribution.  These aggregate values are recorded in in addition to raw\n  ## bucket counts; if they are enabled.\n  ##\n  ## For a list of aligner strings see:\n  ##   https://cloud.google.com/monitoring/api/ref_v3/rpc/google.monitoring.v3#aligner\n  # distribution_aggregation_aligners = [\n  #   \"ALIGN_PERCENTILE_99\",\n  #   \"ALIGN_PERCENTILE_95\",\n  #   \"ALIGN_PERCENTILE_50\",\n  # ]\n\n  ## Filters can be added to reduce the number of time series matched.  All\n  ## functions are supported: starts_with, ends_with, has_substring, and\n  ## one_of.  Only the '=' operator is supported.\n  ##\n  ## The logical operators when combining filters are defined statically using\n  ## the following values:\n  ##   filter ::= <resource_labels> {AND <metric_labels>}\n  ##   resource_labels ::= <resource_labels> {OR <resource_label>}\n  ##   metric_labels ::= <metric_labels> {OR <metric_label>}\n  ##\n  ## For more details, see https://cloud.google.com/monitoring/api/v3/filters\n  #\n  ## Resource labels refine the time series selection with the following expression:\n  ##   resource.labels.<key> = <value>\n  # [[inputs.stackdriver.filter.resource_labels]]\n  #   key = \"instance_name\"\n  #   value = 'starts_with(\"localhost\")'\n  #\n  ## Metric labels refine the time series selection with the following expression:\n  ##   metric.labels.<key> = <value>\n  #  [[inputs.stackdriver.filter.metric_labels]]\n  #    key = \"device_name\"\n  #    value = 'one_of(\"sda\", \"sdb\")'\n```\n\n### Authentication\n\nIt is recommended to use a service account to authenticate with the\nStackdriver Monitoring API. [Getting Started with Authentication](https://cloud.google.com/docs/authentication/getting-started).\n\n## Troubleshooting\n\nWhen agent is ran with `--debug`, detailed information about the performed\nqueries will be logged.\n","logo":{"light":"/img/library/gcp-logging.svg"},"attributes":{"implementation":"cua"},"tags":["cloud","agent","google","cloud","platform"],"module":"httptrap:cua:stackdriver_circonus:logging.googleapis.com"},{"name":"gcp-network-topology","title":"GCP Network Topology","content":"# GCP Network Topology\n\n## Overview\n\nThe Circonus GCP Network Topology plugin depends on installation of the Circonus Unified Agent (CUA) for Google Cloud Platform (GCP).\n\nCUA uses the [Cloud Monitoring API v3](https://cloud.google.com/monitoring/api/v3/) to query Google Cloud Monitoring (formerly Stackdriver).\n\nPlease note that this integration accesses APIs which are [chargeable](https://cloud.google.com/stackdriver/pricing#stackdriver_monitoring_services); you might incur costs.\n\n## Configuration\n\n```toml\n[[inputs.stackdriver]]\n  ## Instance ID is required\n  instance_id = \"gcp\"\n\n  ## GCP Project\n  project = \"erudite-bloom-151019\"\n\n  ## Most metrics are updated no more than once per minute; it is recommended\n  ## to override the agent level interval with a value of 1m or greater.\n  interval = \"1m\"\n\n  ## Maximum number of API calls to make per second.  The quota for accounts\n  ## varies, it can be viewed on the API dashboard:\n  ##   https://cloud.google.com/monitoring/quotas#quotas_and_limits\n  # rate_limit = 14\n\n  ## The delay and window options control the number of points selected on\n  ## each gather.  When set, metrics are gathered between:\n  ##   start: now() - delay - window\n  ##   end:   now() - delay\n  #\n  ## Collection delay; if set too low metrics may not yet be available.\n  # delay = \"5m\"\n  #\n  ## If unset, the window will start at 1m and be updated dynamically to span\n  ## the time between calls (approximately the length of the plugin interval).\n  # window = \"1m\"\n\n  ## TTL for cached list of metric types.  This is the maximum amount of time\n  ## it may take to discover new metrics.\n  # cache_ttl = \"1h\"\n\n  ## If true, raw bucket counts are collected for distribution value types.\n  ## For a more lightweight collection, you may wish to disable and use\n  ## distribution_aggregation_aligners instead.\n  # gather_raw_distribution_buckets = true\n\n  ## Aggregate functions to be used for metrics whose value type is\n  ## distribution.  These aggregate values are recorded in in addition to raw\n  ## bucket counts; if they are enabled.\n  ##\n  ## For a list of aligner strings see:\n  ##   https://cloud.google.com/monitoring/api/ref_v3/rpc/google.monitoring.v3#aligner\n  # distribution_aggregation_aligners = [\n  #   \"ALIGN_PERCENTILE_99\",\n  #   \"ALIGN_PERCENTILE_95\",\n  #   \"ALIGN_PERCENTILE_50\",\n  # ]\n\n  ## Filters can be added to reduce the number of time series matched.  All\n  ## functions are supported: starts_with, ends_with, has_substring, and\n  ## one_of.  Only the '=' operator is supported.\n  ##\n  ## The logical operators when combining filters are defined statically using\n  ## the following values:\n  ##   filter ::= <resource_labels> {AND <metric_labels>}\n  ##   resource_labels ::= <resource_labels> {OR <resource_label>}\n  ##   metric_labels ::= <metric_labels> {OR <metric_label>}\n  ##\n  ## For more details, see https://cloud.google.com/monitoring/api/v3/filters\n  #\n  ## Resource labels refine the time series selection with the following expression:\n  ##   resource.labels.<key> = <value>\n  # [[inputs.stackdriver.filter.resource_labels]]\n  #   key = \"instance_name\"\n  #   value = 'starts_with(\"localhost\")'\n  #\n  ## Metric labels refine the time series selection with the following expression:\n  ##   metric.labels.<key> = <value>\n  #  [[inputs.stackdriver.filter.metric_labels]]\n  #    key = \"device_name\"\n  #    value = 'one_of(\"sda\", \"sdb\")'\n```\n\n### Authentication\n\nIt is recommended to use a service account to authenticate with the\nStackdriver Monitoring API. [Getting Started with Authentication](https://cloud.google.com/docs/authentication/getting-started).\n\n## Troubleshooting\n\nWhen agent is ran with `--debug`, detailed information about the performed\nqueries will be logged.\n","logo":{"light":"/img/library/gcp-network-topology.svg"},"attributes":{"implementation":"cua"},"tags":["cloud","agent","google","cloud","platform"],"module":"httptrap:cua:stackdriver_circonus:networking.googleapis.com"},{"name":"github","title":"GitHub","content":"# GitHub\n\n## Overview\n\nGather repository information from [GitHub](https://www.github.com/) hosted repositories.\n\n**Note:** The agent also contains the [webhook](https://github.com/circonus-labs/circonus-unified-agent/blob/master/plugins/inputs/webhooks/github) input which can be used as an\nalternative method for collecting repository information.\n\n## Configuration\n\n```toml\n[[inputs.github]]\n  ## List of repositories to monitor\n  repositories = [\n\t  \"circonus-labs/circonus-unified-agent\",\n\t  \"circonus-labs/circonus-kubernetes-agent\"\n  ]\n\n  ## Github API access token.  Unauthenticated requests are limited to 60 per hour.\n  # access_token = \"\"\n\n  ## Github API enterprise url. Github Enterprise accounts must specify their base url.\n  # enterprise_base_url = \"\"\n\n  ## Timeout for HTTP requests.\n  # http_timeout = \"5s\"\n```\n","logo":{"light":"/img/library/github.svg","dark":"/img/library/github-dark.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:github"},{"name":"gnmi","title":"gNMI","content":"# gNMI\n\n## Overview\n\nThis plugin consumes telemetry data based on the [gNMI](https://github.com/openconfig/reference/blob/master/rpc/gnmi/gnmi-specification.md) (gRPC Network Management Interface) Subscribe method. TLS is supported for authentication and encryption. This input plugin is vendor-agnostic and is supported on any platform that supports the gNMI spec.\n\nFor Cisco devices:\nIt has been optimized to support gNMI telemetry as produced by Cisco IOS XR (64-bit) version 6.5.1, Cisco NX-OS 9.3 and Cisco IOS XE 16.12 and later.\n\n## Configuration\n\n```toml\n[[inputs.gnmi]]\n  ## Address and port of the gNMI GRPC server\n  addresses = [\"10.49.234.114:57777\"]\n\n  ## define credentials\n  username = \"cisco\"\n  password = \"cisco\"\n\n  ## gNMI encoding requested (one of: \"proto\", \"json\", \"json_ietf\", \"bytes\")\n  # encoding = \"proto\"\n\n  ## redial in case of failures after\n  redial = \"10s\"\n\n  ## enable client-side TLS and define CA to authenticate the device\n  # enable_tls = true\n  # tls_ca = \"/etc/circonus-unified-agent/ca.pem\"\n  # insecure_skip_verify = true\n\n  ## define client-side TLS certificate & key to authenticate to the device\n  # tls_cert = \"/etc/circonus-unified-agent/cert.pem\"\n  # tls_key = \"/etc/circonus-unified-agent/key.pem\"\n\n  ## gNMI subscription prefix (optional, can usually be left empty)\n  ## See: https://github.com/openconfig/reference/blob/master/rpc/gnmi/gnmi-specification.md#222-paths\n  # origin = \"\"\n  # prefix = \"\"\n  # target = \"\"\n\n  ## Define additional aliases to map telemetry encoding paths to simple measurement names\n  # [inputs.gnmi.aliases]\n  #   ifcounters = \"openconfig:/interfaces/interface/state/counters\"\n\n  [[inputs.gnmi.subscription]]\n    ## Name of the measurement that will be emitted\n    name = \"ifcounters\"\n\n    ## Origin and path of the subscription\n    ## See: https://github.com/openconfig/reference/blob/master/rpc/gnmi/gnmi-specification.md#222-paths\n    ##\n    ## origin usually refers to a (YANG) data model implemented by the device\n    ## and path to a specific substructure inside it that should be subscribed to (similar to an XPath)\n    ## YANG models can be found e.g. here: https://github.com/YangModels/yang/tree/master/vendor/cisco/xr\n    origin = \"openconfig-interfaces\"\n    path = \"/interfaces/interface/state/counters\"\n\n    # Subscription mode (one of: \"target_defined\", \"sample\", \"on_change\") and interval\n    subscription_mode = \"sample\"\n    sample_interval = \"10s\"\n\n    ## Suppress redundant transmissions when measured values are unchanged\n    # suppress_redundant = false\n\n    ## If suppression is enabled, send updates at least every X seconds anyway\n    # heartbeat_interval = \"60s\"\n```\n","logo":{"light":"/img/library/grpc.svg","dark":"/img/library/grpc-dark.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:gnmi"},{"name":"google-cloud-platform","title":"Google Cloud Platform","content":"# Google Cloud Platform\n\n## Overview\n\nUses predefined metric prefixes to support the Circonus Stackdriver Dashboard(s).\n\nQuery data from Google Cloud Monitoring (formerly Stackdriver) using the\n[Cloud Monitoring API v3](https://cloud.google.com/monitoring/api/v3/).\n\nThis plugin accesses APIs which are [chargeable](https://cloud.google.com/stackdriver/pricing#stackdriver_monitoring_services); you might incur\ncosts.\n\n## Configuration\n\n```toml\n[[inputs.stackdriver]]\n  ## Instance ID is required\n  instance_id = \"gcp\"\n\n  ## GCP Project\n  project = \"erudite-bloom-151019\"\n\n  ## Most metrics are updated no more than once per minute; it is recommended\n  ## to override the agent level interval with a value of 1m or greater.\n  interval = \"1m\"\n\n  ## Maximum number of API calls to make per second.  The quota for accounts\n  ## varies, it can be viewed on the API dashboard:\n  ##   https://cloud.google.com/monitoring/quotas#quotas_and_limits\n  # rate_limit = 14\n\n  ## The delay and window options control the number of points selected on\n  ## each gather.  When set, metrics are gathered between:\n  ##   start: now() - delay - window\n  ##   end:   now() - delay\n  #\n  ## Collection delay; if set too low metrics may not yet be available.\n  # delay = \"5m\"\n  #\n  ## If unset, the window will start at 1m and be updated dynamically to span\n  ## the time between calls (approximately the length of the plugin interval).\n  # window = \"1m\"\n\n  ## TTL for cached list of metric types.  This is the maximum amount of time\n  ## it may take to discover new metrics.\n  # cache_ttl = \"1h\"\n\n  ## If true, raw bucket counts are collected for distribution value types.\n  ## For a more lightweight collection, you may wish to disable and use\n  ## distribution_aggregation_aligners instead.\n  # gather_raw_distribution_buckets = true\n\n  ## Aggregate functions to be used for metrics whose value type is\n  ## distribution.  These aggregate values are recorded in in addition to raw\n  ## bucket counts; if they are enabled.\n  ##\n  ## For a list of aligner strings see:\n  ##   https://cloud.google.com/monitoring/api/ref_v3/rpc/google.monitoring.v3#aligner\n  # distribution_aggregation_aligners = [\n  #   \"ALIGN_PERCENTILE_99\",\n  #   \"ALIGN_PERCENTILE_95\",\n  #   \"ALIGN_PERCENTILE_50\",\n  # ]\n\n  ## Filters can be added to reduce the number of time series matched.  All\n  ## functions are supported: starts_with, ends_with, has_substring, and\n  ## one_of.  Only the '=' operator is supported.\n  ##\n  ## The logical operators when combining filters are defined statically using\n  ## the following values:\n  ##   filter ::= <resource_labels> {AND <metric_labels>}\n  ##   resource_labels ::= <resource_labels> {OR <resource_label>}\n  ##   metric_labels ::= <metric_labels> {OR <metric_label>}\n  ##\n  ## For more details, see https://cloud.google.com/monitoring/api/v3/filters\n  #\n  ## Resource labels refine the time series selection with the following expression:\n  ##   resource.labels.<key> = <value>\n  # [[inputs.stackdriver.filter.resource_labels]]\n  #   key = \"instance_name\"\n  #   value = 'starts_with(\"localhost\")'\n  #\n  ## Metric labels refine the time series selection with the following expression:\n  ##   metric.labels.<key> = <value>\n  #  [[inputs.stackdriver.filter.metric_labels]]\n  #    key = \"device_name\"\n  #    value = 'one_of(\"sda\", \"sdb\")'\n```\n\n### Authentication\n\nIt is recommended to use a service account to authenticate with the\nStackdriver Monitoring API. [Getting Started with Authentication](https://cloud.google.com/docs/authentication/getting-started).\n\n## Troubleshooting\n\nWhen agent is ran with `--debug`, detailed information about the performed\nqueries will be logged.\n","logo":{"light":"/img/library/google-cloud-platform.svg"},"attributes":{"implementation":"cua"},"tags":["cloud","agent","google","cloud","platform"],"module":"httptrap:cua:stackdriver_circonus"},{"name":"graphite","title":"Graphite","content":"# Graphite\n\n## Overview\n\nGraphite is an enterprise-ready monitoring tool designed to run on commodity hardware. Teams use Graphite to aggregate metrics regarding the performance of their websites, applications, business services, and networked servers.\n\nThe Graphite check allow you to forward your Graphite ingested metrics directly to Circonus, enabling long-term storage and advanced analytics.\n\n## Configuration\n\nThere are three different Graphite modules:\n\n- **Plain**: Plain and simple Graphite metric ingestion; must be run on an Enterprise Broker.\n\n- **Pickle**: Ingest Graphite metrics using the Pickle format; must be run on an Enterprise Broker.\n\n- **TLS**: Ingest Graphite metrics over a connection protected by TLS; may be run on a public Circonus Broker.\n","logo":{"light":"/img/library/graphite.png"},"attributes":{"implementation":"broker"},"tags":["carbon","graphite","whisper"],"module":"graphite"},{"name":"graylog","title":"GrayLog","content":"# GrayLog\n\n## Overview\n\nThe Graylog plugin can collect data from remote Graylog service URLs.\n\nPlugin currently support two type of end points:-\n\n- multiple (Ex `http://[graylog-server-ip]:12900/system/metrics/multiple`)\n- namespace (Ex `http://[graylog-server-ip]:12900/system/metrics/namespace/{namespace}`)\n\nEnd Point can be a mix of one multiple end point and several namespaces end points\n\nNote: if namespace end point specified metrics array will be ignored for that call.\n\n## Configuration\n\n```toml\n# Read flattened metrics from one or more GrayLog HTTP endpoints\n[[inputs.graylog]]\n  ## API endpoint, currently supported API:\n  ##\n  ##   - multiple  (Ex http://<host>:12900/system/metrics/multiple)\n  ##   - namespace (Ex http://<host>:12900/system/metrics/namespace/{namespace})\n  ##\n  ## For namespace endpoint, the metrics array will be ignored for that call.\n  ## Endpoint can contain namespace and multiple type calls.\n  ##\n  ## Please check http://[graylog-server-ip]:12900/api-browser for full list\n  ## of endpoints\n  servers = [\n    \"http://[graylog-server-ip]:12900/system/metrics/multiple\",\n  ]\n\n  ## Metrics list\n  ## List of metrics can be found on Graylog webservice documentation.\n  ## Or by hitting the web service api at:\n  ##   http://[graylog-host]:12900/system/metrics\n  metrics = [\n    \"jvm.cl.loaded\",\n    \"jvm.memory.pools.Metaspace.committed\"\n  ]\n\n  ## Username and password\n  username = \"\"\n  password = \"\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/circonus-unified-agent/ca.pem\"\n  # tls_cert = \"/etc/circonus-unified-agent/cert.pem\"\n  # tls_key = \"/etc/circonus-unified-agent/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n\nPlease refer to GrayLog metrics api browser for full metric end points: [http://host:12900/api-browser](http://host:12900/api-browser)\n","logo":{"light":"/img/library/graylog.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:graylog"},{"name":"haproxy-legacy","title":"HAProxy","content":"# HAProxy\n\n## Overview\n\nHAProxy, which stands for High Availability Proxy, is an open source, high performance TCP/HTTP load balancer and proxy server that spreads requests across multiple backend servers.\n\nThe HAProxy Check monitors your HAProxy load-balancers.\n\n## Configuration\n\nThe only required parameter is the stats URI, typically /admin?stats;csv to pull management stats in comma-separated-value format.\n\nOptional parameters:\n| Name | Description |\n|----------|-------------|\n| host | The Host header value to include in the HTTP request. |\n| port | TCP port to which the broker should connect (default: 80) |\n| select |A regular expression to choose which metrics to report (default is to report all metrics found). The match is against the pxname and svname columns concatenated with a comma. |\n| auth_user | The user to authenticate as. |\n| auth_password | The password to use during authentication. |\n| use_ssl | Upgrade the TCP connection to use SSL/TLS (default: false) |\n","logo":{"light":"/img/library/haproxy.svg"},"attributes":{"implementation":"broker","legacy":true},"tags":["proxy","accelerator","load","balancer","balancing"],"module":"haproxy"},{"name":"haproxy","title":"HAProxy","content":"# HAProxy\n\n## Overview\n\nThe [HAProxy](http://www.haproxy.org/) input plugin gathers\n[statistics](https://cbonte.github.io/haproxy-dconv/1.9/intro.html#3.3.16)\nusing the [stats socket](https://cbonte.github.io/haproxy-dconv/1.9/management.html#9.3)\nor [HTTP statistics page](https://cbonte.github.io/haproxy-dconv/1.9/management.html#9) of a HAProxy server.\n\n## Configuration\n\n```toml\n# Read metrics of HAProxy, via socket or HTTP stats page\n[[inputs.haproxy]]\n  ## An array of address to gather stats about. Specify an ip on hostname\n  ## with optional port. ie localhost, 10.10.3.33:1936, etc.\n  ## Make sure you specify the complete path to the stats endpoint\n  ## including the protocol, ie http://10.10.3.33:1936/haproxy?stats\n\n  ## Credentials for basic HTTP authentication\n  # username = \"admin\"\n  # password = \"admin\"\n\n  ## If no servers are specified, then default to 127.0.0.1:1936/haproxy?stats\n  servers = [\"http://myhaproxy.com:1936/haproxy?stats\"]\n\n  ## You can also use local socket with standard wildcard globbing.\n  ## Server address not starting with 'http' will be treated as a possible\n  ## socket, so both examples below are valid.\n  # servers = [\"socket:/run/haproxy/admin.sock\", \"/run/haproxy/*.sock\"]\n\n  ## By default, some of the fields are renamed from what haproxy calls them.\n  ## Setting this option to true results in the plugin keeping the original\n  ## field names.\n  # keep_field_names = false\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/circonus-unified-agent/ca.pem\"\n  # tls_cert = \"/etc/circonus-unified-agent/cert.pem\"\n  # tls_key = \"/etc/circonus-unified-agent/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n\n### HAProxy Configuration\n\nThe following information may be useful when getting started, but please\nconsult the HAProxy documentation for complete and up to date instructions.\n\nThe [`stats enable`](https://cbonte.github.io/haproxy-dconv/1.8/configuration.html#4-stats%20enable)\noption can be used to add unauthenticated access over HTTP using the default\nsettings. To enable the unix socket begin by reading about the\n[`stats socket`](https://cbonte.github.io/haproxy-dconv/1.8/configuration.html#3.1-stats%20socket)\noption.\n\n### servers\n\nServer addresses must explicitly start with 'http' if you wish to use HAProxy\nstatus page. Otherwise, addresses will be assumed to be an UNIX socket and\nany protocol (if present) will be discarded.\n\nWhen using socket names, wildcard expansion is supported so plugin can gather\nstats from multiple sockets at once.\n\nTo use HTTP Basic Auth add the username and password in the userinfo section\nof the URL: `http://user:password@1.2.3.4/haproxy?stats`. The credentials are\nsent via the `Authorization` header and not using the request URL.\n\n### keep_field_names\n\nBy default, some of the fields are renamed from what haproxy calls them.\nSetting the `keep_field_names` parameter to `true` will result in the plugin\nkeeping the original field names.\n\nThe following renames are made:\n\n- `pxname` -> `proxy`\n- `svname` -> `sv`\n- `act` -> `active_servers`\n- `bck` -> `backup_servers`\n- `cli_abrt` -> `cli_abort`\n- `srv_abrt` -> `srv_abort`\n- `hrsp_1xx` -> `http_response.1xx`\n- `hrsp_2xx` -> `http_response.2xx`\n- `hrsp_3xx` -> `http_response.3xx`\n- `hrsp_4xx` -> `http_response.4xx`\n- `hrsp_5xx` -> `http_response.5xx`\n- `hrsp_other` -> `http_response.other`\n","logo":{"light":"/img/library/haproxy.svg"},"attributes":{"implementation":"cua"},"tags":["proxy","accelerator","load","balancer","balancing"],"module":"httptrap:cua:haproxy"},{"name":"hashicorp-consul-legacy","title":"Hashicorp Consul","content":"# Hashicorp Consul\n\n## Overview\n\nThe Hashicorp Consul Check collects metrics from Consul nodes. Consul makes available a range of metrics in various formats in order to measure the health and stability of a cluster, and to diagnose or predict potential issues.\n\nHashicorp Consul is a distributed service mesh to connect, secure, and configure services across any runtime platform and public or private cloud.\n\n## Configuration\n\nThe only required parameter is the URL, made up of the target host and URI path. The URI path defaults to `/v1/health/state/any` to return all checks in all states, but it can be set to a number of other values. See the [Consul Health HTTP endpoint documentation](https://www.consul.io/api/health.html) for details.\n\nOptional parameters:\n\n| Name                 | Description                                                    |\n| -------------------- | -------------------------------------------------------------- |\n| port                 | The TCP port used to connect to the Consul API (default: 8500) |\n| check_name           | A check name to use as a filter.                               |\n| service_name         | A service name to use as a filter.                             |\n| consul_dc            | A datacenter name to use as a filter.                          |\n| service_blacklist    | A comma-separated list of service names to skip.               |\n| node_blacklist       | A comma-separated list of node names to skip.                  |\n| check_name_blacklist | A comma-separated list of check names to skip.                 |\n","logo":{"light":"/img/library/hashicorp-consul.svg"},"attributes":{"implementation":"broker","legacy":true},"tags":["hashi","health","service","node","state"],"module":"consul"},{"name":"hashicorp-consul","title":"Hashicorp Consul","content":"# Hashicorp Consul\n\n## Overview\n\nThis plugin will collect statistics about all health checks registered in the Consul. It uses [Consul API](https://www.consul.io/docs/agent/http/health.html#health_state) to query the data. It will not report the [telemetry](https://www.consul.io/docs/agent/telemetry.html) but Consul can report those stats already using StatsD protocol if needed.\n\n## Configuration\n\n```toml\n# Gather health check statuses from services registered in Consul\n[[inputs.consul]]\n  ## Consul server address\n  # address = \"localhost:8500\"\n\n  ## URI scheme for the Consul server, one of \"http\", \"https\"\n  # scheme = \"http\"\n\n  ## Metric version controls the mapping from Consul metrics into\n  ## circonus metrics. Version 2 moved all fields with string values\n  ## to tags.\n  ##\n  ##   example: metric_version = 1; deprecated in 1.16\n  ##            metric_version = 2; recommended version\n  # metric_version = 1\n\n  ## ACL token used in every request\n  # token = \"\"\n\n  ## HTTP Basic Authentication username and password.\n  # username = \"\"\n  # password = \"\"\n\n  ## Data center to query the health checks from\n  # datacenter = \"\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/circonus-unified-agent/ca.pem\"\n  # tls_cert = \"/etc/circonus-unified-agent/cert.pem\"\n  # tls_key = \"/etc/circonus-unified-agent/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = true\n\n  ## Consul checks' tag splitting\n  # When tags are formatted like \"key:value\" with \":\" as a delimiter then\n  # they will be split and reported as proper key:value in metric streams\n  # tag_delimiter = \":\"\n```\n","logo":{"light":"/img/library/hashicorp-consul.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:consul"},{"name":"http-json","title":"HTTP JSON","content":"# HTTP JSON\n\n## Overview\n\nThis input plugin provides the ability to fetch [Circonus HTTPTrap stream tag and structured format metrics](/circonus/integrations/library/json-push-httptrap/#httptrap-json-format) and forward them to a Circonus Unified Agent check.\n\n## Configuration\n\n[IRONdb](https://docs.circonus.com/irondb/administration/monitoring/#json) example:\n\n```toml\n[[inputs.circ_http_json]]\n  instance_id = \"idb_stats\"\n  url = \"http://127.0.0.1:8112/stats.json?format=tagged\"\n\n[[inputs.circ_http_json]]\n  instance_id = \"idb_mtev\"\n  url = \"http://127.0.0.1:8112/mtev/stats.json?format=tagged\"\n```\n\nNote the addition of `?format=tagged` use for these endpoints to ensure stream tagged, structured metric format.\n","logo":{"light":"/img/library/json.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:circ_http_json"},{"name":"http-legacy","title":"HTTP","content":"# HTTP\n\n## Overview\n\nThe HTTP Check monitors the status of your local or remote HTTP endpoints by issuing HTTP requests and collecting HTTP response data like response code, time and size.\n\n## Configuration\n\nThe only required parameter is the URL to check, including scheme and hostname (as you would type into a browser's location bar.)\n\nOptional parameters:\n|Name|Description|\n|----|-----------|\n|method|The HTTP method to use (default: GET)|\n|http_version|The HTTP protocol version to use (default: 1.1)|\n|header name/value|Include an arbitrary header in the HTTP request.|\n|payload|Information sent as the payload of the HTTP request.|\n|auth_method|HTTP authentication method to use (default: None)|\n|auth_user|The user to authenticate as.|\n|auth_password|The password to use during authentication.|\n|redirects|The maximum number of Location header redirects to follow (default: 0)|\n\nIf server authorization is necessary for this resource, you will need to enter the server authorization information (Auth Username, Auth Password, Auth Method) under \"Advanced Configuration\". You can also enter a payload to send and copy and paste a CA Certificate under \"Advanced Configuration\".\n\n## Metrics\n\nTypical metrics include:\n|Name|Type|Description|\n|----|----|-----------|\n|bytes|numeric|Total bytes received.|\n|code|text|Response code received.|\n|duration|numeric|Total request duration, in milliseconds.|\n|truncated|numeric|Response, truncated.|\n|tt_connect|numeric|Time to connect, in milliseconds.|\n|tt_firstbyte|numeric|Time to receive first byte, in milliseconds.|\n\nIf SSL/TLS is enabled, the following additional metrics are returned:\n|Name|Type|Description|\n|----|----|-----------|\n|cert_end|numeric|The Unix epoch time representing the expiration date of the TLS certificate.|\n|cert_end_in|numeric|The number of seconds between now (as measured at the Circonus broker) and the cert_end value.|\n|cert_error|text|Text of any certificate validation error(s), or null if no errors.|\n|cert_issuer|text|The subject of the issuer's certificate, typically a Certificate Authority (CA) certificate.|\n|cert_start|numeric|The Unix epoch time representing the validity start date of the TLS certificate.|\n|cert_subject|text|The subject of the server's TLS certificate.|\n|cert_subject_alternative_names|text|A list of any X509v3 Subject Alternative Names (SAN) that the TLS certificate protects.|\n","logo":{"light":"/img/library/http.svg"},"attributes":{"implementation":"broker","legacy":true},"tags":["protocol","website","performance","availability","hypertext"],"module":"http"},{"name":"http","title":"HTTP","content":"# HTTP\n\n## Overview\n\nThe HTTP input plugin collects metrics from one or more HTTP(S) endpoints. The endpoint should have metrics formatted in one of the supported [input data formats](https://github.com/circonus-labs/circonus-unified-agent/blob/master/docs/DATA_FORMATS_INPUT.md). Each data format has its own unique set of configuration options which can be added to the input configuration.\n\n## Configuration\n\n```toml\n# Read formatted metrics from one or more HTTP endpoints\n[[inputs.http]]\n  ## One or more URLs from which to read formatted metrics\n  urls = [\n    \"http://localhost/metrics\"\n  ]\n\n  ## HTTP method\n  # method = \"GET\"\n\n  ## Optional HTTP headers\n  # headers = {\"X-Special-Header\" = \"Special-Value\"}\n\n  ## HTTP entity-body to send with POST/PUT requests.\n  # body = \"\"\n\n  ## HTTP Content-Encoding for write request body, can be set to \"gzip\" to\n  ## compress body or \"identity\" to apply no encoding.\n  # content_encoding = \"identity\"\n\n  ## Optional file with Bearer token\n  ## file content is added as an Authorization header\n  # bearer_token = \"/path/to/file\"\n\n  ## Optional HTTP Basic Auth Credentials\n  # username = \"username\"\n  # password = \"pa$$word\"\n\n  ## HTTP Proxy support\n  # http_proxy_url = \"\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/circonus-unified-agent/ca.pem\"\n  # tls_cert = \"/etc/circonus-unified-agent/cert.pem\"\n  # tls_key = \"/etc/circonus-unified-agent/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n\n  ## Amount of time allowed to complete the HTTP request\n  # timeout = \"5s\"\n\n  ## List of success status codes\n  # success_status_codes = [200]\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/circonus-labs/circonus-unified-agent/blob/master/docs/DATA_FORMATS_INPUT.md\n  # data_format = \"json\"\n\n```\n","logo":{"light":"/img/library/http.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:http"},{"name":"imap","title":"IMAP","content":"# IMAP\n\n## Overview\n\nThe IMAP Check checks mail retrieval over the Internet Message Access Protocol (IMAP).\n\n## Configuration\n\nAdvanced Configuration allows you to specify the folder that should be examined, an optional IMAP SEARCH operation to execute after EXAMINE. You can also specify which Folder to examine.\n\nRequired parameters:\n\n| Name          | Description                                  |\n| ------------- | -------------------------------------------- |\n| port          | The TCP port to connect to the IMAP service. |\n| auth_user     | The IMAP user to use for authentication.     |\n| auth_password | The IMAP password to use for authentication. |\n\nOptional parameters:\n\n| Name    | Description                                                                                                                   |\n| ------- | ----------------------------------------------------------------------------------------------------------------------------- |\n| folder  | The folder to be examined (default: INBOX)                                                                                    |\n| search  | Specify an IMAP SEARCH operation to execute after EXAMINE.                                                                    |\n| fetch   | Perform a FETCH operation on the highest message ID, or the last SEARCH result if a search is configured (default: false/off) |\n| use_ssl | Upgrade the TCP connection to use SSL/TLS (default: false/off)                                                                |\n\nIf SSL is used, you can also specify a list of ciphers to be used in the SSL protocol and the expected certificate name to check for in SSL certificates.\n","logo":{"light":"/img/library/imap.svg"},"attributes":{"implementation":"broker"},"tags":["email","protocol","messaging"],"module":"imap"},{"name":"index","title":"Library","content":"import DocCardList from \"@theme/DocCardList\";\nimport {\n  useCurrentSidebarCategory,\n  filterDocCardListItems,\n} from \"@docusaurus/theme-common\";\n\n<DocCardList />\n","logo":{},"attributes":{}},{"name":"intel-rdt","title":"Intel RDT","content":"# Intel RDT\n\n## Overview\n\nThe `intel_rdt` plugin collects information provided by monitoring features of\nthe Intel Resource Director Technology (Intel(R) RDT). Intel RDT provides the hardware framework to monitor\nand control the utilization of shared resources (ex: last level cache, memory bandwidth).\n\n### About Intel RDT\n\nIntel’s Resource Director Technology (RDT) framework consists of:\n\n- Cache Monitoring Technology (CMT)\n- Memory Bandwidth Monitoring (MBM)\n- Cache Allocation Technology (CAT)\n- Code and Data Prioritization (CDP)\n\nAs multithreaded and multicore platform architectures emerge, the last level cache and\nmemory bandwidth are key resources to manage for running workloads in single-threaded,\nmultithreaded, or complex virtual machine environments. Intel introduces CMT, MBM, CAT\nand CDP to manage these workloads across shared resources.\n\n### Prerequsities - PQoS Tool\n\nTo gather Intel RDT metrics, the `intel_rdt` plugin uses _pqos_ cli tool which is a\npart of [Intel(R) RDT Software Package](https://github.com/intel/intel-cmt-cat).\nBefore using this plugin please be sure _pqos_ is properly installed and configured regarding that the plugin\nrun _pqos_ to work with `OS Interface` mode. This plugin supports _pqos_ version 4.0.0 and above.\nNote: pqos tool needs root privileges to work properly.\n\nMetrics will be constantly reported from the following `pqos` commands within the given interval:\n\n#### In case of cores monitoring:\n\n```\npqos -r --iface-os --mon-file-type=csv --mon-interval=INTERVAL --mon-core=all:[CORES]\\;mbt:[CORES]\n```\n\nwhere `CORES` is equal to group of cores provided in config. User can provide many groups.\n\n#### In case of process monitoring:\n\n```\npqos -r --iface-os --mon-file-type=csv --mon-interval=INTERVAL --mon-pid=all:[PIDS]\\;mbt:[PIDS]\n```\n\nwhere `PIDS` is group of processes IDs which name are equal to provided process name in a config.\nUser can provide many process names which lead to create many processes groups.\n\nIn both cases `INTERVAL` is equal to sampling_interval from config.\n\nBecause PIDs association within system could change in every moment, Intel RDT plugin provides a\nfunctionality to check on every interval if desired processes change their PIDs association.\nIf some change is reported, plugin will restart _pqos_ tool with new arguments. If provided by user\nprocess name is not equal to any of available processes, will be omitted and plugin will constantly\ncheck for process availability.\n\n### Useful links\n\nPqos installation process: https://github.com/intel/intel-cmt-cat/blob/master/INSTALL  \nEnabling OS interface: https://github.com/intel/intel-cmt-cat/wiki, https://github.com/intel/intel-cmt-cat/wiki/resctrl  \nMore about Intel RDT: https://www.intel.com/content/www/us/en/architecture-and-technology/resource-director-technology.html\n\n## Configuration\n\n```toml\n# Read Intel RDT metrics\n[[inputs.intel_rdt]]\n  ## Optionally set sampling interval to Nx100ms.\n  ## This value is propagated to pqos tool. Interval format is defined by pqos itself.\n  ## If not provided or provided 0, will be set to 10 = 10x100ms = 1s.\n  # sampling_interval = \"10\"\n\n  ## Optionally specify the path to pqos executable.\n  ## If not provided, auto discovery will be performed.\n  # pqos_path = \"/usr/local/bin/pqos\"\n\n  ## Optionally specify if IPC and LLC_Misses metrics shouldn't be propagated.\n  ## If not provided, default value is false.\n  # shortened_metrics = false\n\n  ## Specify the list of groups of CPU core(s) to be provided as pqos input.\n  ## Mandatory if processes aren't set and forbidden if processes are specified.\n  ## e.g. [\"0-3\", \"4,5,6\"] or [\"1-3,4\"]\n  # cores = [\"0-3\"]\n\n  ## Specify the list of processes for which Metrics will be collected.\n  ## Mandatory if cores aren't set and forbidden if cores are specified.\n  ## e.g. [\"qemu\", \"pmd\"]\n  # processes = [\"process\"]\n```\n","logo":{"light":"/img/library/intel.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:intel_rdt"},{"name":"jenkins","title":"Jenkins","content":"# Jenkins\n\n## Overview\n\nThe jenkins plugin gathers information about the nodes and jobs running in a jenkins instance.\n\nThis plugin does not require a plugin on jenkins and it makes use of Jenkins API to retrieve all the information needed.\n\n## Configuration\n\n```toml\n[[inputs.jenkins]]\n  ## The Jenkins URL in the format \"schema://host:port\"\n  url = \"http://my-jenkins-instance:8080\"\n  # username = \"admin\"\n  # password = \"admin\"\n\n  ## Set response_timeout\n  response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/circonus-unified-agent/ca.pem\"\n  # tls_cert = \"/etc/circonus-unified-agent/cert.pem\"\n  # tls_key = \"/etc/circonus-unified-agent/key.pem\"\n  ## Use SSL but skip chain & host verification\n  # insecure_skip_verify = false\n\n  ## Optional Max Job Build Age filter\n  ## Default 1 hour, ignore builds older than max_build_age\n  # max_build_age = \"1h\"\n\n  ## Optional Sub Job Depth filter\n  ## Jenkins can have unlimited layer of sub jobs\n  ## This config will limit the layers of pulling, default value 0 means\n  ## unlimited pulling until no more sub jobs\n  # max_subjob_depth = 0\n\n  ## Optional Sub Job Per Layer\n  ## In workflow-multibranch-plugin, each branch will be created as a sub job.\n  ## This config will limit to call only the lasted branches in each layer,\n  ## empty will use default value 10\n  # max_subjob_per_layer = 10\n\n  ## Jobs to exclude from gathering\n  # job_exclude = [ \"job1\", \"job2/subjob1/subjob2\", \"job3/*\"]\n\n  ## Nodes to exclude from gathering\n  # node_exclude = [ \"node1\", \"node2\" ]\n\n  ## Worker pool for jenkins plugin only\n  ## Empty this field will use default value 5\n  # max_connections = 5\n```\n","logo":{"light":"/img/library/jenkins.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:jenkins"},{"name":"jmx","title":"JMX","content":"# JMX\n\n## Overview\n\nThe JMX Check monitors a JMX-enabled service. Circonus queries the JMX Application and pulls all available metrics, including those available as MBeans (such as Tomcat and JBoss).\n\nJava Management Extensions (JMX) is an architecture to manage resources dynamically at runtime. JMX is used mostly in enterprise applications to make the system configurable or to get the state of application at any point of time.\n\n## Configuration\n\nTo set up a JMX check, select a host and destination port between 0 and 65535. Advanced Configuration allows you to set server authorization information and specify MBean domains.\n","logo":{"light":"/img/library/java.svg"},"attributes":{"implementation":"broker"},"tags":["Java","MBeans"],"module":"jmx"},{"name":"jolokia","title":"Jolokia","content":"# Jolokia\n\n## Overview\n\nThe [Jolokia](http://jolokia.org) _agent_ and _proxy_ input plugins collect JMX metrics from an HTTP endpoint using Jolokia's [JSON-over-HTTP protocol](https://jolokia.org/reference/html/protocol.html).\n\n## Configuration\n\n### Jolokia Agent Configuration\n\nThe `jolokia2_agent` input plugin reads JMX metrics from one or more [Jolokia agent](https://jolokia.org/agent/jvm.html) REST endpoints.\n\n```toml\n[[inputs.jolokia2_agent]]\n  urls = [\"http://agent:8080/jolokia\"]\n\n  [[inputs.jolokia2_agent.metric]]\n    name  = \"jvm_runtime\"\n    mbean = \"java.lang:type=Runtime\"\n    paths = [\"Uptime\"]\n```\n\nOptionally, specify TLS options for communicating with agents:\n\n```toml\n[[inputs.jolokia2_agent]]\n  urls = [\"https://agent:8080/jolokia\"]\n  tls_ca   = \"/var/private/ca.pem\"\n  tls_cert = \"/var/private/client.pem\"\n  tls_key  = \"/var/private/client-key.pem\"\n  #insecure_skip_verify = false\n\n  [[inputs.jolokia2_agent.metric]]\n    name  = \"jvm_runtime\"\n    mbean = \"java.lang:type=Runtime\"\n    paths = [\"Uptime\"]\n```\n\n### Jolokia Proxy Configuration\n\nThe `jolokia2_proxy` input plugin reads JMX metrics from one or more _targets_ by interacting with a [Jolokia proxy](https://jolokia.org/features/proxy.html) REST endpoint.\n\n```toml\n[[inputs.jolokia2_proxy]]\n  url = \"http://proxy:8080/jolokia\"\n\n  #default_target_username = \"\"\n  #default_target_password = \"\"\n  [[inputs.jolokia2_proxy.target]]\n    url = \"service:jmx:rmi:///jndi/rmi://targethost:9999/jmxrmi\"\n    # username = \"\"\n    # password = \"\"\n\n  [[inputs.jolokia2_proxy.metric]]\n    name  = \"jvm_runtime\"\n    mbean = \"java.lang:type=Runtime\"\n    paths = [\"Uptime\"]\n```\n\nOptionally, specify TLS options for communicating with proxies:\n\n```toml\n[[inputs.jolokia2_proxy]]\n  url = \"https://proxy:8080/jolokia\"\n\n  tls_ca   = \"/var/private/ca.pem\"\n  tls_cert = \"/var/private/client.pem\"\n  tls_key  = \"/var/private/client-key.pem\"\n  #insecure_skip_verify = false\n\n  #default_target_username = \"\"\n  #default_target_password = \"\"\n  [[inputs.jolokia2_proxy.target]]\n    url = \"service:jmx:rmi:///jndi/rmi://targethost:9999/jmxrmi\"\n    # username = \"\"\n    # password = \"\"\n\n  [[inputs.jolokia2_agent.metric]]\n    name  = \"jvm_runtime\"\n    mbean = \"java.lang:type=Runtime\"\n    paths = [\"Uptime\"]\n```\n\n### Jolokia Metric Configuration\n\nEach `metric` declaration generates a Jolokia request to fetch telemetry from a JMX MBean.\n\n| Key            | Required | Description                                                                                                                                              |\n| -------------- | -------- | -------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `mbean`        | yes      | The object name of a JMX MBean. MBean property-key values can contain a wildcard `*`, allowing you to fetch multiple MBeans with one declaration.        |\n| `paths`        | no       | A list of MBean attributes to read.                                                                                                                      |\n| `tag_keys`     | no       | A list of MBean property-key names to convert into tags. The property-key name becomes the tag name, while the property-key value becomes the tag value. |\n| `tag_prefix`   | no       | A string to prepend to the tag names produced by this `metric` declaration.                                                                              |\n| `field_name`   | no       | A string to set as the name of the field produced by this metric; can contain substitutions.                                                             |\n| `field_prefix` | no       | A string to prepend to the field names produced by this `metric` declaration; can contain substitutions.                                                 |\n\nUse `paths` to refine which fields to collect.\n\n```toml\n[[inputs.jolokia2_agent.metric]]\n  name  = \"jvm_memory\"\n  mbean = \"java.lang:type=Memory\"\n  paths = [\"HeapMemoryUsage\", \"NonHeapMemoryUsage\", \"ObjectPendingFinalizationCount\"]\n```\n\nThe preceeding `jvm_memory` `metric` declaration produces the following output:\n\n```\njvm_memory HeapMemoryUsage.committed=4294967296,HeapMemoryUsage.init=4294967296,HeapMemoryUsage.max=4294967296,HeapMemoryUsage.used=1750658992,NonHeapMemoryUsage.committed=67350528,NonHeapMemoryUsage.init=2555904,NonHeapMemoryUsage.max=-1,NonHeapMemoryUsage.used=65821352,ObjectPendingFinalizationCount=0 1503762436000000000\n```\n\nUse `*` wildcards against `mbean` property-key values to create distinct series by capturing values into `tag_keys`.\n\n```toml\n[[inputs.jolokia2_agent.metric]]\n  name     = \"jvm_garbage_collector\"\n  mbean    = \"java.lang:name=*,type=GarbageCollector\"\n  paths    = [\"CollectionTime\", \"CollectionCount\"]\n  tag_keys = [\"name\"]\n```\n\nSince `name=*` matches both `G1 Old Generation` and `G1 Young Generation`, and `name` is used as a tag, the preceeding `jvm_garbage_collector` `metric` declaration produces two metrics.\n\n```\njvm_garbage_collector,name=G1\\ Old\\ Generation CollectionCount=0,CollectionTime=0 1503762520000000000\njvm_garbage_collector,name=G1\\ Young\\ Generation CollectionTime=32,CollectionCount=2 1503762520000000000\n```\n\nUse `tag_prefix` along with `tag_keys` to add detail to tag names.\n\n```toml\n[[inputs.jolokia2_agent.metric]]\n  name       = \"jvm_memory_pool\"\n  mbean      = \"java.lang:name=*,type=MemoryPool\"\n  paths      = [\"Usage\", \"PeakUsage\", \"CollectionUsage\"]\n  tag_keys   = [\"name\"]\n  tag_prefix = \"pool_\"\n```\n\nThe preceeding `jvm_memory_pool` `metric` declaration produces six metrics, each with a distinct `pool_name` tag.\n\n```\njvm_memory_pool,pool_name=Compressed\\ Class\\ Space PeakUsage.max=1073741824,PeakUsage.committed=3145728,PeakUsage.init=0,Usage.committed=3145728,Usage.init=0,PeakUsage.used=3017976,Usage.max=1073741824,Usage.used=3017976 1503764025000000000\njvm_memory_pool,pool_name=Code\\ Cache PeakUsage.init=2555904,PeakUsage.committed=6291456,Usage.committed=6291456,PeakUsage.used=6202752,PeakUsage.max=251658240,Usage.used=6210368,Usage.max=251658240,Usage.init=2555904 1503764025000000000\njvm_memory_pool,pool_name=G1\\ Eden\\ Space CollectionUsage.max=-1,PeakUsage.committed=56623104,PeakUsage.init=56623104,PeakUsage.used=53477376,Usage.max=-1,Usage.committed=49283072,Usage.used=19922944,CollectionUsage.committed=49283072,CollectionUsage.init=56623104,CollectionUsage.used=0,PeakUsage.max=-1,Usage.init=56623104 1503764025000000000\njvm_memory_pool,pool_name=G1\\ Old\\ Gen CollectionUsage.max=1073741824,CollectionUsage.committed=0,PeakUsage.max=1073741824,PeakUsage.committed=1017118720,PeakUsage.init=1017118720,PeakUsage.used=137032208,Usage.max=1073741824,CollectionUsage.init=1017118720,Usage.committed=1017118720,Usage.init=1017118720,Usage.used=134708752,CollectionUsage.used=0 1503764025000000000\njvm_memory_pool,pool_name=G1\\ Survivor\\ Space Usage.max=-1,Usage.init=0,CollectionUsage.max=-1,CollectionUsage.committed=7340032,CollectionUsage.used=7340032,PeakUsage.committed=7340032,Usage.committed=7340032,Usage.used=7340032,CollectionUsage.init=0,PeakUsage.max=-1,PeakUsage.init=0,PeakUsage.used=7340032 1503764025000000000\njvm_memory_pool,pool_name=Metaspace PeakUsage.init=0,PeakUsage.used=21852224,PeakUsage.max=-1,Usage.max=-1,Usage.committed=22282240,Usage.init=0,Usage.used=21852224,PeakUsage.committed=22282240 1503764025000000000\n```\n\nUse substitutions to create fields and field prefixes with MBean property-keys captured by wildcards. In the following example, `$1` represents the value of the property-key `name`, and `$2` represents the value of the property-key `topic`.\n\n```toml\n[[inputs.jolokia2_agent.metric]]\n  name         = \"kafka_topic\"\n  mbean        = \"kafka.server:name=*,topic=*,type=BrokerTopicMetrics\"\n  field_prefix = \"$1\"\n  tag_keys     = [\"topic\"]\n```\n\nThe preceeding `kafka_topic` `metric` declaration produces a metric per Kafka topic. The `name` Mbean property-key is used as a field prefix to aid in gathering fields together into the single metric.\n\n```\nkafka_topic,topic=my-topic BytesOutPerSec.MeanRate=0,FailedProduceRequestsPerSec.MeanRate=0,BytesOutPerSec.EventType=\"bytes\",BytesRejectedPerSec.Count=0,FailedProduceRequestsPerSec.RateUnit=\"SECONDS\",FailedProduceRequestsPerSec.EventType=\"requests\",MessagesInPerSec.RateUnit=\"SECONDS\",BytesInPerSec.EventType=\"bytes\",BytesOutPerSec.RateUnit=\"SECONDS\",BytesInPerSec.OneMinuteRate=0,FailedFetchRequestsPerSec.EventType=\"requests\",TotalFetchRequestsPerSec.MeanRate=146.301533938701,BytesOutPerSec.FifteenMinuteRate=0,TotalProduceRequestsPerSec.MeanRate=0,BytesRejectedPerSec.FifteenMinuteRate=0,MessagesInPerSec.FiveMinuteRate=0,BytesInPerSec.Count=0,BytesRejectedPerSec.MeanRate=0,FailedFetchRequestsPerSec.MeanRate=0,FailedFetchRequestsPerSec.FiveMinuteRate=0,FailedFetchRequestsPerSec.FifteenMinuteRate=0,FailedProduceRequestsPerSec.Count=0,TotalFetchRequestsPerSec.FifteenMinuteRate=128.59314292334466,TotalFetchRequestsPerSec.OneMinuteRate=126.71551273850747,TotalFetchRequestsPerSec.Count=1353483,TotalProduceRequestsPerSec.FifteenMinuteRate=0,FailedFetchRequestsPerSec.OneMinuteRate=0,FailedFetchRequestsPerSec.Count=0,FailedProduceRequestsPerSec.FifteenMinuteRate=0,TotalFetchRequestsPerSec.FiveMinuteRate=130.8516148751592,TotalFetchRequestsPerSec.RateUnit=\"SECONDS\",BytesRejectedPerSec.RateUnit=\"SECONDS\",BytesInPerSec.MeanRate=0,FailedFetchRequestsPerSec.RateUnit=\"SECONDS\",BytesRejectedPerSec.OneMinuteRate=0,BytesOutPerSec.Count=0,BytesOutPerSec.OneMinuteRate=0,MessagesInPerSec.FifteenMinuteRate=0,MessagesInPerSec.MeanRate=0,BytesInPerSec.FiveMinuteRate=0,TotalProduceRequestsPerSec.RateUnit=\"SECONDS\",FailedProduceRequestsPerSec.OneMinuteRate=0,TotalProduceRequestsPerSec.EventType=\"requests\",BytesRejectedPerSec.FiveMinuteRate=0,BytesRejectedPerSec.EventType=\"bytes\",BytesOutPerSec.FiveMinuteRate=0,FailedProduceRequestsPerSec.FiveMinuteRate=0,MessagesInPerSec.Count=0,TotalProduceRequestsPerSec.FiveMinuteRate=0,TotalProduceRequestsPerSec.OneMinuteRate=0,MessagesInPerSec.EventType=\"messages\",MessagesInPerSec.OneMinuteRate=0,TotalFetchRequestsPerSec.EventType=\"requests\",BytesInPerSec.RateUnit=\"SECONDS\",BytesInPerSec.FifteenMinuteRate=0,TotalProduceRequestsPerSec.Count=0 1503767532000000000\n```\n\nBoth `jolokia2_agent` and `jolokia2_proxy` plugins support default configurations that apply to every `metric` declaration.\n\n| Key                       | Default Value | Description                                                                   |\n| ------------------------- | ------------- | ----------------------------------------------------------------------------- |\n| `default_field_separator` | `.`           | A character to use to join Mbean attributes when creating fields.             |\n| `default_field_prefix`    | _None_        | A string to prepend to the field names produced by all `metric` declarations. |\n| `default_tag_prefix`      | _None_        | A string to prepend to the tag names produced by all `metric` declarations.   |\n\n### Example Configurations\n\n- [ActiveMQ](https://github.com/circonus-labs/circonus-unified-agent/tree/master/plugins/inputs/jolokia2/examples/activemq.conf)\n- [BitBucket](https://github.com/circonus-labs/circonus-unified-agent/tree/master/plugins/inputs/jolokia2/examples/bitbucket.conf)\n- [Cassandra](https://github.com/circonus-labs/circonus-unified-agent/tree/master/plugins/inputs/jolokia2/examples/cassandra.conf)\n- [Hadoop-HDFS](https://github.com/circonus-labs/circonus-unified-agent/tree/master/plugins/inputs/jolokia2/examples/hadoop-hdfs.conf)\n- [Java JVM](https://github.com/circonus-labs/circonus-unified-agent/tree/master/plugins/inputs/jolokia2/examples/java.conf)\n- [JBoss](https://github.com/circonus-labs/circonus-unified-agent/tree/master/plugins/inputs/jolokia2/examples/jboss.conf)\n- [Kafka](https://github.com/circonus-labs/circonus-unified-agent/tree/master/plugins/inputs/jolokia2/examples/kafka.conf)\n- [Tomcat](https://github.com/circonus-labs/circonus-unified-agent/tree/master/plugins/inputs/jolokia2/examples/tomcat.conf)\n- [Weblogic](https://github.com/circonus-labs/circonus-unified-agent/tree/master/plugins/inputs/jolokia2/examples/weblogic.conf)\n- [ZooKeeper](https://github.com/circonus-labs/circonus-unified-agent/tree/master/plugins/inputs/jolokia2/examples/zookeeper.conf)\n\nPlease help improve this list and contribute new configuration files by opening an issue or pull request.\n","logo":{"light":"/img/library/jolokia.png"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:jolokia2"},{"name":"json-pull","title":"JSON:Pull","content":"# JSON:Pull\n\n## Overview\n\nThe JSON: Pull Check retrieves metrics formatted as JSON. Circonus will pull the metrics from a specified HTTP endpoint.\n\nJavaScript Object Notation (JSON) is a lightweight format for storing and transporting data. JSON is often used when data is sent from a server to a web page, and is a convenient way to format metrics for Circonus.\n\n## Configuration\n\nGather metrics formatted in JSON. Here is an example of the JSON format:\n\n```\n{\n  \"number\": 1.23,\n  'bignum_as_string': '281474976710656',\n  'test': 'a text string',\n  'container': { 'key1': 1234 },\n  'array': [  1234,\n              'string',\n              { 'crazy': 'like a fox' }\n           ]\n}\n```\n\nThere is no particular data structure required by Circonus; format your data however you wish and Circonus will parse it accordingly. Circonus would parse the above example into the following metrics (services tells how many metrics resulted from parsing):\n\n| Name             | Type    | Value             |\n| ---------------- | ------- | ----------------- |\n| array`0          | numeric | 1234              |\n| array`1          | text    | \"string\"          |\n| array\\`2\\`crazy  | text    | \"like a fox\"      |\n| bignum_as_string | text    | \"281474976710656\" |\n| container`key1   | numeric | 1234              |\n| number           | numeric | 1.23000000        |\n| test             | text    | \"a text string\"   |\n| services         | numeric | 7                 |\n","logo":{"light":"/img/library/json.svg"},"attributes":{"implementation":"broker"},"tags":["JSON"],"module":"json"},{"name":"json-push-httptrap","title":"JSON:Push (HTTPTrap)","content":"# JSON:Push (HTTPTrap)\n\n## Overview\n\nThe JSON:Push (HTTPTrap) is a little different than the rest of the Circonus checks; instead of pulling information on a regular interval, it accepts JSON payloads sent via HTTP PUT requests. This data is not polled regularly from the Circonus Broker, but is pushed to the Broker from the monitored target. This is the easiest way to get arbitrary data into Circonus.\n\nThe JSON: Push Check receives metrics formatted as JSON (JavaScript Object Notation), a lightweight format for storing and transporting data. JSON is often used when data is sent from a server to a web page, and is a convenient way to format metrics for Circonus.\n\n## Configuration\n\nDuring the configuration process you will be asked for 2 items: the target host for this check and a \"secret\". The host for push style checks should be the IP or the resolvable server name from where the packets originate. The \"secret\" will be used as part of your submission URL for added security. The secret is a string containing letters, numbers, or underscores.\n\n![Image: 'check_httptrap_initial3.png'](../../img/check_httptrap_initial3.png)\n\nThe \"JSON Docs\" button describes how the JSON you PUT will be parsed into metrics. See [below](/circonus/integrations/library/json-push-httptrap/#httptrap-json-format) for more details.\n\nClicking \"Test Check\" will navigate to the final confirmation screen as normal. Since Circonus can't pull the data, you will be asked to enter your metric names on this screen. It's alright if you don't know the metrics at this point; just click \"Finish\" and the check will be created with no metrics.\n\n![Image: 'check_httptrap_final3.png'](../../img/check_httptrap_final3.png)\n\nAfter you create your check, we will provide a URL to which you will PUT your data. At this point, navigate to the details page for your newly created HTTPTrap check. Note the \"Data Submission URL\" link in the middle row.\n\n![Image: 'check_httptrap_details3.png'](../../img/check_httptrap_details3.png)\n\nAfter submitting data for the first time, you will want to specify which metrics you collect. Use the Menu at top right and click \"Change Brokers & Metrics\" to switch the metrics list into an edit mode.\n\n![Image: 'check_httptrap_change_metrics3.png'](../../img/check_httptrap_change_metrics3.png)\n\nThis will allow you to select and deselect metrics you want to collect. Click the \"Save\" button to finalize your choices.\n\n![Image: 'check_httptrap_metrics3.png'](../../img/check_httptrap_metrics3.png)\n\n### HTTPTrap JSON Format\n\nThis subsection describes how the JSON you PUT will be parsed into metrics.\n\nThis is an example of JSON format:\n\n```json\n{\n  \"number\": 1.23,\n  \"bignum_as_string\": \"281474976710656\",\n  \"test\": \"a text string\",\n  \"container\": { \"key1\": 1234 },\n  \"array\": [1234, \"string\", { \"crazy\": \"like a fox\" }]\n}\n```\n\nThere is no particular data structure required by Circonus; format your data however you wish and Circonus will parse it accordingly. Circonus would parse the above example into the following metrics (services tells how many metrics resulted from parsing):\n\n| Name             | Type    | Value             |\n| ---------------- | ------- | ----------------- |\n| array`0          | numeric | 1234              |\n| array`1          | text    | \"string\"          |\n| array\\`2\\`crazy  | text    | \"like a fox\"      |\n| bignum_as_string | text    | \"281474976710656\" |\n| container`key1   | numeric | 1234              |\n| number           | numeric | 1.23000000        |\n| test             | text    | \"a text string\"   |\n| services         | numeric | 7                 |\n\nIn addition to strings and numeric values, values can also be described using\n`{ \"_type\": <type>, \"_value\": <value> }` syntax. The available types are the\nsame used in the\n[circonus-agent](https://github.com/circonus-labs/circonus-agent/tree/master/plugins#metric-types)\n(`s, l, L, i, I,` and `n`). Values can be strings or numbers, but are\ninterpreted pursuant to the type specified. For example,\n\n- `{ \"_type\": \"s\", \"_value\": 812345 }` becomes the string `812345`.\n- `{ \"_type\": \"L\", \"_value\": \"2187345234234\" }` becomes the unsigned, 64-bit\n  integer `2187345234234`.\n\nFor example, to pass multiple values for histogram data using httptrap as an array, you could use the following example format:\n\n```json\n{\n  \"histogram\": {\n    \"_type\": \"h\",\n    \"_value\": [1, 2, 3, 4, 5]\n  }\n}\n```\n\n#### Histograms\n\nHistogram submission must use the `{ \"_type\": <type>, \"_value\": <value> }`\nformat. The type is `h`.\n\nNumeric values for histograms can be provided in two ways:\n\n- As a list. For example `[123,123,234,345,234,1]`.\n- As a prebucketed histogram. For example `[\"H[0.1]=3\", \"H[11]=7\"]`, would mean that in the bin 0.1 (which is 0.10 to 0.11) there are 3 samples and in the bin 11 (which is 11 to 12) there are 7 samples.\n\nAn example of the list form:\n\n```json\n{\n  \"foo\": {\n    \"_type\": \"h\",\n    \"_value\": [123, 123, 234, 345, 234, 1]\n  }\n}\n```\n\nAn example of the prebucketed form:\n\n```json\n{\n  \"foo\": {\n    \"_type\": \"h\",\n    \"_value\": [\"H[0.1]=3\", \"H[11]=7\"]\n  }\n}\n```\n\n#### Stream Tags\n\nThe HTTPTrap JSON format also permits the inclusion of [metric stream\ntags](https://www.circonus.com/2018/11/introducing-circonus-stream-tags/). Tags\ntake the following form:\n\n```\nfoo|ST[env:prod,app:web]\n```\n\nIn this case, `foo` is the metric name, and `ST[]` encompasses a\ncomma-separated list of `category:value` pairs. The tag section is separated\nfrom the metric name with a `|` (vertical bar). Here we have specified two\ntags, `env:prod` and `app:web`.\n\nCategory strings may contain upper- and lowercase letters (`A-Z` and `a-z`),\nnumerals (`0-9`), and the following characters:\n\n```\n`+!@#$%^&\"'/?._-\n```\n\nValues may contain all of the above, plus colon (`:`) and equals (`=`).\n\nAny tag characters that do not fall into the above set can still be submitted\nif they are base64-encoded and passed in a special wrapper format. For example,\na metric like this:\n\n```\nfoo|ST[~(bar):<quux>]\n```\n\nhas tilde (`~`), parentheses `()`, and greater/less-then (`<>`) which are all\noutside the allowed character set. In this case, base64-encode the category and\ntag separately, and enclose each in `b\"\"`, separating category from value with\na colon as usual:\n\n```\nfoo|ST[b\"fihiYXIp\":b\"PHF1dXg+\"]\n```\n\nThe full `category:value` string, including the colon, may not exceed 256\ncharacters. This applies regardless of whether the base64-encoded form is used\nor not.\n\nEach unique combination of metric name and tags counts as one \"metric stream\"\nin Circonus. For example:\n\n```\nfoo|ST[env:prod,app:web]\nfoo|ST[env:qa,app:web]\nfoo|ST[env:prod,app:database]\nfoo|ST[env:qa,app:database]\n```\n\nrepresent 4 separate streams. **Use caution when applying tags for dimensions\nwith high cardinality, such as user-id, container-id, UUIDs, or other unbounded\nsets of values.**\n\nSubmitting tagged data works the same way as untagged data. The full name with\ntags becomes the key (note that the quotes around base64-encoded tags must be\nescaped):\n\n```json\n{\n  \"foo|ST[env:prod,app:web]\": { \"_type\": \"n\", \"_value\": 12 },\n  \"foo|ST[env:qa,app:web]\": { \"_type\": \"n\", \"_value\": 0 },\n  \"foo|ST[b\\\"fihiYXIp\\\":b\\\"PHF1dXg+\\\"]\": { \"_type\": \"n\", \"_value\": 3 }\n}\n```\n\nBoth tagged and untagged metrics may be submitted together.\n\n### Timestamped Submission\n\nIf HTTPtrap submissions contain an additional value with `_ts`, the individual\nmeasurement will be timestamped with the provided value instead of the default\n\"now.\" The value of `_ts` should be specified in milliseconds since UNIX epoch\n1970-01-01 00:00:00-0000. `_ts` is a peer to the `_type` and `_value` keys\nspecified above.\n\nTimestamped submissions are not subject to the period-based accumulations noted\nin [Advanced Configuration](#advanced-configuration) above. They are stored\nwith millisecond granularity. If multiple measurements for a metric are\ntimestamped in the same millisecond, the largest by absolute value will\nultimately be stored.\n\nHistograms used in conjunction with `_ts` values, do not support the array formats,\nand must be sent as a full base64 encoded histogram record. This record format is\nprovided as part of the `libcircllhist` library. For example in Python:\n\n```python\nfrom circllhist import Circllhist\n\nh = Circllhist()\nh.insert(0.1,3)\nh.insert(0.2,3)\nprint(h.to_b64())\n```\n\nA base64 example JSON document would be:\n\n```json\n{\n  \"foo\": {\n    \"_type\": \"h\",\n    \"_value\": \"AAwAAAGeAVD2AX8BFPcAkzL4AawBMvkBWQEU+gARMvsAO0b8AXUBWv0ADQr+ALJa/wCfFAAAgA==\",\n    \"_ts\": 1604936367789\n  }\n}\n```\n\n### Batched Timestamped Submission\n\nWhen sending multiple measurements in a single payload, these may be batched into a single JSON document:\n\n```json\n{\n  \"foo\": { \"_type\": \"n\", \"_value\": 1, \"_ts\": 1604936367789 },\n  \"foo\": { \"_type\": \"n\", \"_value\": 2, \"_ts\": 1604936552774 },\n  \"foo\": { \"_type\": \"n\", \"_value\": 3, \"_ts\": 1604936616422 }\n}\n```\n\nAlternatively, multiple complete JSON payloads, each with one measurement per\nmetric, may be streamed in succession. This may be necessary if your JSON\nimplementation does not permit the same name/key to be repeated in one document\n(which is allowed per the JSON specification, but not always implemented as\nsuch.)\n\n```json\n{ \"foo\": { \"_type\": \"n\", \"_value\": 1, \"_ts\": 1604936367789 } }\n{ \"foo\": { \"_type\": \"n\", \"_value\": 2, \"_ts\": 1604936552774 } }\n{ \"foo\": { \"_type\": \"n\", \"_value\": 3, \"_ts\": 1604936616422 } }\n```\n\nIt is important to note that both JSON objects, and multiple serial objects, are both\nprocessed as a stream, rather than after the entire object has been loaded. For example,\nif a JSON object had 10,000 datapoints in it, and the last one had a parse error, the previous\n9,999 datapoints would still be ingested even though the object as a whole was invalid JSON.\n\n#### Period\n\nIn the absence of [explicit timestamps](#timestamped-submission), multiple\nmeasurements received in one check \"Period\" are handled according to the value of\nthe Asynchronous collection setting in the check configuration\n(`asynch_metrics` in the API object):\n\n- With Asynchronous enabled, the largest of the simultaneous values seen (with\n  millisecond granularity) will be stored immediately.\n- With Asynchronous disabled, each arriving measurement updates the last value seen\n  for the metric. When the period expires, the last value seen is stored.\n\n\"Timeout\" is not relevant for HTTPTrap checks.\n\n### Examples\n\nHere is a complete example of how to submit data to a HTTP JSON Trap:\n\n```\ncurl -X PUT 'https://api.circonus.com/module/httptrap/a9856a6a-3b46-e18b-d890-acafaa955348/mys3cr3t' --data '{\n    \"number\": 1.23,\n    \"bignum_as_string\": \"281474976710656\",\n    \"test\": \"a text string\",\n    \"container\": { \"key1\": 1234 },\n    \"array\": [  1234,\n                \"string\",\n                { \"crazy\": \"like a fox\" }\n             ]\n  }'\n```\n\nAn example of streaming multiple JSON documents:\n\n```\ncurl -X PUT 'https://api.circonus.com/module/httptrap/a9856a6a-3b46-e18b-d890-acafaa955348/mys3cr3t' --data '\n    { \"foo\": { \"_type\": \"n\", \"_value\": 1, \"_ts\": 1605033941001 } }\n    { \"foo\": { \"_type\": \"n\", \"_value\": 2, \"_ts\": 1605033941002 } }\n    { \"foo\": { \"_type\": \"n\", \"_value\": 3, \"_ts\": 1605033941003 } }'\n```\n","logo":{"light":"../../img/library/json.svg"},"attributes":{"implementation":"broker"},"tags":["JSON","push"],"module":"httptrap"},{"name":"jti-openconfig-telemetry","title":"JTI OpenConfig Telemetry","content":"# JTI OpenConfig Telemetry\n\n## Overview\n\nThis plugin reads Juniper Networks implementation of OpenConfig telemetry data from listed sensors using Junos Telemetry Interface. Refer to\n[openconfig.net](http://openconfig.net/) for more details about OpenConfig and [Junos Telemetry Interface (JTI)](https://www.juniper.net/documentation/en_US/junos/topics/concept/junos-telemetry-interface-oveview.html).\n\n## Configuration\n\n```toml\n# Subscribe and receive OpenConfig Telemetry data using JTI\n[[inputs.jti_openconfig_telemetry]]\n  ## List of device addresses to collect telemetry from\n  servers = [\"localhost:1883\"]\n\n  ## Authentication details. Username and password are must if device expects\n  ## authentication. Client ID must be unique when connecting from multiple instances\n  ## of circonus-unified-agent to the same device\n  username = \"user\"\n  password = \"pass\"\n  client_id = \"circonus\"\n\n  ## Frequency to get data\n  sample_frequency = \"1000ms\"\n\n  ## Sensors to subscribe for\n  ## A identifier for each sensor can be provided in path by separating with space\n  ## Else sensor path will be used as identifier\n  ## When identifier is used, we can provide a list of space separated sensors.\n  ## A single subscription will be created with all these sensors and data will\n  ## be saved to measurement with this identifier name\n  sensors = [\n   \"/interfaces/\",\n   \"collection /components/ /lldp\",\n  ]\n\n  ## We allow specifying sensor group level reporting rate. To do this, specify the\n  ## reporting rate in Duration at the beginning of sensor paths / collection\n  ## name. For entries without reporting rate, we use configured sample frequency\n  sensors = [\n   \"1000ms customReporting /interfaces /lldp\",\n   \"2000ms collection /components\",\n   \"/interfaces\",\n  ]\n\n  ## Optional TLS Config\n  # enable_tls = true\n  # tls_ca = \"/etc/circonus-unified-agent/ca.pem\"\n  # tls_cert = \"/etc/circonus-unified-agent/cert.pem\"\n  # tls_key = \"/etc/circonus-unified-agent/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n\n  ## Delay between retry attempts of failed RPC calls or streams. Defaults to 1000ms.\n  ## Failed streams/calls will not be retried if 0 is provided\n  retry_delay = \"1000ms\"\n\n  ## To treat all string values as tags, set this to true\n  str_as_tags = false\n```\n","logo":{"light":"/img/library/juniper-networks.svg","dark":"/img/library/juniper-networks-dark.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:jti_openconfig_telemetry"},{"name":"kubernetes","title":"Kubernetes","content":"# Kubernetes\n\n## Overview\n\nFor automated monitoring of a Kubernetes cluster, install the [Circonus Kubernetes Agent](https://docs.circonus.com/circonus/integrations/agents/circonus-kubernetes-agent/). Follow the provided instructions to install the agent and run it; the agent will automatically create a Kubernetes check for its metrics.\n\nThe Circonus Kubernetes Agent relies on [kube-state-metrics](https://github.com/kubernetes/kube-state-metrics) to be installed and enabled within your cluster.\n\n## Configuration\n\n### 1. Clone repo\n\nExecute the following command within the terminal to clone the repository to a local directory of your choosing:\n\n```\ngit clone https://github.com/circonus-labs/circonus-kubernetes-agent\n```\n\n### 2. Edit configs\n\nIn `deploy/default/configuration.yaml`, set the following required attributes:\n\n**circonus-api-key**\nYour unique Circonus API token\n\n**kubernetes-name**\nThe name corresponding to your Kubernetes cluster. We recommend a short, unique string without spaces.\n\n**contact.email**\nThe email address that you would like to use for default alerts. _See the section entitled `default-alerts.json`._\n\n### 3. Deploy\n\nFrom within the cloned directory (`circonus-kubernetes-agent`) execute the following command within the terminal to deploy:\n\n```\nkubectl apply -f deploy/default/\n```\n\nThe Circonus Kubernetes Agent will fetch a variety of different metrics depending on the services for which it is configured. [Learn more](https://docs.circonus.com/circonus/integrations/agents/circonus-kubernetes-agent/#configuration-options)\n","logo":{"light":"/img/library/kubernetes.svg"},"attributes":{"implementation":"kubernetes_agent"},"tags":["agent"],"module":"httptrap:kubernetes"},{"name":"ldap","title":"LDAP","content":"# LDAP\n\n## Overview\n\nThe LDAP Check monitors directory services using the Lightweight Directory Access Protocol.\n\n## Configuration\n\nOptional parameters:\n\n| Name               | Description                                                                                     |\n| ------------------ | ----------------------------------------------------------------------------------------------- |\n| port               | The TCP port to connect to the directory server (default: 389)                                  |\n| dn                 | The Distinguished Name to query.                                                                |\n| authtype           | The authentication type to use for the bind request. Choices are none or simple (default: none) |\n| security_principal | The username to be used for authentication to the directory server.                             |\n| password           | The password to be used for authentication to the directory server.                             |\n\n## Metrics\n\n| Name                          | Type    | Description                                                             |\n| ----------------------------- | ------- | ----------------------------------------------------------------------- |\n| time_to_connect_ms            | Numeric | The elapsed time, in milliseconds, to connect to the LDAP server.       |\n| total_objects                 | Numeric | The number of objects returned by the query.                            |\n| [ objectname ] \\_object_count | Numeric | The count of objects of each type (e.g., cn, ou) returned by the query. |\n","logo":{"light":"/img/library/ldap.svg"},"attributes":{"implementation":"broker"},"tags":["server","protocol","authentication"],"module":"ldap"},{"name":"logstash","title":"Logstash","content":"# Logstash\n\n## Overview\n\nThis plugin reads metrics exposed by [Logstash Monitoring API](https://www.elastic.co/guide/en/logstash/current/monitoring-logstash.html).\n\nLogstash 5 and later is supported.\n\n## Configuration\n\n```toml\n[[inputs.logstash]]\n  ## The URL of the exposed Logstash API endpoint.\n  url = \"http://127.0.0.1:9600\"\n\n  ## Use Logstash 5 single pipeline API, set to true when monitoring\n  ## Logstash 5.\n  # single_pipeline = false\n\n  ## Enable optional collection components.  Can contain\n  ## \"pipelines\", \"process\", and \"jvm\".\n  # collect = [\"pipelines\", \"process\", \"jvm\"]\n\n  ## Timeout for HTTP requests.\n  # timeout = \"5s\"\n\n  ## Optional HTTP Basic Auth credentials.\n  # username = \"username\"\n  # password = \"pa$$word\"\n\n  ## Optional TLS Config.\n  # tls_ca = \"/etc/circonus-unified-agent/ca.pem\"\n  # tls_cert = \"/etc/circonus-unified-agent/cert.pem\"\n  # tls_key = \"/etc/circonus-unified-agent/key.pem\"\n\n  ## Use TLS but skip chain & host verification.\n  # insecure_skip_verify = false\n\n  ## Optional HTTP headers.\n  # [inputs.logstash.headers]\n  #   \"X-Special-Header\" = \"Special-Value\"\n```\n","logo":{"light":"/img/library/logstash.svg","dark":"/img/library/logstash-dark.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:logstash"},{"name":"lustre","title":"Lustre","content":"# Lustre\n\n## Overview\n\nThe [Lustre®](http://lustre.org/) file system is an open-source, parallel file system that supports many requirements of leadership class HPC simulation environments.\n\nThis plugin monitors the Lustre file system using its entries in the proc filesystem.\n\n## Configuration\n\n```toml\n# Read metrics from local Lustre service on OST, MDS\n[[inputs.lustre2]]\n  ## An array of /proc globs to search for Lustre stats\n  ## If not specified, the default will work on Lustre 2.5.x\n  ##\n  # ost_procfiles = [\n  #   \"/proc/fs/lustre/obdfilter/*/stats\",\n  #   \"/proc/fs/lustre/osd-ldiskfs/*/stats\",\n  #   \"/proc/fs/lustre/obdfilter/*/job_stats\",\n  # ]\n  # mds_procfiles = [\n  #   \"/proc/fs/lustre/mdt/*/md_stats\",\n  #   \"/proc/fs/lustre/mdt/*/job_stats\",\n  # ]\n```\n\n## Troubleshooting\n\nCheck for the default or custom procfiles in the proc filesystem, and reference the [Lustre Monitoring and Statistics Guide](http://wiki.lustre.org/Lustre_Monitoring_and_Statistics_Guide. This plugin does not report all information from these files, only a limited set of items corresponding to the above metric fields.\n","logo":{"light":"/img/library/lustre.png"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:lustre2"},{"name":"mailchimp","title":"Mailchimp","content":"# Mailchimp\n\n## Overview\n\nPulls campaign reports from the [Mailchimp API](https://developer.mailchimp.com/).\n\n## Configuration\n\nThis section contains the default TOML to configure the plugin. You can\ngenerate it using `circonus-unified-agent --usage mailchimp`.\n\n```toml\n[[inputs.mailchimp]]\n  ## MailChimp API key\n  ## get from https://admin.mailchimp.com/account/api/\n  api_key = \"\" # required\n\n  ## Reports for campaigns sent more than days_old ago will not be collected.\n  ## 0 means collect all and is the default value.\n  days_old = 0\n\n  ## Campaign ID to get, if empty gets all campaigns, this option overrides days_old\n  # campaign_id = \"\"\n```\n","logo":{"light":"/img/library/mailchimp.svg","dark":"/img/library/mailchimp-dark.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:mailchimp"},{"name":"marklogic","title":"MarkLogic","content":"# MarkLogic\n\n## Overview\n\nThe MarkLogic plugin gathers health status metrics from one or more host.\n\n## Configuration\n\nThis section contains the default TOML to configure the plugin. You can\ngenerate it using `circonus-unified-agent --usage mailchimp`.\n\n```toml\n[[inputs.marklogic]]\n  ## Base URL of the MarkLogic HTTP Server.\n  url = \"http://localhost:8002\"\n\n  ## List of specific hostnames to retrieve information. At least (1) required.\n  # hosts = [\"hostname1\", \"hostname2\"]\n\n  ## Using HTTP Basic Authentication. Management API requires 'manage-user' role privileges\n  # username = \"myuser\"\n  # password = \"mypassword\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/circonus-unified-agent/ca.pem\"\n  # tls_cert = \"/etc/circonus-unified-agent/cert.pem\"\n  # tls_key = \"/etc/circonus-unified-agent/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n","logo":{"light":"/img/library/marklogic.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:marklogic"},{"name":"mcrouter","title":"Mcrouter","content":"# Mcrouter\n\n## Overview\n\nThis plugin gathers statistics data from a Mcrouter server.\n\n## Configuration\n\n```toml\n# Read metrics from one or many mcrouter servers.\n[[inputs.mcrouter]]\n  ## An array of address to gather stats about. Specify an ip or hostname\n  ## with port. ie tcp://localhost:11211, tcp://10.0.0.1:11211, etc.\n  servers = [\"tcp://localhost:11211\", \"unix:///var/run/mcrouter.sock\"]\n\n  ## Timeout for metric collections from all servers.  Minimum timeout is \"1s\".\n  # timeout = \"5s\"\n```\n","logo":{"light":"/img/library/mcrouter.svg"},"attributes":{"implementation":"cua"},"tags":["memory","cache","memcached","proxy","load","balancer"],"module":"httptrap:cua:mcrouter"},{"name":"memcached-legacy","title":"Memcached","content":"# Memcached\n\n## Overview\n\nThe Memcache Check monitors your Memcached instances.\n\nMemcached is a free open-source general purpose distributed memory caching system.\n\n## Configuration\n\nOptional parameters:\n\n| Name | Description                                                      |\n| ---- | ---------------------------------------------------------------- |\n| port | The TCP port to connect to the memcached server (default: 11211) |\n\n## Metrics\n\nTypical metrics include memory usage, cache hits, cache misses, cache evictions, cache fill percentage, and many more.\n","logo":{"light":"/img/library/memcached.svg"},"attributes":{"implementation":"broker","legacy":true},"tags":["memory","cache"],"module":"memcached"},{"name":"memcached","title":"Memcached","content":"# Memcached\n\n## Overview\n\nThis plugin gathers statistics data from a Memcached server.\n\n## Configuration\n\n```toml\n# Read metrics from one or many memcached servers.\n[[inputs.memcached]]\n  # An array of address to gather stats about. Specify an ip on hostname\n  # with optional port. ie localhost, 10.0.0.1:11211, etc.\n  servers = [\"localhost:11211\"]\n  # An array of unix memcached sockets to gather stats about.\n  # unix_sockets = [\"/var/run/memcached.sock\"]\n```\n","logo":{"light":"/img/library/memcached.svg"},"attributes":{"implementation":"cua"},"tags":["memory","cache"],"module":"httptrap:cua:memcached"},{"name":"microsoft-sqlserver","title":"Microsoft SQLServer","content":"# Microsoft SQLServer\n\n## Overview\n\nThe `sqlserver` plugin provides metrics for your SQL Server instance. It\ncurrently works with SQL Server 2008 SP3 and newer. Recorded metrics are\nlightweight and use Dynamic Management Views supplied by SQL Server.\n\n### The SQL Server plugin supports the following editions/versions of SQL Server\n\n- SQL Server\n  - 2008 SP3 (with CU3)\n  - SQL Server 2008 R2 SP3 and newer versions\n- Azure SQL Database (Single)\n- Azure SQL Managed Instance\n\n### Additional Setup:\n\nYou have to create a login on every SQL Server instance or Azure SQL Managed instance you want to monitor, with following script:\n\n```sql\nUSE master;\nGO\nCREATE LOGIN [cua] WITH PASSWORD = N'mystrongpassword';\nGO\nGRANT VIEW SERVER STATE TO [cua];\nGO\nGRANT VIEW ANY DEFINITION TO [cua];\nGO\n```\n\nFor Azure SQL Database, you require the View Database State permission and can create a user with a password directly in the database.\n\n```sql\nCREATE USER [cua] WITH PASSWORD = N'mystrongpassword';\nGO\nGRANT VIEW DATABASE STATE TO [cua];\nGO\n```\n\n## Configuration\n\n```toml\n[agent]\n  ## Default data collection interval for all inputs, can be changed as per collection interval needs\n  interval = \"10s\"\n\n# Read metrics from Microsoft SQL Server\n[[inputs.sqlserver]]\n  ## Specify instances to monitor with a list of connection strings.\n  ## All connection parameters are optional.\n  ## By default, the host is localhost, listening on default port, TCP 1433.\n  ##   for Windows, the user is the currently running AD user (SSO).\n  ##   See https://github.com/denisenkom/go-mssqldb for detailed connection\n  ##   parameters, in particular, tls connections can be created like so:\n  ##   \"encrypt=true;certificate=<cert>;hostNameInCertificate=<SqlServer host fqdn>\"\n  # servers = [\n  #  \"Server=192.168.1.10;Port=1433;User Id=<user>;Password=<pw>;app name=cua;log=1;\",\n  # ]\n\n  ## This enables a specific set of queries depending on the database type. If specified, it replaces azuredb = true/false and query_version = 2\n  ## In the config file, the sql server plugin section should be repeated  each with a set of servers for a specific database_type.\n  ## Possible values for database_type are\n  ## \"AzureSQLDB\"\n  ## \"SQLServer\"\n  ## \"AzureSQLManagedInstance\"\n  # database_type = \"AzureSQLDB\"\n\n  ## Optional parameter, setting this to 2 will use a new version\n  ## of the collection queries that break compatibility with the original dashboards.\n  ## Version 2 - is compatible from SQL Server 2008 Sp3 and later versions and also for SQL Azure DB\n  ## Version 2 is in the process of being deprecated, please consider using database_type.\n  # query_version = 2\n\n  ## If you are using AzureDB, setting this to true will gather resource utilization metrics\n  # azuredb = false\n\n  ## Possible queries accross different versions of the collectors\n  ## Queries enabled by default for specific Database Type\n\n  ## database_type =  AzureSQLDB  by default collects the following queries\n  ## - AzureSQLDBWaitStats\n  ## - AzureSQLDBResourceStats\n  ## - AzureSQLDBResourceGovernance\n  ## - AzureSQLDBDatabaseIO\n  ## - AzureSQLDBServerProperties\n  ## - AzureSQLDBOsWaitstats\n  ## - AzureSQLDBMemoryClerks\n  ## - AzureSQLDBPerformanceCounters\n  ## - AzureSQLDBRequests\n  ## - AzureSQLDBSchedulers\n\n   ## database_type =  AzureSQLManagedInstance by default collects the following queries\n   ## - AzureSQLMIResourceStats\n   ## - AzureSQLMIResourceGovernance\n   ## - AzureSQLMIDatabaseIO\n   ## - AzureSQLMIServerProperties\n   ## - AzureSQLMIOsWaitstats\n   ## - AzureSQLMIMemoryClerks\n   ## - AzureSQLMIPerformanceCounters\n   ## - AzureSQLMIRequests\n   ## - AzureSQLMISchedulers\n\n   ## database_type =  SQLServer by default collects the following queries\n   ## - SQLServerPerformanceCounters\n   ## - SQLServerWaitStatsCategorized\n   ## - SQLServerDatabaseIO\n   ## - SQLServerProperties\n   ## - SQLServerMemoryClerks\n   ## - SQLServerSchedulers\n   ## - SQLServerRequests\n   ## - SQLServerVolumeSpace\n   ## - SQLServerCpu\n\n  ## Version 2 by default collects the following queries\n  ## Version 2 is being deprecated, please consider using database_type.\n  ## - PerformanceCounters\n  ## - WaitStatsCategorized\n  ## - DatabaseIO\n  ## - ServerProperties\n  ## - MemoryClerk\n  ## - Schedulers\n  ## - SqlRequests\n  ## - VolumeSpace\n  ## - Cpu\n\n  ## Version 1 by default collects the following queries\n  ## Version 1 is deprecated, please consider using database_type.\n  ## - PerformanceCounters\n  ## - WaitStatsCategorized\n  ## - CPUHistory\n  ## - DatabaseIO\n  ## - DatabaseSize\n  ## - DatabaseStats\n  ## - DatabaseProperties\n  ## - MemoryClerk\n  ## - VolumeSpace\n  ## - PerformanceMetrics\n\n\n\n  ## A list of queries to include. If not specified, all the above listed queries are used.\n  # include_query = []\n\n  ## A list of queries to explicitly ignore.\n  exclude_query = [ 'Schedulers' , 'SqlRequests' ]\n\n\n\n```\n","logo":{"light":"/img/library/microsoft-sqlserver.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:sqlserver"},{"name":"modbus","title":"Modbus","content":"# Modbus\n\n## Overview\n\nThe Modbus plugin collects Discrete Inputs, Coils, Input Registers and Holding Registers via Modbus TCP or Modbus RTU/ASCII.\n\n## Configuration\n\n```toml\n[[inputs.modbus]]\n  ## Connection Configuration\n  ##\n  ## The plugin supports connections to PLCs via MODBUS/TCP or\n  ## via serial line communication in binary (RTU) or readable (ASCII) encoding\n  ##\n  ## Device name\n  name = \"Device\"\n\n  ## Slave ID - addresses a MODBUS device on the bus\n  ## Range: 0 - 255 [0 = broadcast; 248 - 255 = reserved]\n  slave_id = 1\n\n  ## Timeout for each request\n  timeout = \"1s\"\n\n  ## Maximum number of retries and the time to wait between retries\n  ## when a slave-device is busy.\n  # busy_retries = 0\n  # busy_retries_wait = \"100ms\"\n\n  # TCP - connect via Modbus/TCP\n  controller = \"tcp://localhost:502\"\n\n  ## Serial (RS485; RS232)\n  # controller = \"file:///dev/ttyUSB0\"\n  # baud_rate = 9600\n  # data_bits = 8\n  # parity = \"N\"\n  # stop_bits = 1\n  # transmission_mode = \"RTU\"\n\n\n  ## Measurements\n  ##\n\n  ## Digital Variables, Discrete Inputs and Coils\n  ## measurement - the (optional) measurement name, defaults to \"modbus\"\n  ## name        - the variable name\n  ## address     - variable address\n\n  discrete_inputs = [\n    { name = \"start\",          address = [0]},\n    { name = \"stop\",           address = [1]},\n    { name = \"reset\",          address = [2]},\n    { name = \"emergency_stop\", address = [3]},\n  ]\n  coils = [\n    { name = \"motor1_run\",     address = [0]},\n    { name = \"motor1_jog\",     address = [1]},\n    { name = \"motor1_stop\",    address = [2]},\n  ]\n\n  ## Analog Variables, Input Registers and Holding Registers\n  ## measurement - the (optional) measurement name, defaults to \"modbus\"\n  ## name        - the variable name\n  ## byte_order  - the ordering of bytes\n  ##  |---AB, ABCD   - Big Endian\n  ##  |---BA, DCBA   - Little Endian\n  ##  |---BADC       - Mid-Big Endian\n  ##  |---CDAB       - Mid-Little Endian\n  ## data_type  - INT16, UINT16, INT32, UINT32, INT64, UINT64, FLOAT32-IEEE, FLOAT64-IEEE (the IEEE 754 binary representation)\n  ##              FLOAT32 (deprecated), FIXED, UFIXED (fixed-point representation on input)\n  ## scale      - the final numeric variable representation\n  ## address    - variable address\n\n  holding_registers = [\n    { name = \"power_factor\", byte_order = \"AB\",   data_type = \"FIXED\", scale=0.01,  address = [8]},\n    { name = \"voltage\",      byte_order = \"AB\",   data_type = \"FIXED\", scale=0.1,   address = [0]},\n    { name = \"energy\",       byte_order = \"ABCD\", data_type = \"FIXED\", scale=0.001, address = [5,6]},\n    { name = \"current\",      byte_order = \"ABCD\", data_type = \"FIXED\", scale=0.001, address = [1,2]},\n    { name = \"frequency\",    byte_order = \"AB\",   data_type = \"UFIXED\", scale=0.1,  address = [7]},\n    { name = \"power\",        byte_order = \"ABCD\", data_type = \"UFIXED\", scale=0.1,  address = [3,4]},\n  ]\n  input_registers = [\n    { name = \"tank_level\",   byte_order = \"AB\",   data_type = \"INT16\",   scale=1.0,     address = [0]},\n    { name = \"tank_ph\",      byte_order = \"AB\",   data_type = \"INT16\",   scale=1.0,     address = [1]},\n    { name = \"pump1_speed\",  byte_order = \"ABCD\", data_type = \"INT32\",   scale=1.0,     address = [3,4]},\n  ]\n```\n","logo":{"light":"/img/library/modbus.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:modbus"},{"name":"mongodb-legacy","title":"MongoDB","content":"# MongoDB\n\n## Overview\n\nMongoDB is an open-source NoSQL database that uses a document-oriented database model to make it easier to split data across multiple servers for increased security and productivity.\n\nThe MongoDB Check collects metrics from your MongoDB instances. Circonus pulls information from MongoDB in JSON format via the database's native interface.\n\n## Configuration\n\nRequired parameters:\n| Name | Description |\n|--------|------------------------------------------------|\n| port | The TCP port to connect to the MongoDB server. |\n| dbname | The name of the database to query. |\n\nOptional parameters:\n| Name | Description |\n|----------|------------------------------------------------------------------------------------------------------|\n| username | The user name to be used for authentication to the MongoDB server. |\n| password | The password to be used for authentication to the MongoDB server. |\n| command | A command (query) that will be run using the runCommand method, and metrics created from the result. |\n","logo":{"light":"/img/library/mongodb.svg"},"attributes":{"implementation":"broker","legacy":true},"tags":["nosql","database"],"module":"mongodb"},{"name":"mongodb","title":"MongoDB","content":"# MongoDB\n\n## Configuration\n\n```toml\n[[inputs.mongodb]]\n  ## An array of URLs of the form:\n  ##   \"mongodb://\" [user \":\" pass \"@\"] host [ \":\" port]\n  ## For example:\n  ##   mongodb://user:auth_key@10.10.3.30:27017,\n  ##   mongodb://10.10.3.33:18832,\n  servers = [\"mongodb://127.0.0.1:27017\"]\n\n  ## When true, collect cluster status.\n  ## Note that the query that counts jumbo chunks triggers a COLLSCAN, which\n  ## may have an impact on performance.\n  # gather_cluster_status = true\n\n  ## When true, collect per database stats\n  # gather_perdb_stats = false\n\n  ## When true, collect per collection stats\n  # gather_col_stats = false\n\n  ## List of db where collections stats are collected\n  ## If empty, all db are concerned\n  # col_stats_dbs = [\"local\"]\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/circonus-unified-agent/ca.pem\"\n  # tls_cert = \"/etc/circonus-unified-agent/cert.pem\"\n  # tls_key = \"/etc/circonus-unified-agent/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n\n### Permissions:\n\nIf your MongoDB instance has access control enabled you will need to connect\nas a user with sufficient rights.\n\nWith MongoDB 3.4 and higher, the `clusterMonitor` role can be used. In\nversion 3.2 you may also need these additional permissions:\n\n```\n> db.grantRolesToUser(\"user\", [{role: \"read\", actions: \"find\", db: \"local\"}])\n```\n\nIf the user is missing required privileges you may see an error in the logs similar to:\n\n```\nError in input [mongodb]: not authorized on admin to execute command { serverStatus: 1, recordStats: 0 }\n```\n\nSome permission related errors are logged at debug level, you can check these\nmessages by setting `debug = true` in the agent section of the configuration or\nby running agent with the `--debug` argument.\n","logo":{"light":"/img/library/mongodb.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:mongodb"},{"name":"mqtt","title":"MQTT","content":"# MQTT\n\n## Overview\n\nThe [MQTT](https://mqtt.org) consumer plugin reads from the specified MQTT topics and creates metrics using one of the supported [input data formats](https://github.com/circonus-labs/circonus-unified-agent/blob/master/docs/DATA_FORMATS_INPUT.md).\n\n## Configuration\n\n```toml\n[[inputs.mqtt_consumer]]\n  ## Broker URLs for the MQTT server or cluster.  To connect to multiple\n  ## clusters or standalone servers, use a seperate plugin instance.\n  ##   example: servers = [\"tcp://localhost:1883\"]\n  ##            servers = [\"ssl://localhost:1883\"]\n  ##            servers = [\"ws://localhost:1883\"]\n  servers = [\"tcp://127.0.0.1:1883\"]\n\n  ## Topics that will be subscribed to.\n  topics = [\n    \"circonus/host01/cpu\",\n    \"circonus/+/mem\",\n    \"sensors/#\",\n  ]\n\n  ## The message topic will be stored in a tag specified by this value.  If set\n  ## to the empty string no topic tag will be created.\n  # topic_tag = \"topic\"\n\n  ## QoS policy for messages\n  ##   0 = at most once\n  ##   1 = at least once\n  ##   2 = exactly once\n  ##\n  ## When using a QoS of 1 or 2, you should enable persistent_session to allow\n  ## resuming unacknowledged messages.\n  # qos = 0\n\n  ## Connection timeout for initial connection in seconds\n  # connection_timeout = \"30s\"\n\n  ## Maximum messages to read from the broker that have not been written by an\n  ## output.  For best throughput set based on the number of metrics within\n  ## each message and the size of the output's metric_batch_size.\n  ##\n  ## For example, if each message from the queue contains 10 metrics and the\n  ## output metric_batch_size is 1000, setting this to 100 will ensure that a\n  ## full batch is collected and the write is triggered immediately without\n  ## waiting until the next flush_interval.\n  # max_undelivered_messages = 1000\n\n  ## Persistent session disables clearing of the client session on connection.\n  ## In order for this option to work you must also set client_id to identify\n  ## the client.  To receive messages that arrived while the client is offline,\n  ## also set the qos option to 1 or 2 and don't forget to also set the QoS when\n  ## publishing.\n  # persistent_session = false\n\n  ## If unset, a random client ID will be generated.\n  # client_id = \"\"\n\n  ## Username and password to connect MQTT server.\n  # username = \"username\"\n  # password = \"metricsmetricsmetricsmetrics\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/circonus-unified-agent/ca.pem\"\n  # tls_cert = \"/etc/circonus-unified-agent/cert.pem\"\n  # tls_key = \"/etc/circonus-unified-agent/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/circonus-labs/circonus-unified-agent/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"json\"\n```\n","logo":{"light":"/img/library/mqtt.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:mqtt_consumer"},{"name":"munin","title":"Munin","content":"# Munin\n\n## Overview\n\nMunin is a free open-source computer system monitoring, network monitoring and infrastructure monitoring software application. It is designed around a client-server architecture and can be configured to monitor any number of client machines.\n\nThe Munin Check collects metrics from the Munin Monitoring tool. Circonus will pull the metrics from the specified Munin Master host.\n\n## Configuration\n\n| Name | Description                                               |\n| ---- | --------------------------------------------------------- |\n| port | The TCP port to connect to the Munin host (default: 4949) |\n\n## Metrics\n\nThe metrics available from the Munin check depend on the specific statistics returned by the target node. One additional text metric, remote_plugins, will be either set to the list of plugins returned by the target node, or set to no plugins if no plugins were found.\n","logo":{"light":"/img/library/munin.svg","dark":"/img/library/munin-dark.svg"},"attributes":{"implementation":"broker"},"module":"munin"},{"name":"mysql-legacy","title":"MySQL","content":"# MySQL\n\nThe MySQL Check allows querying of your MySQL database, enabling you to collect arbitrary metrics stored therein.\n\nMySQL is an open-source relational database management developed, distributed, and supported by the Oracle Corporation.\n\n## Configuration\n\nIf you are configuring a MySQL check via the Circonus API, the values for host, database, username, and password are combined into a single parameter called dsn: host=5.4.3.2 port=5432 username=myuser password=mypass dbname=test\n\nRequired parameters:\n|Name|Description|\n|----|-----------|\n|database|The name of the database to which the check will connect to run a query.|\n|sql|The SQL query to run. There are a number of predefined options, or you may enter a |custom query in the SQL Query field.|\n|port|The TCP port to which the check will connect to MySQL (default: 3306)|\n\n| Name     | Description                                                    |\n| -------- | -------------------------------------------------------------- |\n| username | The user name for server authorization.                        |\n| password | The password for server authorization.                         |\n| sslmode  | Upgrade the TCP connection to use SSL/TLS (default: false/off) |\n\nAdditional options allow you to set Server Authorization, use SSL, or to change the default Period (60 second), Timeout (10 seconds), and Port (3306).\n\n## Metrics\n\nPre-defined SQL queries are provided to retrieve common performance metrics such as binlog status, connections, InnoDB buffer pool, locks, query cache tables, and transactions.\n\nIt’s also possible to collect arbitrary metrics with this check. Given this sample database table:\n| |Col1 |Col2|Col3|\n|-----|-----|----|----|\n|Row1 |Name1|Val1|Val3|\n|Row2 |Name2|Val2|Val4|\n\nCirconus would parse the above table into the following metrics:\n|Name|Value|\n|----|-----|\n|Name1`Col2|Val1|\n|Name1`Col3|Val3|\n|Name2`Col2|Val2|\n|Name2`Col3|Val4|\n","logo":{"light":"/img/library/mysql.svg"},"attributes":{"implementation":"broker","legacy":true},"tags":["database","SQL"],"module":"mysql"},{"name":"mysql","title":"MySQL","content":"# MySQL\n\n## Overview\n\nThis plugin gathers the statistic data from MySQL server.\n\n- Global statuses\n- Global variables\n- Slave statuses\n- Binlog size\n- Process list\n- User Statistics\n- Info schema auto increment columns\n- InnoDB metrics\n- Table I/O waits\n- Index I/O waits\n- Perf Schema table lock waits\n- Perf Schema event waits\n- Perf Schema events statements\n- File events statistics\n- Table schema statistics\n\n## Configuration\n\n```toml\n[[inputs.mysql]]\n  ## specify servers via a url matching:\n  ##  [username[:password]@][protocol[(address)]]/[?tls=[true|false|skip-verify|custom]]\n  ##  see https://github.com/go-sql-driver/mysql#dsn-data-source-name\n  ##  e.g.\n  ##    servers = [\"user:passwd@tcp(127.0.0.1:3306)/?tls=false\"]\n  ##    servers = [\"user@tcp(127.0.0.1:3306)/?tls=false\"]\n  #\n  ## If no servers are specified, then localhost is used as the host.\n  servers = [\"tcp(127.0.0.1:3306)/\"]\n\n  ## Selects the metric output format.\n  ##\n  ## This option exists to maintain backwards compatibility, if you have\n  ## existing metrics do not set or change this value until you are ready to\n  ## migrate to the new format.\n  ##\n  ## If you do not have existing metrics from this plugin set to the latest\n  ## version.\n  ##\n  metric_version = 2\n\n  ## if the list is empty, then metrics are gathered from all database tables\n  # table_schema_databases = []\n\n  ## gather metrics from INFORMATION_SCHEMA.TABLES for databases provided above list\n  # gather_table_schema = false\n\n  ## gather thread state counts from INFORMATION_SCHEMA.PROCESSLIST\n  # gather_process_list = false\n\n  ## gather user statistics from INFORMATION_SCHEMA.USER_STATISTICS\n  # gather_user_statistics = false\n\n  ## gather auto_increment columns and max values from information schema\n  # gather_info_schema_auto_inc = false\n\n  ## gather metrics from INFORMATION_SCHEMA.INNODB_METRICS\n  # gather_innodb_metrics = false\n\n  ## gather metrics from SHOW SLAVE STATUS command output\n  # gather_slave_status = false\n\n  ## gather metrics from SHOW BINARY LOGS command output\n  # gather_binary_logs = false\n\n  ## gather metrics from SHOW GLOBAL VARIABLES command output\n  # gather_global_variables = true\n\n  ## gather metrics from PERFORMANCE_SCHEMA.TABLE_IO_WAITS_SUMMARY_BY_TABLE\n  # gather_table_io_waits = false\n\n  ## gather metrics from PERFORMANCE_SCHEMA.TABLE_LOCK_WAITS\n  # gather_table_lock_waits = false\n\n  ## gather metrics from PERFORMANCE_SCHEMA.TABLE_IO_WAITS_SUMMARY_BY_INDEX_USAGE\n  # gather_index_io_waits = false\n\n  ## gather metrics from PERFORMANCE_SCHEMA.EVENT_WAITS\n  # gather_event_waits = false\n\n  ## gather metrics from PERFORMANCE_SCHEMA.FILE_SUMMARY_BY_EVENT_NAME\n  # gather_file_events_stats = false\n\n  ## gather metrics from PERFORMANCE_SCHEMA.EVENTS_STATEMENTS_SUMMARY_BY_DIGEST\n  # gather_perf_events_statements = false\n\n  ## the limits for metrics form perf_events_statements\n  # perf_events_statements_digest_text_limit = 120\n  # perf_events_statements_limit = 250\n  # perf_events_statements_time_limit = 86400\n\n  ## Some queries we may want to run less often (such as SHOW GLOBAL VARIABLES)\n  ##   example: interval_slow = \"30m\"\n  # interval_slow = \"\"\n\n  ## Optional TLS Config (will be used if tls=custom parameter specified in server uri)\n  # tls_ca = \"/etc/circonus-unified-agent/ca.pem\"\n  # tls_cert = \"/etc/circonus-unified-agent/cert.pem\"\n  # tls_key = \"/etc/circonus-unified-agent/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n","logo":{"light":"/img/library/mysql.svg"},"attributes":{"implementation":"cua"},"tags":["database","SQL"],"module":"httptrap:cua:mysql"},{"name":"nats-consumer","title":"NATS Consumer","content":"# NATS Consumer\n\n## Overview\n\nThe [NATS](https://www.nats.io/about/) consumer plugin reads from the specified NATS subjects and\ncreates metrics using one of the supported [input data formats](https://github.com/circonus-labs/circonus-unified-agent/blob/master/docs/DATA_FORMATS_INPUT.md).\n\nA [Queue Group](https://www.nats.io/documentation/concepts/nats-queueing/) is used when subscribing to subjects so multiple\ninstances of circonus-unified-agent can read from a NATS cluster in parallel.\n\n## Configuration\n\n```toml\n[[inputs.nats_consumer]]\n  ## urls of NATS servers\n  servers = [\"nats://localhost:4222\"]\n\n  ## subject(s) to consume\n  subjects = [\"circonus\"]\n\n  ## name a queue group\n  queue_group = \"circonus_consumers\"\n\n  ## Optional credentials\n  # username = \"\"\n  # password = \"\"\n\n  ## Optional NATS 2.0 and NATS NGS compatible user credentials\n  # credentials = \"/etc/circonus-unified-agent/nats.creds\"\n\n  ## Use Transport Layer Security\n  # secure = false\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/circonus-unified-agent/ca.pem\"\n  # tls_cert = \"/etc/circonus-unified-agent/cert.pem\"\n  # tls_key = \"/etc/circonus-unified-agent/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n\n  ## Sets the limits for pending msgs and bytes for each subscription\n  ## These shouldn't need to be adjusted except in very high throughput scenarios\n  # pending_message_limit = 65536\n  # pending_bytes_limit = 67108864\n\n  ## Maximum messages to read from the broker that have not been written by an\n  ## output.  For best throughput set based on the number of metrics within\n  ## each message and the size of the output's metric_batch_size.\n  ##\n  ## For example, if each message from the queue contains 10 metrics and the\n  ## output metric_batch_size is 1000, setting this to 100 will ensure that a\n  ## full batch is collected and the write is triggered immediately without\n  ## waiting until the next flush_interval.\n  # max_undelivered_messages = 1000\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/circonus-labs/circonus-unified-agent/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"json\"\n```\n","logo":{"light":"/img/library/nats.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:nats_consumer"},{"name":"nats","title":"NATS","content":"# NATS\n\n## Overview\n\nThe [NATS](http://www.nats.io/about/) monitoring plugin gathers metrics from\nthe NATS [monitoring http server](https://www.nats.io/documentation/server/gnatsd-monitoring/).\n\n## Configuration\n\n```toml\n[[inputs.nats]]\n  ## The address of the monitoring endpoint of the NATS server\n  server = \"http://localhost:8222\"\n\n  ## Maximum time to receive response\n  # response_timeout = \"5s\"\n```\n","logo":{"light":"/img/library/nats.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:nats"},{"name":"nginx-legacy","title":"NGINX","content":"# NGINX\n\n## Overview\n\nNGiNX is open source software for web serving, reverse proxying, caching, load balancing, media streaming, and more. In addition to its HTTP server capabilities, NGiNX can also function as a reverse proxy, load balancer, and cache.\n\nThe NGiNX Check collects metrics from your NGiNX instances. Circonus will pull the metrics from the specified Nginx server through the Nginx statistics endpoint.\n\n## Configuration\n\nThe only required parameter is the status URL. See the NGiNX documentation for information on configuring a status endpoint.\n\n## Metrics\n\nTypical metrics include the number of upstream servers, the total number of error status codes (broken down by code), the total number of active connections, and cache size, hits, misses, and more.\n","logo":{"light":"/img/library/nginx.svg"},"attributes":{"implementation":"broker","legacy":true},"tags":["reverse-proxy","accelerator","load-balancer","cache","server"],"module":"nginx"},{"name":"nginx-plus-api","title":"NGINX Plus API","content":"# NGINX Plus API\n\n## Overview\n\nNginx Plus is a commercial version of the open source web server Nginx. The use this plugin you will need a license. For more information about the differences between Nginx (F/OSS) and Nginx Plus, visit: https://www.nginx.com/blog/whats-difference-nginx-foss-nginx-plus/.\n\n## Configuration\n\n```toml\n# Read Nginx Plus API advanced status information\n[[inputs.nginx_plus_api]]\n  ## An array of Nginx API URIs to gather stats.\n  urls = [\"http://localhost/api\"]\n  # Nginx API version, default: 3\n  # api_version = 3\n```\n","logo":{"light":"/img/library/nginx-plus.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:nginx_plus_api"},{"name":"nginx-plus","title":"NGINX Plus","content":"# NGINX Plus\n\n## Overview\n\nNginx Plus is a commercial version of the open source web server Nginx. To use this plugin you will need a license. For more information about the differences between Nginx (F/OSS) and Nginx Plus, visit: https://www.nginx.com/blog/whats-difference-nginx-foss-nginx-plus/.\n\nStructures for Nginx Plus have been built based on history of\n[status module documentation](http://nginx.org/en/docs/http/ngx_http_status_module.html)\n\n## Configuration\n\n```toml\n# Read Nginx Plus' advanced status information\n[[inputs.nginx_plus]]\n  ## An array of Nginx status URIs to gather stats.\n  urls = [\"http://localhost/status\"]\n```\n","logo":{"light":"/img/library/nginx-plus.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:nginx_plus"},{"name":"nginx-sts","title":"NGINX Stream Traffic Status (STS)","content":"# NGINX Stream Traffic Status (STS)\n\n## Overview\n\nThis plugin gathers Nginx status using external virtual host traffic status\nmodule - https://github.com/vozlt/nginx-module-sts. This is an Nginx module\nthat provides access to stream host status information. It contains the current\nstatus such as servers, upstreams, caches. This is similar to the live activity\nmonitoring of Nginx plus. For module configuration details please see its\n[documentation](https://github.com/vozlt/nginx-module-sts#synopsis).\n\n## Configuration\n\n```toml\n[[inputs.nginx_sts]]\n  ## An array of ngx_http_status_module or status URI to gather stats.\n  urls = [\"http://localhost/status\"]\n\n  ## HTTP response timeout (default: 5s)\n  response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/circonus-unified-agent/ca.pem\"\n  # tls_cert = \"/etc/circonus-unified-agent/cert.pem\"\n  # tls_key = \"/etc/circonus-unified-agent/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n","logo":{"light":"/img/library/nginx.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:nginx_sts"},{"name":"nginx-upstream-check","title":"NGINX Upstream Check","content":"# NGINX Upstream Check\n\n## Overview\n\nRead the status output of the [nginx_upstream_check](https://github.com/yaoweibin/nginx_upstream_check_module).\nThis module can periodically check the servers in the Nginx's upstream with configured request and interval to determine\nif the server is still available. If checks are failed the server is marked as \"down\" and will not receive any requests\nuntil the check will pass and a server will be marked as \"up\" again.\n\nThe status page displays the current status of all upstreams and servers as well as number of the failed and successful\nchecks. This information can be exported in JSON format and parsed by this input.\n\n## Configuration\n\n```toml\n  ## An URL where Nginx Upstream check module is enabled\n  ## It should be set to return a JSON formatted response\n  url = \"http://127.0.0.1/status?format=json\"\n\n  ## HTTP method\n  # method = \"GET\"\n\n  ## Optional HTTP headers\n  # headers = {\"X-Special-Header\" = \"Special-Value\"}\n\n  ## Override HTTP \"Host\" header\n  # host_header = \"check.example.com\"\n\n  ## Timeout for HTTP requests\n  timeout = \"5s\"\n\n  ## Optional HTTP Basic Auth credentials\n  # username = \"username\"\n  # password = \"pa$$word\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/circonus-unified-agent/ca.pem\"\n  # tls_cert = \"/etc/circonus-unified-agent/cert.pem\"\n  # tls_key = \"/etc/circonus-unified-agent/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n","logo":{"light":"/img/library/nginx.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:nginx_upstream_check"},{"name":"nginx-vts","title":"NGINX Vhost Traffic Status (VTS)","content":"# NGINX Vhost Traffic Status (VTS)\n\n## Overview\n\nThis plugin gathers Nginx status using external virtual host traffic status module - https://github.com/vozlt/nginx-module-vts. This is an Nginx module that provides access to virtual host status information. It contains the current status such as servers, upstreams, caches. This is similar to the live activity monitoring of Nginx plus.\n\nFor module configuration details please see its [documentation](https://github.com/vozlt/nginx-module-vts#synopsis).\n\n## Configuration\n\n```toml\n# Read nginx status information using nginx-module-vts module\n[[inputs.nginx_vts]]\n  ## An array of Nginx status URIs to gather stats.\n  urls = [\"http://localhost/status\"]\n```\n","logo":{"light":"/img/library/nginx.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:nginx_vts"},{"name":"nginx","title":"NGINX","content":"# NGINX\n\n## Configuration\n\n```toml\n# Read Nginx's basic status information (ngx_http_stub_status_module)\n[[inputs.nginx]]\n  ## An array of Nginx stub_status URI to gather stats.\n  urls = [\"http://localhost/server_status\"]\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/circonus-unified-agent/ca.pem\"\n  # tls_cert = \"/etc/circonus-unified-agent/cert.pem\"\n  # tls_key = \"/etc/circonus-unified-agent/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n\n  ## HTTP response timeout (default: 5s)\n  response_timeout = \"5s\"\n```\n","logo":{"light":"/img/library/nginx.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:nginx"},{"name":"nrpe","title":"NRPE","content":"# NRPE\n\n## Overview\n\nThe NRPE Check type checks your system via the Nagios Remote Plugin Executor (NRPE). Circonus will pull the metrics from the specified NRPE enabled host.\n\nThis allows you to remotely monitor machine metrics (disk usage, CPU load, etc.). NRPE can also communicate with some of the Nagios Windows agent addons, so you can execute scripts and check metrics on remote Windows machines as well.\n\n## Configuration\n\nRequired parameters:\n|Name|Description|\n|----|-----------|\n|port|The TCP port on which the NRPE agent can be reached.|\n|command|The command to run on the remote node.|\n\nOptional parameters:\n|Name|Description|\n|----|-----------|\n|append_uom|If the NRPE value is returned with a unit of measure, append it to the metric name. Values are true/false or on/off.|\n|use_ssl|Upgrade the TCP connection to use SSL/TLS (default: true).|\n","logo":{"light":"/img/library/nagios.svg","dark":"/img/library/nagios-dark.svg"},"attributes":{"implementation":"broker"},"tags":["Nagios"],"module":"nrpe"},{"name":"nsd","title":"NSD","content":"# NSD\n\n## Overview\n\nThis plugin gathers stats from [NSD](https://www.nlnetlabs.nl/projects/nsd/about) - an authoritative DNS name server.\n\n## Configuration\n\n```toml\n# A plugin to collect stats from the NSD DNS resolver\n[[inputs.nsd]]\n  ## Address of server to connect to, optionally ':port'. Defaults to the\n  ## address in the nsd config file.\n  server = \"127.0.0.1:8953\"\n\n  ## If running as a restricted user you can prepend sudo for additional access:\n  # use_sudo = false\n\n  ## The default location of the nsd-control binary can be overridden with:\n  # binary = \"/usr/sbin/nsd-control\"\n\n  ## The default location of the nsd config file can be overridden with:\n  # config_file = \"/etc/nsd/nsd.conf\"\n\n  ## The default timeout of 1s can be overridden with:\n  # timeout = \"1s\"\n```\n\n### Permissions\n\nIt's important to note that this plugin references nsd-control, which may\nrequire additional permissions to execute successfully. Depending on the\nuser/group permissions of the user executing this plugin, you may\nneed to alter the group membership, set facls, or use sudo.\n\n**Group membership (Recommended)**\n\n```bash\n$ groups cua\ncua : cua\n\n$ usermod -a -G nsd cua\n\n$ groups cua\ncua : cua nsd\n```\n\n**Sudo privileges**\n\nIf you use this method, you will need the following in your config:\n\n```toml\n[[inputs.nsd]]\n  use_sudo = true\n```\n\nYou will also need to update your sudoers file:\n\n```bash\n$ visudo\n# Add the following line:\nCmnd_Alias NSDCONTROLCTL = /usr/sbin/nsd-control\ncua  ALL=(ALL) NOPASSWD: NSDCONTROLCTL\nDefaults!NSDCONTROLCTL !logfile, !syslog, !pam_session\n```\n\nPlease use the solution you see as most appropriate.\n","logo":{"light":"/img/library/nsd.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:nsd"},{"name":"nsq-consumer","title":"NSQ Consumer","content":"# NSQ Consumer\n\n## Overview\n\nThe [NSQ](https://nsq.io) consumer plugin reads from NSQD and creates metrics using one\nof the supported [input data formats](https://github.com/circonus-labs/circonus-unified-agent/blob/master/docs/DATA_FORMATS_INPUT.md).\n\n## Configuration\n\n```toml\n# Read metrics from NSQD topic(s)\n[[inputs.nsq_consumer]]\n  ## Server option still works but is deprecated, we just prepend it to the nsqd array.\n  # server = \"localhost:4150\"\n\n  ## An array representing the NSQD TCP HTTP Endpoints\n  nsqd = [\"localhost:4150\"]\n\n  ## An array representing the NSQLookupd HTTP Endpoints\n  nsqlookupd = [\"localhost:4161\"]\n  topic = \"circonus\"\n  channel = \"consumer\"\n  max_in_flight = 100\n\n  ## Maximum messages to read from the broker that have not been written by an\n  ## output.  For best throughput set based on the number of metrics within\n  ## each message and the size of the output's metric_batch_size.\n  ##\n  ## For example, if each message from the queue contains 10 metrics and the\n  ## output metric_batch_size is 1000, setting this to 100 will ensure that a\n  ## full batch is collected and the write is triggered immediately without\n  ## waiting until the next flush_interval.\n  # max_undelivered_messages = 1000\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/circonus-labs/circonus-unified-agent/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"json\"\n```\n","logo":{"light":"/img/library/nsq.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:nsq_consumer"},{"name":"nsq","title":"NSQ","content":"# NSQ\n\n## Configuration\n\n```toml\n# Description\n[[inputs.nsq]]\n  ## An array of NSQD HTTP API endpoints\n  endpoints  = [\"http://localhost:4151\"]\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/circonus-unified-agent/ca.pem\"\n  # tls_cert = \"/etc/circonus-unified-agent/cert.pem\"\n  # tls_key = \"/etc/circonus-unified-agent/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n","logo":{"light":"/img/library/nsq.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:nsq"},{"name":"ntp","title":"NTP","content":"# NTP\n\n## Overview\n\nCheck clock skew via Network Time Protocol services.\n\n## Configuration\n\nIf the control parameter is not enabled, the NTP check will determine the time telemetry relative to the Circonus broker's local time. If the control parameter is enabled, the Circonus broker will request the telemetry of the target relative to its preferred peer.\n\nOptional parameters:\n|Name|Description|\n|----|-----------|\n|port|The port used to contact the NTP target host (default: 123).|\n|control|Use NTP control (mode 6) packets to query the server (default: false/off). Note that in the Circonus UI, control defaults to enabled, via the Use Control Protocol checkbox.|\n\n## Metrics\n\nTypical metrics when using control mode include:\n|Name|Type|Description|\n|----|----|-----------|\n|clock_name|text|The name of the currently synced peer.|\n|stratum|numeric|The stratum value of the currently synced peer.|\n|when|numeric|Time difference between when the target received our query and now, calculated with the receive timestamp (rec), or the reference timestamp (reftime) if receive timestamp is not available.|\n|poll|numeric|The current poll interval, in seconds.|\n|delay|numeric|The round-trip delay, in milliseconds, between the target and its currently synced peer.|\n|offset|numeric|The offset of the server clock relative to the system clock.|\n|offset_ms|numeric|Same value as offset above, but expressed in milliseconds.|\n|jitter|numeric|The root-mean-square (RMS) average of the most recent offset differences, representing the nominal error in estimating the offset.|\n|dispersion|numeric|The estimated maximum error of the peer's time source, including network.|\n|xleave|numeric|Interleave delay.|\n|peers|numeric|Count of the target's associated peers.|\n\nTypical metrics when not using control mode include:\n|Name|Type|Description|\n|----|----|-----------|\n|offset|numeric|The offset of the target's clock relative to the broker's clock.|\n|offset_ms|numeric|Same value as offset above, but expressed in milliseconds.|\n|requests|numeric|Number of request packets sent to the target.|\n|responses|numeric|Number of responses received from the target.|\n|stratum|numeric|The stratum value of the target.|\n|poll|numeric|The current poll interval, in seconds.|\n|precision|numeric|Precision of the system clock, in seconds.|\n|rtdisp|numeric|Root dispersion, the total accumulated dispersion to the reference clock in seconds.|\n|rtdelay|numeric|Total round-trip delay to the reference clock.|\n\nFor more details on the metrics below, consult the NTPv4 specification, RFC 5905.\n","logo":{"light":"/img/library/ntp.svg"},"attributes":{"implementation":"broker"},"tags":["network","time","protocol","clock","skew"],"module":"ntp"},{"name":"nvidia-smi","title":"Nvidia SMI","content":"# Nvidia SMI\n\n## Overview\n\nThe Nvidia SMI (System Management Interface) plugin uses a query on the [`nvidia-smi`](https://developer.nvidia.com/nvidia-system-management-interface) binary to pull GPU stats including memory and GPU usage, temp and other.\n\n## Configuration\n\n```toml\n# Pulls statistics from nvidia GPUs attached to the host\n[[inputs.nvidia_smi]]\n  ## Optional: path to nvidia-smi binary, defaults to $PATH via exec.LookPath\n  # bin_path = \"/usr/bin/nvidia-smi\"\n\n  ## Optional: timeout for GPU polling\n  # timeout = \"5s\"\n```\n\n### Windows\n\nOn Windows, `nvidia-smi` is generally located at `C:\\Program Files\\NVIDIA Corporation\\NVSMI\\nvidia-smi.exe`\nOn Windows 10, you may also find this located here `C:\\Windows\\System32\\nvidia-smi.exe`\n\nYou'll need to escape the `\\` within the `circonus-unified-agent.conf` like this: `C:\\\\Program Files\\\\NVIDIA Corporation\\\\NVSMI\\\\nvidia-smi.exe`\n\n## Troubleshooting\n\nCheck the full output by running `nvidia-smi` binary manually.\n\nLinux:\n\n```sh\nsudo -u cua -- /usr/bin/nvidia-smi -q -x\n```\n\nWindows:\n\n```\n\"C:\\Program Files\\NVIDIA Corporation\\NVSMI\\nvidia-smi.exe\" -q -x\n```\n\nPlease include the output of this command if opening an GitHub issue.\n\n### Limitations\n\nNote that there seems to be an issue with getting current memory clock values when the memory is overclocked.\nThis may or may not apply to everyone but it's confirmed to be an issue on an EVGA 2080 Ti.\n","logo":{"light":"/img/library/nvidia.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:nvidia_smi"},{"name":"opcua","title":"OPC UA Client","content":"# OPC UA Client\n\n## Overview\n\nThe `opcua` plugin retrieves data from OPC UA client devices.\n\n## Configuration\n\n```toml\n[[inputs.opcua]]\n  ## Device name\n  # name = \"localhost\"\n  #\n  ## OPC UA Endpoint URL\n  # endpoint = \"opc.tcp://localhost:4840\"\n  #\n  ## Maximum time allowed to establish a connect to the endpoint.\n  # connect_timeout = \"10s\"\n  #\n  ## Maximum time allowed for a request over the estabilished connection.\n  # request_timeout = \"5s\"\n  #\n  ## Security policy, one of \"None\", \"Basic128Rsa15\", \"Basic256\",\n  ## \"Basic256Sha256\", or \"auto\"\n  # security_policy = \"auto\"\n  #\n  ## Security mode, one of \"None\", \"Sign\", \"SignAndEncrypt\", or \"auto\"\n  # security_mode = \"auto\"\n  #\n  ## Path to cert.pem. Required when security mode or policy isn't \"None\".\n  ## If cert path is not supplied, self-signed cert and key will be generated.\n  # certificate = \"/etc/circonus-unified-agent/cert.pem\"\n  #\n  ## Path to private key.pem. Required when security mode or policy isn't \"None\".\n  ## If key path is not supplied, self-signed cert and key will be generated.\n  # private_key = \"/etc/circonus-unified-agent/key.pem\"\n  #\n  ## Authentication Method, one of \"Certificate\", \"UserName\", or \"Anonymous\".  To\n  ## authenticate using a specific ID, select 'Certificate' or 'UserName'\n  # auth_method = \"Anonymous\"\n  #\n  ## Username. Required for auth_method = \"UserName\"\n  # username = \"\"\n  #\n  ## Password. Required for auth_method = \"UserName\"\n  # password = \"\"\n  #\n  ## Node ID configuration\n  ## name             - the variable name\n  ## namespace        - integer value 0 thru 3\n  ## identifier_type  - s=string, i=numeric, g=guid, b=opaque\n  ## identifier       - tag as shown in opcua browser\n  ## data_type        - boolean, byte, short, int, uint, uint16, int16,\n  ##                        uint32, int32, float, double, string, datetime, number\n  ## Example:\n  ## {name=\"ProductUri\", namespace=\"0\", identifier_type=\"i\", identifier=\"2262\", data_type=\"string\", description=\"http://open62541.org\"}\n  nodes = [\n    {name=\"\", namespace=\"\", identifier_type=\"\", identifier=\"\", data_type=\"\", description=\"\"},\n    {name=\"\", namespace=\"\", identifier_type=\"\", identifier=\"\", data_type=\"\", description=\"\"},\n  ]\n```\n\n### Example Node Configuration\n\nAn OPC UA node ID may resemble: \"n=3,s=Temperature\". In this example:\n\n- n=3 is indicating the `namespace` is 3\n- s=Temperature is indicting that the `identifier_type` is a string and `identifier` value is 'Temperature'\n- This example temperature node has a value of 79.0, which makes the `data_type` a 'float'.\n\nTo gather data from this node enter the following line into the 'nodes' property above:\n\n```sh\n{name=\"LabelName\", namespace=\"3\", identifier_type=\"s\", identifier=\"Temperature\", data_type=\"float\", description=\"Description of node\"},\n```\n","logo":{"light":"/img/library/opcua.svg","dark":"/img/library/opcua-dark.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:opcua"},{"name":"openldap","title":"OpenLDAP","content":"# OpenLDAP\n\n## Overview\n\nThis plugin gathers metrics from OpenLDAP's cn=Monitor backend.\n\n## Configuration\n\nTo use this plugin you must enable the [slapd monitoring](https://www.openldap.org/devel/admin/monitoringslapd.html) backend.\n\n```toml\n[[inputs.openldap]]\n  host = \"localhost\"\n  port = 389\n\n  # ldaps, starttls, or no encryption. default is an empty string, disabling all encryption.\n  # note that port will likely need to be changed to 636 for ldaps\n  # valid options: \"\" | \"starttls\" | \"ldaps\"\n  tls = \"\"\n\n  # skip peer certificate verification. Default is false.\n  insecure_skip_verify = false\n\n  # Path to PEM-encoded Root certificate to use to verify server certificate\n  tls_ca = \"/etc/ssl/certs.pem\"\n\n  # dn/password to bind with. If bind_dn is empty, an anonymous bind is performed.\n  bind_dn = \"\"\n  bind_password = \"\"\n\n  # reverse metric names so they sort more naturally\n  # Defaults to false if unset, but is set to true when generating a new config\n  reverse_metric_names = true\n```\n","logo":{"light":"/img/library/openldap.png"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:openldap"},{"name":"openmetrics","title":"OpenMetrics","content":"# OpenMetrics\n\n## Overview\n\nThe OpenMetrics Check uses the OpenMetrics format which is evolved from the Prometheus standard. It pulls metrics from a standard Prometheus scrape endpoint or \"target\". This Check affords the ability to leverage the many [Prometheus Exporters](https://prometheus.io/docs/instrumenting/exporters/) to expose metrics and pull them into the Circonus platform, enabling long-term storage and analytics beyond the capabilities of Prometheus.\n\nEvolved from Prometheus, OpenMetrics is an effort to create an open standard for transmitting metrics at scale, with support for both text representation and Protocol Buffers. For more info about OpenMetrics, please see the [OpenMetrics site](https://openmetrics.io/).\n\n## Configuration\n\nThe only required parameter is the URL to check, including scheme and hostname (as you would type into a browser's location bar.)\n\nOptional parameters:\n\n| Name              | Description                                                |\n| ----------------- | ---------------------------------------------------------- |\n| auth_user         | The user to authenticate as.                               |\n| auth_password     | The password to use during authentication.                 |\n| header name/value | Include an arbitrary header in the HTTP request.           |\n| host              | The Host header value to include in the HTTP request.      |\n| port              | TCP port to which the broker should connect (default: 80)  |\n| use_ssl           | Upgrade the TCP connection to use SSL/TLS (default: false) |\n","logo":{"light":"/img/library/openmetrics.svg"},"attributes":{"implementation":"broker"},"tags":["prometheus","scraper","open","metrics"],"module":"promtext"},{"name":"openntpd","title":"OpenNTPD","content":"# OpenNTPD\n\n## Overview\n\nGet standard NTP query metrics from [OpenNTPD](http://www.openntpd.org/) using the ntpctl command.\n\nBelow is the documentation of the various headers returned from the NTP query\ncommand when running `ntpctl -s peers`.\n\n- remote – The remote peer or server being synced to.\n- wt – the peer weight\n- tl – the peer trust level\n- st (stratum) – The remote peer or server Stratum\n- next – number of seconds until the next poll\n- poll – polling interval in seconds\n- delay – Round trip communication delay to the remote peer\n  or server (milliseconds);\n- offset – Mean offset (phase) in the times reported between this local host and\n  the remote peer or server (RMS, milliseconds);\n- jitter – Mean deviation (jitter) in the time reported for that remote peer or\n  server (RMS of difference of multiple time samples, milliseconds);\n\n## Configuration\n\n```toml\n[[inputs.openntpd]]\n  ## Run ntpctl binary with sudo.\n  # use_sudo = false\n\n  ## Location of the ntpctl binary.\n  # binary = \"/usr/sbin/ntpctl\"\n\n  ## Maximum time the ntpctl binary is allowed to run.\n  # timeout = \"5ms\"\n```\n\n### Permissions\n\nIt's important to note that this plugin references ntpctl, which may require\nadditional permissions to execute successfully.\nDepending on the user/group permissions of the user executing this\nplugin, you may need to alter the group membership, set facls, or use sudo.\n\n**Group membership (Recommended)**\n\n```bash\n$ groups cua\ncua : cua\n\n$ usermod -a -G ntpd cua\n\n$ groups cua\ncua : cua ntpd\n```\n\n**Sudo privileges**\n\nIf you use this method, you will need the following in your circonus-unified-agent config:\n\n```toml\n[[inputs.openntpd]]\n  use_sudo = true\n```\n\nYou will also need to update your sudoers file:\n\n```bash\n$ visudo\n# Add the following lines:\nCmnd_Alias NTPCTL = /usr/sbin/ntpctl\ncua ALL=(ALL) NOPASSWD: NTPCTL\nDefaults!NTPCTL !logfile, !syslog, !pam_session\n```\n\nPlease use the solution you see as most appropriate.\n","logo":{"light":"/img/library/openntpd.png"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:openntpd"},{"name":"opensmtpd","title":"OpenSMTPD","content":"# OpenSMTPD\n\n## Overview\n\nThis plugin gathers stats from [OpenSMTPD - a FREE implementation of the server-side SMTP protocol](https://www.opensmtpd.org/)\n\n## Configuration\n\n```toml\n [[inputs.opensmtpd]]\n   ## If running as a restricted user you can prepend sudo for additional access:\n   #use_sudo = false\n\n   ## The default location of the smtpctl binary can be overridden with:\n   binary = \"/usr/sbin/smtpctl\"\n\n   # The default timeout of 1s can be overridden with:\n   #timeout = \"1s\"\n```\n\n### Permissions\n\nIt's important to note that this plugin references smtpctl, which may require additional permissions to execute successfully.\nDepending on the user/group permissions of the user executing this plugin, you may need to alter the group membership, set facls, or use sudo.\n\n**Group membership (Recommended)**\n\n```bash\n$ groups cua\ncua : cua\n\n$ usermod -a -G opensmtpd cua\n\n$ groups cua\ncua : cua opensmtpd\n```\n\n**Sudo privileges**\n\nIf you use this method, you will need the following in your circonus-unified-agent config:\n\n```toml\n[[inputs.opensmtpd]]\n  use_sudo = true\n```\n\nYou will also need to update your sudoers file:\n\n```bash\n$ visudo\n# Add the following line:\nCmnd_Alias SMTPCTL = /usr/sbin/smtpctl\ncua  ALL=(ALL) NOPASSWD: SMTPCTL\nDefaults!SMTPCTL !logfile, !syslog, !pam_session\n```\n\nPlease use the solution you see as most appropriate.\n","logo":{"light":"/img/library/opensmtpd.png"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:opensmtpd"},{"name":"opentelemetry","title":"OpenTelemetry","content":"# OpenTelemetry\n\n## Overview\n\nThe OpenTelemetry module provides a simple way to push native OpenTelemetry data into reconnoiter via the otlphttp exporter.\n\n## Configuration\n\nSimply configure otel collector to write to the provided URL in its otlphttp endpoint.\n\n### Example\n\n```\n  exporters:\n      otlphttp:\n        metrics_endpoint: https://broker:43191/module/otlphttp/1b4e28ba-2fa1-11d2-893f-e9b761bde3fb/s3cr3tk3y\n```\n","logo":{"light":"/img/library/opentelemetry.svg"},"attributes":{"implementation":"broker"},"module":"otlphttp"},{"name":"opentsdb","title":"OpenTSDB","content":"# OpenTSDB\n\n## Overview\n\nThe OpenTSDB Check allows you to submit your OpenTSDB formatted metrics directly to Circonus, improving ingestion throughput, enabling long-term storage and enabling advanced analytics of your OpenTSDB oriented data.\n\nThis check is intended to be run on Enterprise Brokers.\n\nOpenTSDB is an open-source time-series database built on top of Hadoop and HBase. It allows users to store and analyze time-series data.\n","logo":{"light":"/img/library/opentsdb.svg"},"attributes":{"implementation":"broker"},"module":"opentsdb"},{"name":"openweathermap","title":"OpenWeatherMap","content":"# OpenWeatherMap\n\n## Overview\n\nCollect current weather and forecast data from OpenWeatherMap.\n\nTo use this plugin you will need an [api key](https://openweathermap.org/appid) (app_id).\n\nCity identifiers can be found in the [city list](http://bulk.openweathermap.org/sample/city.list.json.gz). Alternately you can [search](https://openweathermap.org/find) by name; the `city_id` can be found as the last digits of the URL: https://openweathermap.org/city/2643743. Language identifiers can be found in the [lang list](https://openweathermap.org/current#multi). Documentation for condition ID, icon, and main is at [weather conditions](https://openweathermap.org/weather-conditions).\n\n## Configuration\n\n```toml\n[[inputs.openweathermap]]\n  ## OpenWeatherMap API key.\n  app_id = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n\n  ## City ID's to collect weather data from.\n  city_id = [\"5391959\"]\n\n  ## Language of the description field. Can be one of \"ar\", \"bg\",\n  ## \"ca\", \"cz\", \"de\", \"el\", \"en\", \"fa\", \"fi\", \"fr\", \"gl\", \"hr\", \"hu\",\n  ## \"it\", \"ja\", \"kr\", \"la\", \"lt\", \"mk\", \"nl\", \"pl\", \"pt\", \"ro\", \"ru\",\n  ## \"se\", \"sk\", \"sl\", \"es\", \"tr\", \"ua\", \"vi\", \"zh_cn\", \"zh_tw\"\n  # lang = \"en\"\n\n  ## APIs to fetch; can contain \"weather\" or \"forecast\".\n  fetch = [\"weather\", \"forecast\"]\n\n  ## OpenWeatherMap base URL\n  # base_url = \"https://api.openweathermap.org/\"\n\n  ## Timeout for HTTP response.\n  # response_timeout = \"5s\"\n\n  ## Preferred unit system for temperature and wind speed. Can be one of\n  ## \"metric\", \"imperial\", or \"standard\".\n  # units = \"metric\"\n\n  ## Query interval; OpenWeatherMap weather data is updated every 10\n  ## minutes.\n  interval = \"10m\"\n```\n","logo":{"light":"/img/library/openweathermap.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:openweathermap"},{"name":"oracle","title":"Oracle","content":"# Oracle\n\n## Overview\n\nThe Oracle database Circonus Unified Agent (CUA) input plugin collects metrics from Oracle's RDBMS using dynamic performance views.\n\n**_This document is oriented around the Oracle database installed on a Windows host._**\n\n## Configuration\n\nThere are a few configuration options available for collecting telemetry from an Oracle database where the database is installed on a Windows host. A couple examples are included below to help guide you through the process.\n\n### Configuration Example 1\n\n**_In this example, the Oracle database is installed on a Windows host and you are collecting the metrics from CUA that is installed on this same Windows host._**\n\nThis configuration executes a sidecar powered by python3 to gather metrics from an Oracle database. The installation of [python3](https://www.python.org/downloads/) and the [cx_Oracle](https://cx-oracle.readthedocs.io/en/latest/user_guide/installation.html) extension module are required to run the sidecar.\n\nHere is an example configuration for the CUA Oracle input plugin (Windows CUA):\n\n```toml\n# ##Input Plugin for Oracle Database (Windows CUA)\n[[inputs.exec]]\n  # Specify an instance_id for the oracle database\n  instance_id = \"\" # Required\n  commands = ['python \"C:\\\\Program Files\\\\Circonus\\\\Circonus-Unified-Agent\\\\external_plugins\\\\oracle\\\\oracle_metrics.py\" --dsn \"(DESCRIPTION=(ADDRESS=(PROTOCOL=<TCP or UDP>)(HOST=<hostIp>)(PORT=1521))(CONNECT_DATA=(SERVICE_NAME=<serviceName>)))\" --user \"<userName>\" --password \"<password>\" --instance \"<instanceName>\"']\n  timeout = \"60s\"\n  data_format = \"influx\"\n#  ## Execution interval, can override default interval setting in [agent] section\n  interval = \"60s\"\n```\n\n- Create an `instance_id` tag value that describes this Oracle database deployment.\n- Modify the [connection string](https://cx-oracle.readthedocs.io/en/latest/user_guide/connection_handling.html#connection-strings) to connect with your database.\n- Create an Oracle username and password with SELECT_CATALOG_ROLE granted, and input those credentials into the connection string above.\n- Define the execution interval that the collection happens on. If left commented out, the execution interval defaults to the current CUA polling interval.\n\n### Configuration Example 2\n\n**_In this example, the Oracle database is installed on a Windows host and you are collecting the metrics from CUA that is installed on a remote Linux host._**\n\nThis configuration executes a sidecar powered by python3 and Oracle Instant Client to gather metrics from an Oracle database. The installation of [python3](https://www.python.org/downloads/) and the [cx_Oracle](https://cx-oracle.readthedocs.io/en/latest/user_guide/installation.html) extension module are required to run the sidecar.\n\nInstall [Oracle Instant Client](https://www.oracle.com/database/technologies/instant-client/downloads.html). The Oracle Instant Client will require network configurations that are specific to your deployment model to enable remote connectivity. The remote network configuration will requre IP address modification in the client and in the Oracle Databases's `tnsnames.ora` and `listner.ora` files. These files can be found in your Oracle Database directory. Example: `C:\\OracleApp\\WINDOWS.X64_193000_db_home\\network\\admin`\n\nHere is an example configuration for the CUA Oracle input plugin (Linux CUA):\n\n```toml\n# ##Input Plugin for Oracle Database (Linux CUA)\n[[inputs.exec]]\n  # Specify an instance_id for the oracle database\n  instance_id = \"\" # Required\n  commands = ['python3 \"/opt/circonus/unified-agent/external_plugins/oracle_metrics.py\" --dsn \"(DESCRIPTION=(ADDRESS=(PROTOCOL=<TCP or UDP>)(HOST=<hostIp>)(PORT=1521))(CONNECT_DATA=(SERVICE_NAME=<serviceName>)))\" --user \"<userName>\" --password \"<password>\" --instance \"<instanceName>\"']\n  timeout = \"60s\"\n  data_format = \"influx\"\n#  ## Execution interval, can override default interval setting in [agent] section\n  interval = \"60s\"\n\n```\n\n- Create an `instance_id` tag value that describes this Oracle database deployment.\n- Modify the [connection string](https://cx-oracle.readthedocs.io/en/latest/user_guide/connection_handling.html#connection-strings) to connect with your database.\n- Create an Oracle username and password with SELECT_CATALOG_ROLE granted, and input those credentials into the connection string above.\n- Define the execution interval that the collection happens on. If left commented out, the execution interval defaults to the current CUA polling interval.\n","logo":{"light":"/img/library/oracle.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:oracle"},{"name":"pgbouncer","title":"PgBouncer","content":"# PgBouncer\n\n## Overview\n\nThe `pgbouncer` plugin provides metrics for your PgBouncer load balancer.\n\nMore information about the meaning of these metrics can be found in the\n[PgBouncer Documentation](https://pgbouncer.github.io/usage.html).\n\n- PgBouncer minimum tested version: 1.5\n\n## Configuration\n\n### Configuration example\n\n```toml\n[[inputs.pgbouncer]]\n  ## specify address via a url matching:\n  ##   postgres://[pqgotest[:password]]@host:port[/dbname]\\\n  ##       ?sslmode=[disable|verify-ca|verify-full]\n  ## or a simple string:\n  ##   host=localhost port=5432 user=pqgotest password=... sslmode=... dbname=app_production\n  ##\n  ## All connection parameters are optional.\n  ##\n  address = \"host=localhost user=pgbouncer sslmode=disable\"\n```\n\n#### `address`\n\nSpecify address via a postgresql connection string:\n\n`host=/run/postgresql port=6432 user=cua database=pgbouncer`\n\nOr via an url matching:\n\n`postgres://[pqgotest[:password]]@host:port[/dbname]?sslmode=[disable|verify-ca|verify-full]`\n\nAll connection parameters are optional.\n\nWithout the dbname parameter, the driver will default to a database with the same name as the user.\nThis dbname is just for instantiating a connection with the server and doesn't restrict the databases we are trying to grab metrics for.\n","logo":{"light":"/img/library/postgresql.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:pgbouncer"},{"name":"php-fpm","title":"PHP-FPM","content":"# PHP-FPM\n\n## Overview\n\nGet phpfpm stats using either HTTP status page or fpm socket.\n\n## Configuration\n\n```toml\n# Read metrics of phpfpm, via HTTP status page or socket\n[[inputs.phpfpm]]\n  ## An array of addresses to gather stats about. Specify an ip or hostname\n  ## with optional port and path\n  ##\n  ## Plugin can be configured in three modes (either can be used):\n  ##   - http: the URL must start with http:// or https://, ie:\n  ##       \"http://localhost/status\"\n  ##       \"http://192.168.130.1/status?full\"\n  ##\n  ##   - unixsocket: path to fpm socket, ie:\n  ##       \"/var/run/php5-fpm.sock\"\n  ##      or using a custom fpm status path:\n  ##       \"/var/run/php5-fpm.sock:fpm-custom-status-path\"\n  ##      glob patterns are also supported:\n  ##       \"/var/run/php*.sock\"\n  ##\n  ##   - fcgi: the URL must start with fcgi:// or cgi://, and port must be present, ie:\n  ##       \"fcgi://10.0.0.12:9000/status\"\n  ##       \"cgi://10.0.10.12:9001/status\"\n  ##\n  ## Example of multiple gathering from local socket and remote host\n  ## urls = [\"http://192.168.1.20/status\", \"/tmp/fpm.sock\"]\n  urls = [\"http://localhost/status\"]\n\n  ## Duration allowed to complete HTTP requests.\n  # timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/circonus-unified-agent/ca.pem\"\n  # tls_cert = \"/etc/circonus-unified-agent/cert.pem\"\n  # tls_key = \"/etc/circonus-unified-agent/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n\nWhen using `unixsocket`, you have to ensure that agent runs on same\nhost, and socket path is accessible to `cua` user.\n","logo":{"light":"/img/library/php.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:phpfpm"},{"name":"phusion-passenger","title":"Phusion Passenger","content":"# Phusion Passenger\n\n## Overview\n\nGather [Phusion Passenger](https://www.phusionpassenger.com/) metrics using the `passenger-status` command line utility.\n\n**Series Cardinality Warning**\n\nDepending on your environment, this `passenger_process` measurement of this\nplugin can quickly create a high number of series which, when unchecked, can\ncause high load. You can use the following techniques to\nmanage your series cardinality:\n\n- Use the\n  [measurement filtering](https://github.com/circonus-labs/circonus-unified-agent/blob/master/docs/CONFIGURATION.md#measurement-filtering)\n  options to exclude unneeded tags. In some environments, you may wish to use\n  `tagexclude` to remove the `pid` and `process_group_id` tags.\n- Add `__rollup:false` tag to limit cardinality and manage retention.\n\n## Configuration\n\n```toml\n# Read metrics of passenger using passenger-status\n[[inputs.passenger]]\n  ## Path of passenger-status.\n  ##\n  ## Plugin gather metric via parsing XML output of passenger-status\n  ## More information about the tool:\n  ##   https://www.phusionpassenger.com/library/admin/apache/overall_status_report.html\n  ##\n  ## If no path is specified, then the plugin simply execute passenger-status\n  ## hopefully it can be found in your PATH\n  command = \"passenger-status -v --show=xml\"\n```\n\n### Permissions:\n\nAgent must have permission to execute the `passenger-status` command. On most systems, agent runs as the `cua` user.\n","logo":{"light":"/img/library/phusion-passenger.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:passenger"},{"name":"ping-legacy","title":"Ping","content":"# Ping\n\n## Overview\n\nThe Ping Check tests the availability of a host on a network and measures the round-trip time for messages exchanged with that host through the Internet Control Message Protocol (ICMP) protocol.\n\nPing sends ICMP echo request packets to the target host and then waits for a response. It measures the elapsed time between the initial transmission and receiving the response (the round-trip time) and records any packet loss.\n\n## Configuration\n\nNote that the timeout should be adjusted appropriately when changing the number of packets and/or the packet interval. A minimum of packets × interval is required to ensure all packets can be sent before a timeout occurs.\n\nOptional parameters:\n|Name|Description|\n|----|-----------|\n|count|The number of ICMP echo request packets to send (default: 5).|\n|interval_seconds|The time, in seconds, between sending each ICMP echo request packet (default: 2).|\n|timeout_seconds|The maximum time, in seconds, to wait for responses to all echo request packets (default: 10).|\n\n## Metrics\n\nTypical metrics include:\n|Name|Type|Description|\n|----|----|-----------|\n|count|numeric|The number of ICMP packets sent.|\n|available|numeric|The percentage of ICMP requests that received a reply.|\n|minimum|numeric|The lowest latency for a request-response among the responses received.|\n|maximum|numeric|The highest latency for a request-response among the responses received.|\n|average|numeric|The average response latency over all the requests sent by the check.|\n","logo":{"light":"/img/library/ping.svg"},"attributes":{"implementation":"broker","legacy":true},"tags":["protocol","system","availability","network","latency"],"module":"ping_icmp"},{"name":"ping","title":"Ping","content":"# Ping\n\n## Overview\n\nSends a ping message by executing the system ping command and reports the results.\n\nThis plugin has two main methods of operation: `exec` and `native`. The\nrecommended method is `native`, which has greater system compatibility and\nperformance.\n\nWhen using `method = \"exec\"`, the systems ping utility is executed to send the\nping packets.\n\nMost ping command implementations are supported, one notable exception being\nthat there is currently no support for GNU Inetutils ping. You may instead use\nthe iputils-ping implementation:\n\n```sh\napt-get install iputils-ping\n```\n\nWhen using `method = \"native\"` a ping is sent and the results are reported in\nnative Go by the agent process, eliminating the need to execute the system\n`ping` command.\n\n## Configuration\n\n```toml\n[[inputs.ping]]\n  instance_id = \"\" ## REQUIRED\n\n  ## Hosts to send ping packets to.\n  urls = [\"example.org\"]\n\n  ## Method used for sending pings, can be either \"exec\" or \"native\".  When set\n  ## to \"exec\" the systems ping command will be executed.  When set to \"native\"\n  ## the plugin will send pings directly.\n  ##\n  # method = \"native\"\n\n  ## Number of ping packets to send per interval.  Corresponds to the \"-c\"\n  ## option of the ping command.\n  # count = 3\n\n  ## Time to wait between sending ping packets in seconds.  Operates like the\n  ## \"-i\" option of the ping command.\n  # ping_interval = 1.0\n\n  ## If set, the time to wait for a ping response in seconds.  Operates like\n  ## the \"-W\" option of the ping command.\n  # timeout = 1.0\n\n  ## If set, the total ping deadline, in seconds.  Operates like the -w option\n  ## of the ping command.\n  # deadline = 10\n\n  ## Interface or source address to send ping from.  Operates like the -I or -S\n  ## option of the ping command.\n  # interface = \"\"\n\n  ## Percentiles to calculate. This only works with the native method.\n  # percentiles = [50, 95, 99]\n\n  ## Specify the ping executable binary.\n  # binary = \"ping\"\n\n  ## Arguments for ping command. When arguments is not empty, the command from\n  ## the binary option will be used and other options (ping_interval, timeout,\n  ## etc) will be ignored.\n  # arguments = [\"-c\", \"3\"]\n\n  ## Use only IPv6 addresses when resolving a hostname.\n  # ipv6 = false\n\n  ## Number of data bytes to be sent. Corresponds to the \"-s\"\n  ## option of the ping command. This only works with the native method.\n  # size = 56\n```\n\n### File Limit\n\nSince this plugin runs the ping command, it may need to open multiple files per\nhost. The number of files used is lessened with the `native` option but still\nmany files are used. With a large host list you may receive a `too many open\nfiles` error.\n\nTo increase this limit on platforms using systemd the recommended method is to\nuse the \"drop-in directory\", usually located at\n`/etc/systemd/system/circonus-unified-agent.service.d`.\n\nYou can create or edit a drop-in file in the correct location using:\n\n```sh\nsystemctl edit circonus-unified-agent\n```\n\nIncrease the number of open files:\n\n```ini\n[Service]\nLimitNOFILE=8192\n```\n\nRestart circonus-unified-agent:\n\n```sh\nsystemctl edit circonus-unified-agent\n```\n\n### Linux Permissions\n\nWhen using `method = \"native\"`, agent will attempt to use privileged raw\nICMP sockets. On most systems, doing so requires `CAP_NET_RAW` capabilities.\n\nWith systemd:\n\n```sh\nsystemctl edit circonus-unified-agent\n```\n\n```ini\n[Service]\nCapabilityBoundingSet=CAP_NET_RAW\nAmbientCapabilities=CAP_NET_RAW\n```\n\n```sh\nsystemctl restart circonus-unified-agent\n```\n\nWithout systemd:\n\n```sh\nsetcap cap_net_raw=eip /opt/circonus/unified-agent/sbin/circonus-unified-agentd\n```\n\nReference [`man 7 capabilities`](http://man7.org/linux/man-pages/man7/capabilities.7.html) for more information about\nsetting capabilities.\n\nWhen agent cannot listen on a privileged ICMP socket it will attempt to use\nICMP echo sockets. If you wish to use this method you must ensure agent's\ngroup, usually `cua`, is allowed to use ICMP echo sockets:\n\n```sh\nsysctl -w net.ipv4.ping_group_range=\"GROUP_ID_LOW   GROUP_ID_HIGH\"\n```\n\nReference [`man 7 icmp`](http://man7.org/linux/man-pages/man7/icmp.7.html) for more information about ICMP echo\nsockets and the `ping_group_range` setting.\n","logo":{"light":"/img/library/ping.svg"},"attributes":{"implementation":"cua"},"tags":["protocol","system","availability","network","latency"],"module":"httptrap:cua:ping"},{"name":"pop3","title":"POP3","content":"# POP3\n\n# Overview\n\nThis check type checks mail retrieval with the Post Office Protocol 3 (POP3).\n\nThe check performs a login, followed by a STAT command, after which the associated telemetry is recorded.\n\n## Configuration\n\nRequired parameters:\n|Name|Description|\n|----|-----------|\n|port|The TCP port to which the POP3 check will connect (default: 110).|\n|username|The user name required for access.|\n|password|The password required for access.|\n\nOptional parameters:\n|Name|Description|\n|----|-----------|\n|use_ssl|Upgrade the TCP connection to use SSL/TLS (default: false).|\n|expected_certificate_name|The expected subject name of the server's certificate.|\n\n## Metrics\n\nMetrics\n\nTypical metrics include:\n|Name|Type|Description|\n|----|----|-----------|\n|tt_firstbyte|numeric|The elapsed time between initiating the connection to receiving the first byte of the banner.|\n|banner|text|The initial greeting banner sent by the server after connection.|\n|login`duration|numeric|The elapsed time from the start of the login to login response.|\n|login`status|text|A string representing the success or failure of the login.|\n|stat`duration|numeric|The elapsed time from the start of the STAT operation to STAT response.|\n|stat`message_count|numeric|The number of messages in the mailbox.|\n|stat`message_size|numeric|The total number of bytes used by the mailbox.|\n|quit|text|The server's response to the QUIT command, if the login was successful.|\n","logo":{"light":"/img/library/pop3.svg"},"attributes":{"implementation":"broker"},"tags":["email","protocol"],"module":"pop3"},{"name":"postfix","title":"Postfix","content":"# Postfix\n\n## Overview\n\nThe postfix plugin reports metrics on the postfix queues.\n\nFor each of the active, hold, incoming, maildrop, and deferred queues\n([http://www.postfix.org/QSHAPE_README.html#queues](http://www.postfix.org/QSHAPE_README.html#queues)), it will report the queue\nlength (number of items), size (bytes used by items), and age (age of oldest\nitem in seconds).\n\n## Configuration\n\n```toml\n[[inputs.postfix]]\n  ## Postfix queue directory. If not provided, agent will try to use\n  ## 'postconf -h queue_directory' to determine it.\n  # queue_directory = \"/var/spool/postfix\"\n```\n\n### Permissions\n\nAgent will need read access to the files in the queue directory. You may\nneed to alter the permissions of these directories to provide access to the\ncua user.\n\nThis can be setup either using standard unix permissions or with Posix ACLs,\nyou will only need to use one method:\n\nUnix permissions:\n\n```sh\n$ sudo chgrp -R cua /var/spool/postfix/{active,hold,incoming,deferred}\n$ sudo chmod -R g+rXs /var/spool/postfix/{active,hold,incoming,deferred}\n$ sudo usermod -a -G postdrop cua\n$ sudo chmod g+r /var/spool/postfix/maildrop\n```\n\nPosix ACL:\n\n```sh\n$ sudo setfacl -Rm g:cua:rX /var/spool/postfix/\n$ sudo setfacl -dm g:cua:rX /var/spool/postfix/\n```\n","logo":{"light":"/img/library/postfix.png"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:postfix"},{"name":"postgresql-legacy","title":"PostgreSQL","content":"# PostgreSQL\n\n## Overview\n\nThe PostgreSQL Check allows querying of your PostgreSQL database, enabling you to collect arbitrary metrics stored therein.\n\nPostgreSQL, often simply called Postgres, is a free open source object-relational database management system with an emphasis on extensibility and standards compliance.\n\n## Configuration\n\nIf you are configuring a Postgres check via the Circonus API, the values for host, database, username, and password are combined into a single parameter called dsn: host=5.4.3.2 port=5432 username=myuser password=mypass dbname=test\n\nRequired parameters:\n|Name|Description|\n|----|-----------|\n|database|The name of the database to which the check will connect to run a query.|\n|sql|The SQL query to run. There are a number of predefined options, or you may enter a custom query in the SQL Query field.|\n|port|The TCP port to which the check will connect to PostgreSQL (default: 5432).|\n\nOptional parameters:\n|Name|Description|\n|----|-----------|\n|username|The user name for server authorization.|\n|password|The password for server authorization.|\n|sslmode|Upgrade the TCP connection to use SSL/TLS (default: false/off).|\n\nAdditional options allow you to set Server Authorization, use SSL, or to change the default Period (60 second), Timeout (10 seconds), and Port (5432).\n\n## Metrics\n\nPre-defined SQL queries will populate the SQL Query field for you, from there you can customize it to suit your needs. Pre-defined SQL queries for PostgreSQL checks include:\n\n- autovac\n- connections\n- tables\n- transactions\n- wal_files\n\nIt’s also possible to collect arbitrary metrics with this check. Given this sample database table:\n\n|      | Col1  | Col2 | Col3 |\n| ---- | ----- | ---- | ---- |\n| Row1 | Name1 | Val1 | Val3 |\n| Row2 | Name2 | Val2 | Val4 |\n\nCirconus would parse the above table into the following metrics:\n|Name|Value|\n|----|-----|\n|Name1`Col2|Val1|\n|Name1`Col3|Val3|\n|Name2`Col2|Val2|\n|Name2`Col3|Val4|\n\nTutorials exist [online](http://www.w3schools.com/sql/default.asp) for those unfamiliar with SQL.\n","logo":{"light":"/img/library/postgresql.svg"},"attributes":{"implementation":"broker","legacy":true},"tags":["database","SQL"],"module":"postgres"},{"name":"postgresql","title":"PostgreSQL","content":"# PostgreSQL\n\n## Overview\n\nThis postgresql plugin provides metrics for your postgres database. It currently works with postgres versions 8.1+. It uses data from the built in _pg_stat_database_ and pg_stat_bgwriter views. The metrics recorded depend on your version of postgres. See table:\n\n```\npg version      9.2+   9.1   8.3-9.0   8.1-8.2   7.4-8.0(unsupported)\n---             ---    ---   -------   -------   -------\ndatid            x      x       x         x\ndatname          x      x       x         x\nnumbackends      x      x       x         x         x\nxact_commit      x      x       x         x         x\nxact_rollback    x      x       x         x         x\nblks_read        x      x       x         x         x\nblks_hit         x      x       x         x         x\ntup_returned     x      x       x\ntup_fetched      x      x       x\ntup_inserted     x      x       x\ntup_updated      x      x       x\ntup_deleted      x      x       x\nconflicts        x      x\ntemp_files       x\ntemp_bytes       x\ndeadlocks        x\nblk_read_time    x\nblk_write_time   x\nstats_reset*     x      x\n```\n\n_\\* value ignored and therefore not recorded._\n\nMore information about the meaning of these metrics can be found in the [PostgreSQL Documentation](http://www.postgresql.org/docs/9.2/static/monitoring-stats.html#PG-STAT-DATABASE-VIEW)\n\n## Configuration\n\nSpecify address via a postgresql connection string:\n\n`host=localhost port=5432 user=cua database=cua`\n\nOr via an url matching:\n\n`postgres://[pqgotest[:password]]@host:port[/dbname]?sslmode=[disable|verify-ca|verify-full]`\n\nAll connection parameters are optional. Without the dbname parameter, the driver will default to a database with the same name as the user. This dbname is just for instantiating a connection with the server and doesn't restrict the databases we are trying to grab metrics for.\n\nA list of databases to explicitly ignore. If not specified, metrics for all databases are gathered. Do NOT use with the 'databases' option.\n\n`ignored_databases = [\"postgres\", \"template0\", \"template1\"]`\n\nA list of databases to pull metrics about. If not specified, metrics for all databases are gathered. Do NOT use with the 'ignored_databases' option.\n\n`databases = [\"app_production\", \"testing\"]`\n\n### TLS Configuration\n\nAdd the `sslkey`, `sslcert` and `sslrootcert` options to your DSN:\n\n```\nhost=localhost user=pgotest dbname=app_production sslmode=require sslkey=/etc/circonus-unified-agent/key.pem sslcert=/etc/circonus-unified-agent/cert.pem sslrootcert=/etc/circonus-unified-agent/ca.pem\n```\n\n### Configuration example\n\n```toml\n[[inputs.postgresql]]\n  address = \"postgres://cua@localhost/someDB\"\n  ignored_databases = [\"template0\", \"template1\"]\n```\n","logo":{"light":"/img/library/postgresql.svg"},"attributes":{"implementation":"cua"},"tags":["database","SQL"],"module":"httptrap:cua:postgresql"},{"name":"powerdns-recursor","title":"PowerDNS Recursor","content":"# PowerDNS Recursor\n\n## Overview\n\nThe `powerdns_recursor` plugin gathers metrics about PowerDNS Recursor using\nthe unix controlsocket.\n\n## Configuration\n\n```toml\n[[inputs.powerdns_recursor]]\n  ## Path to the Recursor control socket.\n  unix_sockets = [\"/var/run/pdns_recursor.controlsocket\"]\n\n  ## Directory to create receive socket.  This default is likely not writable,\n  ## please reference the full plugin documentation for a recommended setup.\n  # socket_dir = \"/var/run/\"\n  ## Socket permissions for the receive socket.\n  # socket_mode = \"0666\"\n```\n\n### Permissions\n\nAgent will need read/write access to the control socket and to the\n`socket_dir`. PowerDNS will need to be able to write to the `socket_dir`.\n\nThe setup described below was tested on a Debian Stretch system and may need\nadapted for other systems.\n\nFirst change permissions on the controlsocket in the PowerDNS recursor\nconfiguration, usually in `/etc/powerdns/recursor.conf`:\n\n```\nsocket-mode = 660\n```\n\nThen place the `cua` user into the `pdns` group:\n\n```\nusermod cua -a -G pdns\n```\n\nSince `circonus-unified-agent` cannot write to to the default `/var/run` socket directory,\ncreate a subdirectory and adjust permissions for this directory so that both\nusers can access it.\n\n```sh\n$ mkdir /var/run/pdns\n$ chown root:pdns /var/run/pdns\n$ chmod 770 /var/run/pdns\n```\n","logo":{"light":"/img/library/powerdns.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:powerdns_recursor"},{"name":"powerdns","title":"PowerDNS","content":"# PowerDNS\n\n## Overview\n\nThe powerdns plugin gathers metrics about PowerDNS using unix socket.\n\n## Configuration\n\n```toml\n# Description\n[[inputs.powerdns]]\n  # An array of sockets to gather stats about.\n  # Specify a path to unix socket.\n  #\n  # If no servers are specified, then '/var/run/pdns.controlsocket' is used as the path.\n  unix_sockets = [\"/var/run/pdns.controlsocket\"]\n```\n\n### Permissions\n\nAgent will need read access to the powerdns control socket.\n\nOn many systems this can be accomplished by adding the `cua` user to the\n`pdns` group:\n\n```\nusermod cua -a -G pdns\n```\n","logo":{"light":"/img/library/powerdns.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:powerdns"},{"name":"procstat","title":"ProcStat","content":"# ProcStat\n\n## Overview\n\nThe procstat plugin can be used to monitor the system resource usage of one or more processes. The procstat_lookup metric displays the queried information, specifically the number of PIDs returned on a search.\n\nProcesses can be selected for monitoring using one of several methods:\n\npidfile\nexe\npattern\nuser\nsystemd_unit\ncgroup\n\n## Configuration\n\n```toml\n# Monitor process cpu and memory usage\n[[inputs.procstat]]\n  ## PID file to monitor process\n  pid_file = \"/var/run/nginx.pid\"\n  ## executable name (ie, pgrep <exe>)\n  # exe = \"nginx\"\n  ## pattern as argument for pgrep (ie, pgrep -f <pattern>)\n  # pattern = \"nginx\"\n  ## user as argument for pgrep (ie, pgrep -u <user>)\n  # user = \"nginx\"\n  ## Systemd unit name, supports globs when include_systemd_children is set to true\n  # systemd_unit = \"nginx.service\"\n  # include_systemd_children = false\n  ## CGroup name or path, supports globs\n  # cgroup = \"systemd/system.slice/nginx.service\"\n\n  ## Plugin specific polling interval, this will override global polling rate.\n  # interval = “60s”\n\n  ## Multiple [[inputs.procstat]] configurations can be utilized to define multiple unique search queries. If you do utilize this method, the “instance_id” needs to be unique for each configuration.\n\n  ## override for process_name\n  ## This is optional; default is sourced from /proc/<pid>/status\n  # process_name = \"bar\"\n\n  ## Field name prefix\n  # prefix = \"\"\n\n  ## When true add the full cmdline as a tag.\n  # cmdline_tag = false\n\n  ## Mode to use when calculating CPU usage. Can be one of 'solaris' or 'irix'.\n  # mode = \"irix\"\n\n\n  ## Add the PID as a tag instead of as a field.  When collecting multiple\n  ## processes with otherwise matching tags this setting should be enabled to\n  ## ensure each process has a unique identity.\n  ##\n  ## Enabling this option may result in a large number of series, especially\n  ## when processes have a short lifetime.\n  # pid_tag = false\n\n  ## Method to use when finding process IDs.  Can be one of 'pgrep', or\n  ## 'native'.  The pgrep finder calls the pgrep executable in the PATH while\n  ## the native finder performs the search directly in a manor dependent on the\n  ## platform.  Default is 'pgrep'\n  # pid_finder = \"pgrep\"\n\n```\n\n## Metric Definitions\n\n| Metric                      | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n| --------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- |\n| cpu_time_guest_nice         | The amount of time that the CPU is running a virtual CPU for a guest                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | operating system, which is low-priority and can be interrupted by other processes. This metric is measured in hundredths of a second. |\n| cpu_time_guest              | The amount of time that the CPU is running a virtual CPU for a guest operating system.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n| cpu_time_idle               | The amount of time that the CPU is idle. This metric is measured in hundredths of a second.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n| cpu_time_iowait             | The amount of time that the CPU is waiting for I/O operations to complete. This metric is measured in hundredths of a second.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n| cpu_time_irq                | The amount of time that the CPU is servicing interrupts. This metric is measured in hundredths of a second.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n| cpu_time_nice               | The amount of time that the CPU is in user mode with low-priority processes, which can easily be interrupted by higher-priority processes. This metric is measured in hundredths of a second.                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n| cpu_time_soft_irq           | The amount of time that the CPU is servicing software interrupts. This metric is measured in hundredths of a second.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| cpu_time_steal              | The amount of time that the CPU is in stolen time, which is time spent in other operating systems in a virtualized environment. This metric is measured in hundredths of a second.                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                                                       |\n| cpu_time_system             | The amount of time that the CPU is in system mode. This metric is measured in hundredths of a second.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n| cpu_time_user               | The amount of time that the CPU is in user mode. This metric is measured in hundredths of a second.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n| cpu_usage                   | The percentage of time that the CPU is active in any capacity for this process.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n| created_at                  | Process creation timestamp.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n| memory_locked               | One segment of memory that is locked into the system's physical memory by a specific process, it is locked until the specified app sends an unlock request, or is flushed from memory.                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n| serviceprocess_memory_rss   | A measurement that shows how much RAM has been allocated to a process during its execution.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n| serviceprocess_memory_stack | The stack is used for local variables. Space on the stack is reserved for local variables when they are declared ( at function entrance or elsewhere, depending on the language ), and the space is freed up when the variables go out of scope. Note: that the stack is also used for function return values, and the exact mechanisms of stack management may be language specific.Note: that the stack and the heap start at opposite ends of the process's free space and grow towards each other. If they should ever meet, then either a stack overflow error will occur, or else a call to new or malloc will fail due to insufficient memory available. |\n| serviceprocess_memory_swap  | The amount of swap space currently in use. Note: If you run out of physical memory, you use virtual memory, which stores the data in memory on disk, This is “Swap” Memory.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n| serviceprocess_memory_usage | The amount of memory currently in use by this process.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n| serviceprocess_memory_vms   | Virtual Memory Size allocated to the process.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n| serviceprocess_num_threads  | Number of threads used by the process                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n| pid                         | Process identifier                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n","logo":{"light":"/img/library/procstat.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:ProcStat"},{"name":"prometheus-legacy","title":"Prometheus","content":"# Prometheus\n\n## Overview\n\nThe prometheus module provides a simple way to push data into Circonus via the prometheus write support.\n\n## Configuration\n\nCreate the prometheus check, and then configure your prometheus with the provided URL in its /write endpoint.\n\nSee: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_write\n","logo":{"light":"/img/library/prometheus.svg"},"attributes":{"implementation":"broker","legacy":true},"module":"prometheus"},{"name":"prometheus","title":"Prometheus","content":"# Prometheus\n\n## Overview\n\nThe prometheus input plugin gathers metrics from HTTP servers exposing metrics\nin Prometheus format.\n\n## Configuration\n\n```toml\n# Read metrics from one or many prometheus clients\n[[inputs.prometheus]]\n  ## An array of urls to scrape metrics from.\n  urls = [\"http://localhost:9100/metrics\"]\n\n  ## Metric version controls the mapping from Prometheus metrics into\n  ## circonus metrics.  When using the prometheus_client output, use the same\n  ## value in both plugins to ensure metrics are round-tripped without\n  ## modification.\n  ##\n  ##   example: metric_version = 1; deprecated in 1.13\n  ##            metric_version = 2; recommended version\n  # metric_version = 1\n\n  ## An array of Kubernetes services to scrape metrics from.\n  # kubernetes_services = [\"http://my-service-dns.my-namespace:9100/metrics\"]\n\n  ## Kubernetes config file to create client from.\n  # kube_config = \"/path/to/kubernetes.config\"\n\n  ## Scrape Kubernetes pods for the following prometheus annotations:\n  ## - prometheus.io/scrape: Enable scraping for this pod\n  ## - prometheus.io/scheme: If the metrics endpoint is secured then you will need to\n  ##     set this to `https` & most likely set the tls config.\n  ## - prometheus.io/path: If the metrics path is not /metrics, define it with this annotation.\n  ## - prometheus.io/port: If port is not 9102 use this annotation\n  # monitor_kubernetes_pods = true\n  ## Restricts Kubernetes monitoring to a single namespace\n  ##   ex: monitor_kubernetes_pods_namespace = \"default\"\n  # monitor_kubernetes_pods_namespace = \"\"\n  # label selector to target pods which have the label\n  # kubernetes_label_selector = \"env=dev,app=nginx\"\n  # field selector to target pods\n  # eg. To scrape pods on a specific node\n  # kubernetes_field_selector = \"spec.nodeName=$HOSTNAME\"\n\n  ## Use bearer token for authorization. ('bearer_token' takes priority)\n  # bearer_token = \"/path/to/bearer/token\"\n  ## OR\n  # bearer_token_string = \"abc_123\"\n\n  ## HTTP Basic Authentication username and password. ('bearer_token' and\n  ## 'bearer_token_string' take priority)\n  # username = \"\"\n  # password = \"\"\n\n  ## Specify timeout duration for slower prometheus clients (default is 3s)\n  # response_timeout = \"3s\"\n\n  ## Optional TLS Config\n  # tls_ca = /path/to/cafile\n  # tls_cert = /path/to/certfile\n  # tls_key = /path/to/keyfile\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n\n`urls` can contain a unix socket as well. If a different path is required (default is `/metrics` for both http[s] and unix) for a unix socket, add `path` as a query parameter as follows: `unix:///var/run/prometheus.sock?path=/custom/metrics`\n\n### Kubernetes Service Discovery\n\nURLs listed in the `kubernetes_services` parameter will be expanded\nby looking up all A records assigned to the hostname as described in\n[Kubernetes DNS service discovery](https://kubernetes.io/docs/concepts/services-networking/service/#dns).\n\nThis method can be used to locate all\n[Kubernetes headless services](https://kubernetes.io/docs/concepts/services-networking/service/#headless-services).\n\n### Kubernetes scraping\n\nEnabling this option will allow the plugin to scrape for prometheus annotation on Kubernetes\npods. Currently, you can run this plugin in your kubernetes cluster, or we use the kubeconfig\nfile to determine where to monitor.\nCurrently the following annotation are supported:\n\n- `prometheus.io/scrape` Enable scraping for this pod.\n- `prometheus.io/scheme` If the metrics endpoint is secured then you will need to set this to `https` & most likely set the tls config. (default 'http')\n- `prometheus.io/path` Override the path for the metrics endpoint on the service. (default '/metrics')\n- `prometheus.io/port` Used to override the port. (default 9102)\n\nUsing the `monitor_kubernetes_pods_namespace` option allows you to limit which pods you are scraping.\n\n### Bearer Token\n\nIf set, the file specified by the `bearer_token` parameter will be read on\neach interval and its contents will be appended to the Bearer string in the\nAuthorization header.\n\n### Usage for Caddy HTTP server\n\nIf you want to monitor Caddy, you need to use Caddy with its Prometheus plugin:\n\n- Download Caddy+Prometheus plugin [here](https://caddyserver.com/download)\n- Add the `prometheus` directive in your `CaddyFile`\n- Restart Caddy\n- Configure agent to fetch metrics on it:\n\n```toml\n[[inputs.prometheus]]\n#   ## An array of urls to scrape metrics from.\n  urls = [\"http://localhost:9180/metrics\"]\n```\n\n> This is the default URL where Caddy Prometheus plugin will send data.\n> For more details, please read the [Caddy Prometheus documentation](https://github.com/miekg/caddy-prometheus/blob/master/README.md).\n","logo":{"light":"/img/library/prometheus.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:prometheus"},{"name":"proxmox","title":"Proxmox","content":"# Proxmox\n\n## Overview\n\nThe proxmox plugin gathers metrics about containers and VMs using the Proxmox API.\n\n## Configuration\n\n```toml\n[[inputs.proxmox]]\n  ## API connection configuration. The API token was introduced in Proxmox v6.2. Required permissions for user and token: PVEAuditor role on /.\n  base_url = \"https://localhost:8006/api2/json\"\n  api_token = \"USER@REALM!TOKENID=UUID\"\n  ## Node name, default to OS hostname\n  # node_name = \"\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/circonus-unified-agent/ca.pem\"\n  # tls_cert = \"/etc/circonus-unified-agent/cert.pem\"\n  # tls_key = \"/etc/circonus-unified-agent/key.pem\"\n  ## Use TLS but skip chain & host verification\n  insecure_skip_verify = false\n\n  # HTTP response timeout (default: 5s)\n  response_timeout = \"5s\"\n```\n\n### Permissions\n\nThe plugin will need to have access to the Proxmox API. An API token\nmust be provided with the corresponding user being assigned at least the PVEAuditor\nrole on /.\n","logo":{"light":"/img/library/proxmox.svg","dark":"/img/library/proxmox-dark.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:proxmox"},{"name":"puppet-agent","title":"Puppet Agent","content":"# Puppet Agent\n\n## Overview\n\nThe puppetagent plugin collects variables outputted from the 'last_run_summary.yaml' file usually located in `/var/lib/puppet/state/`\n[PuppetAgent Runs](https://puppet.com/blog/puppet-monitoring-how-to-monitor-success-or-failure-of-puppet-runs/).\n\n## Configuration\n\n```\ncat /var/lib/puppet/state/last_run_summary.yaml\n\n---\n  events:\n    failure: 0\n    total: 0\n    success: 0\n  resources:\n    failed: 0\n    scheduled: 0\n    changed: 0\n    skipped: 0\n    total: 109\n    failed_to_restart: 0\n    restarted: 0\n    out_of_sync: 0\n  changes:\n    total: 0\n  time:\n    user: 0.004331\n    schedule: 0.001123\n    filebucket: 0.000353\n    file: 0.441472\n    exec: 0.508123\n    anchor: 0.000555\n    yumrepo: 0.006989\n    ssh_authorized_key: 0.000764\n    service: 1.807795\n    package: 1.325788\n    total: 8.85354707064819\n    config_retrieval: 4.75567007064819\n    last_run: 1444936531\n    cron: 0.000584\n  version:\n    config: 1444936521\n    puppet: \"3.7.5\"\n```\n\n```\njcross@pit-devops-02 ~ >sudo ./circonus-unified-agent_linux_amd64 --input-filter puppetagent --config tele.conf --test\n* Plugin: puppetagent, Collection 1\n> [] puppetagent_events_failure value=0\n> [] puppetagent_events_total value=0\n> [] puppetagent_events_success value=0\n> [] puppetagent_resources_failed value=0\n> [] puppetagent_resources_scheduled value=0\n> [] puppetagent_resources_changed value=0\n> [] puppetagent_resources_skipped value=0\n> [] puppetagent_resources_total value=109\n> [] puppetagent_resources_failedtorestart value=0\n> [] puppetagent_resources_restarted value=0\n> [] puppetagent_resources_outofsync value=0\n> [] puppetagent_changes_total value=0\n> [] puppetagent_time_user value=0.00393\n> [] puppetagent_time_schedule value=0.001234\n> [] puppetagent_time_filebucket value=0.000244\n> [] puppetagent_time_file value=0.587734\n> [] puppetagent_time_exec value=0.389584\n> [] puppetagent_time_anchor value=0.000399\n> [] puppetagent_time_sshauthorizedkey value=0.000655\n> [] puppetagent_time_service value=0\n> [] puppetagent_time_package value=1.297537\n> [] puppetagent_time_total value=9.45297606225586\n> [] puppetagent_time_configretrieval value=5.89822006225586\n> [] puppetagent_time_lastrun value=1444940131\n> [] puppetagent_time_cron value=0.000646\n> [] puppetagent_version_config value=1444940121\n> [] puppetagent_version_puppet value=3.7.5\n```\n","logo":{"light":"/img/library/puppet.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:puppetagent"},{"name":"rabbitmq","title":"RabbitMQ","content":"# RabbitMQ\n\n## Overview\n\nReads metrics from RabbitMQ servers via the [Management Plugin](https://www.rabbitmq.com/management.html).\n\nFor additional details reference the [RabbitMQ Management HTTP Stats](https://raw.githack.com/rabbitmq/rabbitmq-management/rabbitmq_v3_6_9/priv/www/api/index.html).\n\n## Configuration\n\n```toml\n[[inputs.rabbitmq]]\n  ## Management Plugin url. (default: http://localhost:15672)\n  # url = \"http://localhost:15672\"\n  ## Credentials\n  # username = \"guest\"\n  # password = \"guest\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/circonus-unified-agent/ca.pem\"\n  # tls_cert = \"/etc/circonus-unified-agent/cert.pem\"\n  # tls_key = \"/etc/circonus-unified-agent/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n\n  ## Optional request timeouts\n  ##\n  ## ResponseHeaderTimeout, if non-zero, specifies the amount of time to wait\n  ## for a server's response headers after fully writing the request.\n  # header_timeout = \"3s\"\n  ##\n  ## client_timeout specifies a time limit for requests made by this client.\n  ## Includes connection time, any redirects, and reading the response body.\n  # client_timeout = \"4s\"\n\n  ## A list of nodes to gather as the rabbitmq_node measurement. If not\n  ## specified, metrics for all nodes are gathered.\n  # nodes = [\"rabbit@node1\", \"rabbit@node2\"]\n\n  ## A list of queues to gather as the rabbitmq_queue measurement. If not\n  ## specified, metrics for all queues are gathered.\n  # queues = [\"cua\"]\n\n  ## A list of exchanges to gather as the rabbitmq_exchange measurement. If not\n  ## specified, metrics for all exchanges are gathered.\n  # exchanges = [\"cua\"]\n\n  ## Queues to include and exclude. Globs accepted.\n  ## Note that an empty array for both will include all queues\n  # queue_name_include = []\n  # queue_name_exclude = []\n\n  ## Federation upstreams to include and exclude specified as an array of glob\n  ## pattern strings.  Federation links can also be limited by the queue and\n  ## exchange filters.\n  # federation_upstream_include = []\n  # federation_upstream_exclude = []\n```\n","logo":{"light":"/img/library/rabbitmq.svg"},"attributes":{"implementation":"cua"},"tags":["messaging","queue"],"module":"httptrap:cua:rabbitmq"},{"name":"redfish","title":"Redfish","content":"# Redfish\n\n## Overview\n\nThe `redfish` plugin gathers metrics and status information about CPU temperature, fanspeed, Powersupply, voltage, hostname and Location details (datacenter, placement, rack and room) of hardware servers for which [DMTF's Redfish](https://redfish.dmtf.org/) is enabled.\n\n## Configuration\n\n```toml\n[[inputs.redfish]]\n  ## Redfish API Base URL.\n  address = \"https://127.0.0.1:5000\"\n\n  ## Credentials for the Redfish API.\n  username = \"root\"\n  password = \"password123456\"\n\n  ## System Id to collect data for in Redfish APIs.\n  computer_system_id=\"System.Embedded.1\"\n\n  ## Amount of time allowed to complete the HTTP request\n  # timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/circonus-unified-agent/ca.pem\"\n  # tls_cert = \"/etc/circonus-unified-agent/cert.pem\"\n  # tls_key = \"/etc/circonus-unified-agent/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n","logo":{"light":"/img/library/redfish.png"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:redfish"},{"name":"redis-legacy","title":"Redis","content":"# Redis\n\n## Overview\n\nRedis is an open source, BSD licensed, advanced key-value cache and store. It is often referred to as a data structure server since keys can contain strings, hashes, lists, sets, sorted sets, bitmaps and hyperloglogs.\n\nCirconus collects information from Redis in JSON format via the Redis NoSQL database's native interface.\n\nIn a multi-process or clustered environment, a tool such as Redis, with its atomic increments, can be used for global aggregation. Once aggregated, it can be checked and monitored by Circonus, with rates being derived from count changes over time.\n\n## Configuration\n\nRequired parameters:\n|Name|Description|\n|----|-----------|\n|port|The TCP port to which the check will connect (default: 6379).|\n|command|The command to send to the redis server (default: INFO).|\n\nOptional parameters:\n|Name|Description|\n|----|-----------|\n|password|The password for authenticating to the redis server (default: \"no password\").|\n|dbindex|Index of the database the command will run against (default: 0).|\n","logo":{"light":"/img/library/redis.svg"},"attributes":{"implementation":"broker","legacy":true},"tags":["nosql","database"],"module":"redis"},{"name":"redis","title":"Redis","content":"# Redis\n\n## Configuration\n\n```toml\n# Read Redis's basic status information\n[[inputs.redis]]\n  ## specify servers via a url matching:\n  ##  [protocol://][:password]@address[:port]\n  ##  e.g.\n  ##    tcp://localhost:6379\n  ##    tcp://:password@192.168.99.100\n  ##\n  ## If no servers are specified, then localhost is used as the host.\n  ## If no port is specified, 6379 is used\n  servers = [\"tcp://localhost:6379\"]\n  ## Optional. Specify redis commands to retrieve values\n  # [[inputs.redis.commands]]\n  # command = [\"get\", \"sample-key\"]\n  # field = \"sample-key-value\"\n  # type = \"string\"\n\n  ## specify server password\n  # password = \"s#cr@t%\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/circonus-unified-agent/ca.pem\"\n  # tls_cert = \"/etc/circonus-unified-agent/cert.pem\"\n  # tls_key = \"/etc/circonus-unified-agent/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = true\n```\n","logo":{"light":"/img/library/redis.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:redis"},{"name":"rethinkdb","title":"RethinkDB","content":"# RethinkDB\n\n## Overview\n\nCollect metrics from [RethinkDB](https://www.rethinkdb.com/).\n\n## Configuration\n\nThis section contains the default TOML to configure the plugin. You can\ngenerate it using `circonus-unified-agent --usage rethinkdb`.\n\n```toml\n[[inputs.rethinkdb]]\n  ## An array of URI to gather stats about. Specify an ip or hostname\n  ## with optional port add password. ie,\n  ##   rethinkdb://user:auth_key@10.10.3.30:28105,\n  ##   rethinkdb://10.10.3.33:18832,\n  ##   10.0.0.1:10000, etc.\n  servers = [\"127.0.0.1:28015\"]\n\n  ## If you use actual rethinkdb of > 2.3.0 with username/password authorization,\n  ## protocol have to be named \"rethinkdb2\" - it will use 1_0 H.\n  # servers = [\"rethinkdb2://username:password@127.0.0.1:28015\"]\n\n  ## If you use older versions of rethinkdb (<2.2) with auth_key, protocol\n  ## have to be named \"rethinkdb\".\n  # servers = [\"rethinkdb://username:auth_key@127.0.0.1:28015\"]\n```\n","logo":{"light":"/img/library/rethinkdb.svg","dark":"/img/library/rethinkdb-dark.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:rethinkdb"},{"name":"riak-legacy","title":"Riak","content":"# Riak\n\n## Overview\n\nRiak is an open source, distributed NoSQL database. Circonus collects information from riak in JSON format via the Riak database's native interface.\n\nThis check type pulls statistics from the Riak stats interface.\n\n## Configuration\n\nThe only required parameter is the URL to check, including scheme and hostname (as you would type into a browser's location bar.)\n\nOptional parameters:\n|Name|Description|\n|----|-----------|\n|port|The TCP port to which the check will connect (default: 8098).|\n|method|The HTTP method to use (default: GET).|\n|http_version|The HTTP protocol version to use (default: 1.1).|\n|header name/value|Include an arbitrary header in the HTTP request.|\n|auth_method|HTTP authentication method to use (default: None).|\n|auth_user|The user name to use during authentication.|\n|auth_password|The password to use during authentication.|\n","logo":{"light":"/img/library/riak.svg"},"attributes":{"implementation":"broker","legacy":true},"tags":["nosql","database"],"module":"json:riak"},{"name":"riak","title":"Riak","content":"# Riak\n\n## Overview\n\nThe Riak plugin gathers metrics from one or more riak instances.\n\n## Configuration\n\n```toml\n# Description\n[[inputs.riak]]\n  # Specify a list of one or more riak http servers\n  servers = [\"http://localhost:8098\"]\n```\n","logo":{"light":"/img/library/riak.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:riak"},{"name":"riemann","title":"Riemann Listener","content":"# Riemann Listener\n\n## Overview\n\nThe Riemann Listener is a simple input plugin that listens for messages from\nclient that use riemann clients using riemann-protobuff format.\n\n## Configuration\n\nThis is a sample configuration for the plugin.\n\n```toml\n[[inputs.rimann_listener]]\n  ## URL to listen on\n  ## Default is \"tcp://:5555\"\n  #  service_address = \"tcp://:8094\"\n  #  service_address = \"tcp://127.0.0.1:http\"\n  #  service_address = \"tcp4://:8094\"\n  #  service_address = \"tcp6://:8094\"\n  #  service_address = \"tcp6://[2001:db8::1]:8094\"\n  ## Maximum number of concurrent connections.\n  ## 0 (default) is unlimited.\n  #  max_connections = 1024\n  ## Read timeout.\n  ## 0 (default) is unlimited.\n  #  read_timeout = \"30s\"\n  ## Optional TLS configuration.\n  #  tls_cert = \"/opt/circonus/unified-agent/etc/cert.pem\"\n  #  tls_key  = \"/opt/circonus/unified-agent/etc/key.pem\"\n  ## Enables client authentication if set.\n  #  tls_allowed_cacerts = [\"/opt/circonus/unified-agent/etc/clientca.pem\"]\n  ## Maximum socket buffer size (in bytes when no unit specified).\n  #  read_buffer_size = \"64KiB\"\n  ## Period between keep alive probes.\n  ## 0 disables keep alive probes.\n  ## Defaults to the OS configuration.\n  #  keep_alive_period = \"5m\"\n```\n\nJust like Riemann the default port is 5555. This can be configured, refer configuration above.\n\nRiemann `Service` is mapped as `measurement`. `metric` and `TTL` are converted into field values.\nAs Riemann tags as simply an array, they are converted into the internal line format key-value, where both key and value are the tags.\n","logo":{"light":"/img/library/riemann.png","dark":"/img/library/riemann-dark.png"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:riemann_listener"},{"name":"salesforce","title":"Salesforce","content":"# Salesforce\n\n## Overview\n\nThe Salesforce plugin gathers metrics about the limits in your Salesforce organization and the remaining usage.\nIt fetches its data from the [limits endpoint](https://developer.salesforce.com/docs/atlas.en-us.api_rest.meta/api_rest/resources_limits.htm) of Salesforce's REST API.\n\n## Configuration\n\n```toml\n# Gather Metrics about Salesforce limits and remaining usage\n[[inputs.salesforce]]\n  username = \"your_username\"\n  password = \"your_password\"\n  ## (Optional) security token\n  security_token = \"your_security_token\"\n  ## (Optional) environment type (sandbox or production)\n  ## default is: production\n  # environment = \"production\"\n  ## (Optional) API version (default: \"39.0\")\n  # version = \"39.0\"\n```\n","logo":{"light":"/img/library/salesforce.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:salesforce"},{"name":"sflow","title":"SFlow","content":"# SFlow\n\n## Overview\n\nThe SFlow Input Plugin provides support for acting as an SFlow V5 collector in\naccordance with the specification from [sflow.org](https://sflow.org/).\n\nCurrently only Flow Samples of Ethernet / IPv4 & IPv4 TCP & UDP headers are\nturned into metrics. Counters and other header samples are ignored.\n\n## Configuration\n\n```toml\n[[inputs.sflow]]\n  ## Address to listen for sFlow packets.\n  ##   example: service_address = \"udp://:6343\"\n  ##            service_address = \"udp4://:6343\"\n  ##            service_address = \"udp6://:6343\"\n  service_address = \"udp://:6343\"\n\n  ## Set the size of the operating system's receive buffer.\n  ##   example: read_buffer_size = \"64KiB\"\n  # read_buffer_size = \"\"\n```\n\n## Troubleshooting\n\nThe [sflowtool](https://github.com/sflow/sflowtool) utility can be used to print sFlow packets, and compared\nagainst the metrics produced by agent.\n\n```\nsflowtool -p 6343\n```\n\nIf opening an issue, in addition to the output of sflowtool it will also be\nhelpful to collect a packet capture. Adjust the interface, host and port as\nneeded:\n\n```\n$ sudo tcpdump -s 0 -i eth0 -w circonus-unified-agent-sflow.pcap host 127.0.0.1 and port 6343\n```\n","logo":{"light":"/img/library/sflow.png","dark":"/img/library/sflow-dark.png"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:sflow"},{"name":"smart","title":"S.M.A.R.T.","content":"# S.M.A.R.T.\n\n## Overview\n\nGet metrics using the command line utility `smartctl` for S.M.A.R.T. (Self-Monitoring, Analysis and Reporting Technology) storage devices. SMART is a monitoring system included in computer hard disk drives (HDDs) and solid-state drives (SSDs) that detects and reports on various indicators of drive reliability, with the intent of enabling the anticipation of hardware failures. [See smartmontools](https://www.smartmontools.org/).\n\nSMART information is separated between different measurements: `smart_device` is used for general information, while `smart_attribute` stores the detailed attribute information if `attributes = true` is enabled in the plugin configuration.\n\nIf no devices are specified, the plugin will scan for SMART devices via the following command:\n\n```\nsmartctl --scan\n```\n\nMetrics will be reported from the following `smartctl` command:\n\n```\nsmartctl --info --attributes --health -n <nocheck> --format=brief <device>\n```\n\nThis plugin supports _smartmontools_ version 5.41 and above, but v. 5.41 and v. 5.42\nmight require setting `nocheck`, see the comment in the sample configuration.\nAlso, NVMe capabilities were introduced in version 6.5.\n\nTo enable SMART on a storage device run:\n\n```\nsmartctl -s on <device>\n```\n\n## NVMe vendor specific attributes\n\nFor NVMe disk type, plugin can use command line utility `nvme-cli`. It has a feature\nto easy access a vendor specific attributes.\nThis plugin supports nmve-cli version 1.5 and above: https://github.com/linux-nvme/nvme-cli.\nIn case of `nvme-cli` absence NVMe vendor specific metrics will not be obtained.\n\nVendor specific SMART metrics for NVMe disks may be reported from the following `nvme` command:\n\n```\nnvme <vendor> smart-log-add <device>\n```\n\nNote that vendor plugins for `nvme-cli` could require different naming convention and report format.\n\nTo see installed plugin extensions, depended on the nvme-cli version, look at the bottom of:\n\n```\nnvme help\n```\n\nTo gather disk vendor id (vid) `id-ctrl` could be used:\n\n```\nnvme id-ctrl <device>\n```\n\nAssociation between a vid and company can be found there: https://pcisig.com/membership/member-companies.\n\nDevices affiliation to being NVMe or non NVMe will be determined thanks to:\n\n```\nsmartctl --scan\n```\n\nand:\n\n```\nsmartctl --scan -d nvme\n```\n\n## Configuration\n\n```toml\n# Read metrics from storage devices supporting S.M.A.R.T.\n[[inputs.smart]]\n    ## Optionally specify the path to the smartctl executable\n    # path_smartctl = \"/usr/bin/smartctl\"\n\n    ## Optionally specify the path to the nvme-cli executable\n    # path_nvme = \"/usr/bin/nvme\"\n\n    ## Optionally specify if vendor specific attributes should be propagated for NVMe disk case\n    ## [\"auto-on\"] - automatically find and enable additional vendor specific disk info\n    ## [\"vendor1\", \"vendor2\", ...] - e.g. \"Intel\" enable additional Intel specific disk info\n    # enable_extensions = [\"auto-on\"]\n\n    ## On most platforms used cli utilities requires root access.\n    ## Setting 'use_sudo' to true will make use of sudo to run smartctl or nvme-cli.\n    ## Sudo must be configured to allow the agent user to run smartctl or nvme-cli\n    ## without a password.\n    # use_sudo = false\n\n    ## Skip checking disks in this power mode. Defaults to\n    ## \"standby\" to not wake up disks that have stopped rotating.\n    ## See --nocheck in the man pages for smartctl.\n    ## smartctl version 5.41 and 5.42 have faulty detection of\n    ## power mode and might require changing this value to\n    ## \"never\" depending on your disks.\n    # nocheck = \"standby\"\n\n    ## Gather all returned S.M.A.R.T. attribute metrics and the detailed\n    ## information from each drive into the 'smart_attribute' measurement.\n    # attributes = false\n\n    ## Optionally specify devices to exclude from reporting if disks auto-discovery is performed.\n    # excludes = [ \"/dev/pass6\" ]\n\n    ## Optionally specify devices and device type, if unset\n    ## a scan (smartctl --scan and smartctl --scan -d nvme) for S.M.A.R.T. devices will be done\n    ## and all found will be included except for the excluded in excludes.\n    # devices = [ \"/dev/ada0 -d atacam\", \"/dev/nvme0\"]\n\n    ## Timeout for the cli command to complete.\n    # timeout = \"30s\"\n```\n\n### Permissions\n\nIt's important to note that this plugin references smartctl and nvme-cli, which may require additional permissions to execute successfully.\nDepending on the user/group permissions of the user executing this plugin, you may need to use sudo.\n\nYou will need the following in your config:\n\n```toml\n[[inputs.smart]]\n  use_sudo = true\n```\n\nYou will also need to update your sudoers file:\n\n```bash\n$ visudo\n# For smartctl add the following lines:\nCmnd_Alias SMARTCTL = /usr/bin/smartctl\ncua  ALL=(ALL) NOPASSWD: SMARTCTL\nDefaults!SMARTCTL !logfile, !syslog, !pam_session\n\n# For nvme-cli add the following lines:\nCmnd_Alias NVME = /path/to/nvme\ncua  ALL=(ALL) NOPASSWD: NVME\nDefaults!NVME !logfile, !syslog, !pam_session\n```\n\nTo run smartctl or nvme with `sudo` wrapper script can be created. `path_smartctl` or\n`path_nvme` in the configuration should be set to execute this script.\n","logo":{"light":"/img/library/smart.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:smart"},{"name":"smtp","title":"SMTP","content":"# SMTP\n\n## Overview\n\nCheck mail deliverability with the Simple Mail Transport Protocol. The check will deliver a test message to the configured recipient and create metrics based on the various commands and responses during the interaction with the target mail server.\n\n## Notes\n\nRequired parameters:\n|Name|Description|\n|----|-----------|\n|to|The email address of a recipient to whom the test message will be sent.|\n\nOptional parameters:\n|Name|Description|\n|----|-----------|\n|port|The TCP port to which the check will connect (default: 25).|\n|ehlo|The EHLO identification string to send (default: \"noit.local\").|\n|from|The SMTP envelope sender.|\n|starttls|Whether the check should attempt a STARTTLS upgrade of the connection (default: false).|\n|sasl_authentication|Specify the type of SASL (Simple Authentication and Security Layer) authentication to use. Values are off, login, or plain (default: off).|\n|sasl_user|The SASL authentication username.|\n|sasl_password|The SASL authentication password.|\n|sasl_auth_id|The SASL authorization identity, only used for the plain type of authentication.|\n|payload|Specifies the payload sent (on the wire). CR LF DOT CR LF is appended automatically (default: \"Subject: Testing\").|\n|proxy_protocol|If set to true, test mail server response to a PROXY protocol header (default: false).|\n|proxy_family|The protocol family to send in the PROXY header, either TCP4 or TCP6 (default: TCP4).|\n|proxy_source_address|The IP (or string) to use as the source address portion of the PROXY protocol. More on the PROXY protocol in the HAproxy documentation.|\n|proxy_dest_address|The IP (or string) to use as the destination address portion of the PROXY protocol. More on the PROXY protocol in the HAproxy documentation.|\n|proxy_source_port|The port to use as the source port portion of the PROXY protocol. Defaults to the actual source port of the connection to the target.|\n|proxy_dest_port|The port to use as the dest port portion of the PROXY protocol. Defaults to the value of the port parameter or 25.|\n\n## Metrics\n\nThe SMTP check proceeds in phases, from the initial connection through to the final QUIT command. The elapsed time from the start to the end of each phase is emitted as a numeric metric, in milliseconds (ms), named [phasename]\\_time. An SMTP connection goes through the following phases:\n|Phase|Description|\n|-----|-----------|\n|banner|From initial TCP connect to receiving server greeting|\n|ehlo|From sending EHLO to server response|\n|starttls|From sending STARTTLS to server response|\n|sasl_auth|From sending AUTH, username, and password to server response|\n|mailfrom|From sending MAIL FROM to server response|\n|rcptto|From sending RCPT TO to server response|\n|data|From sending DATA to server response|\n|body|From sending payload to server response|\n|quit|From sending QUIT to server response|\n\nTypical metrics also include:\n|Name|Type|Description|\n|----|----|-----------|\ncert_end|numeric|The Unix epoch time representing the expiration date of the TLS certificate.\ncert_end_in|numeric|The number of seconds between now (as measured at the Circonus broker) and the cert_end value.\ncert_error|text|Text of any certificate validation error(s), or null if no errors.\ncert_issuer|text|The subject of the issuer's certificate, typically a Certificate Authority (CA) certificate.\ncert_start|numeric|The Unix epoch time representing the validity start date of the TLS certificate.\ncert_subject|text|The subject of the server's TLS certificate.\ncert_subject_alternative_names|text|A list of any X509v3 Subject Alternative Names (SAN) that the TLS certificate protects.\nduration|numeric|The elapsed time from start to end of the entire check operation, in milliseconds.\n","logo":{"light":"/img/library/smtp.svg"},"attributes":{"implementation":"broker"},"tags":["email","protocol","messaging"],"module":"smtp"},{"name":"snmp-legacy","title":"SNMP","content":"# SNMP\n\n## Overview\n\nCheck one or more Object Identifiers (OIDs) with the Simple Network Management Protocol (SNMP).\n\n## Configuration\n\nThe Circonus UI will pre-fill the community and version parameters, matching the API defaults below. At least one `oid_[name]` parameter is required in the Circonus UI.\n\nOptional parameters:\n|Name|Description|\n|----|-----------|\n|community|The SNMP community string providing read access (default: \"public\").|\n|port|The UDP port to which SNMP queries will be sent (default: 161).|\n|version|The SNMP version used for queries (default: 2c).|\n|oid\\_[name]|Defines a metric to query. Key _oid_foo_ will establish a metric called foo. The value of the parameter should be an OID either in decimal notation or MIB name.|\n|security*level|The security level to use for the SNMP session. Choices are: \\_authPriv* (authenticated and encrypted), _authNoPriv_ (authenticated and unencrypted) and _noAuthNoPriv_ (unauthenticated and unencrypted). Only applicable to SNMP Version 3 (default: authPriv).|\n|auth*protocol|The authentication protocol to use. Choices are \\_MD5* or SHA. Only applicable to SNMP Version 3 (default: MD5).|\n|privacy*protocol|The privacy protocol to use. Choices are \\_DES* or _AES128_. Only applicable to SNMP Version 3 (default: DES).|\n|auth_passphrase|The authentication passphrase to use. Only applicable to SNMP Version 3.|\n|privacy_passphrase|The privacy passphrase to use. Only applicable to SNMP Version 3.|\n|security_engine|The security engine hex value to use. Only applicable to SNMP Version 3.|\n|context_engine|The context engine hex value to use. Only applicable to SNMP Version 3.|\n|security_name|The security name (or user name) to use. Only applicable to SNMP Version 3.|\n|context_name|The context name to use. Only applicable to SNMP Version 3.|\n|separate_queries|Whether or not to query each OID separately (default: false/off).|\n","logo":{"light":"/img/library/snmp.svg"},"attributes":{"implementation":"broker","legacy":true},"module":"snmp"},{"name":"snmp-trap","title":"SNMP Trap","content":"# SNMP Trap\n\n## Overview\n\nThe SNMP Trap plugin is a service input plugin that receives SNMP\nnotifications (traps and inform requests).\n\nNotifications are received on plain UDP. The port to listen is\nconfigurable.\n\n### Prerequisites\n\nThis plugin uses the `snmptranslate` programs from the\n[net-snmp](http://www.net-snmp.org/) project. These tools will need to be installed into the `PATH` in\norder to be located. Other utilities from the net-snmp project may be useful\nfor troubleshooting, but are not directly used by the plugin.\n\nThese programs will load available MIBs on the system. Typically the default\ndirectory for MIBs is `/usr/share/snmp/mibs`, but if your MIBs are in a\ndifferent location you may need to make the paths known to net-snmp. The\nlocation of these files can be configured in the `snmp.conf` or via the\n`MIBDIRS` environment variable. See [`man 1 snmpcmd`](http://net-snmp.sourceforge.net/docs/man/snmpcmd.html#lbAK) for more\ninformation.\n\n## Configuration\n\n```toml\n[[inputs.snmp_trap]]\n  ## Transport, local address, and port to listen on.  Transport must\n  ## be \"udp://\".  Omit local address to listen on all interfaces.\n  ##   example: \"udp://127.0.0.1:1234\"\n  ##\n  ## Special permissions may be required to listen on a port less than\n  ## 1024.  See README.md for details\n  ##\n  # service_address = \"udp://:162\"\n  ## Timeout running snmptranslate command\n  # timeout = \"5s\"\n  ## Snmp version\n  # version = \"2c\"\n  ## SNMPv3 authentication and encryption options.\n  ##\n  ## Security Name.\n  # sec_name = \"myuser\"\n  ## Authentication protocol; one of \"MD5\", \"SHA\" or \"\".\n  # auth_protocol = \"MD5\"\n  ## Authentication password.\n  # auth_password = \"pass\"\n  ## Security Level; one of \"noAuthNoPriv\", \"authNoPriv\", or \"authPriv\".\n  # sec_level = \"authNoPriv\"\n  ## Privacy protocol used for encrypted messages; one of \"DES\", \"AES\", \"AES192\", \"AES192C\", \"AES256\", \"AES256C\" or \"\".\n  # priv_protocol = \"\"\n  ## Privacy password used for encrypted messages.\n  # priv_password = \"\"\n```\n\n#### Using a Privileged Port\n\nOn many operating systems, listening on a privileged port (a port\nnumber less than 1024) requires extra permission. Since the default\nSNMP trap port 162 is in this category, using agent to receive SNMP\ntraps may need extra permission.\n\nInstructions for listening on a privileged port vary by operating\nsystem. It is not recommended to run agent as superuser in order to\nuse a privileged port. Instead follow the principle of least privilege\nand use a more specific operating system mechanism to allow agent to\nuse the port. You may also be able to have agent use an\nunprivileged port and then configure a firewall port forward rule from\nthe privileged port.\n\nTo use a privileged port on Linux, you can use setcap to enable the\nCAP_NET_BIND_SERVICE capability on the agent binary:\n\n```\nsetcap cap_net_bind_service=+ep /opt/circonus/unified-agent/sbin/circonus-unified-agentd\n```\n\nOn Mac OS, listening on privileged ports is unrestricted on versions\n10.14 and later.\n\n```\n  ## SNMPv3 authentication and encryption options.\n  ##\n  ## Security Name.\n  # sec_name = \"myuser\"\n  ## Authentication protocol; one of \"MD5\", \"SHA\", or \"\".\n  # auth_protocol = \"MD5\"\n  ## Authentication password.\n  # auth_password = \"pass\"\n  ## Security Level; one of \"noAuthNoPriv\", \"authNoPriv\", or \"authPriv\".\n  # sec_level = \"authNoPriv\"\n  ## Context Name.\n  # context_name = \"\"\n  ## Privacy protocol used for encrypted messages; one of \"DES\", \"AES\" or \"\".\n  # priv_protocol = \"\"\n  ## Privacy password used for encrypted messages.\n  # priv_password = \"\"\n\n  ## Add fields and tables defining the variables you wish to collect.  This\n  ## example collects the system uptime and interface variables.  Reference the\n  ## full plugin documentation for configuration details.\n  [[inputs.snmp.field]]\n    oid = \"RFC1213-MIB::sysUpTime.0\"\n    name = \"uptime\"\n\n  [[inputs.snmp.field]]\n    oid = \"RFC1213-MIB::sysName.0\"\n    name = \"source\"\n    is_tag = true\n\n  [[inputs.snmp.table]]\n    oid = \"IF-MIB::ifTable\"\n    name = \"interface\"\n    inherit_tags = [\"source\"]\n\n    [[inputs.snmp.table.field]]\n      oid = \"IF-MIB::ifDescr\"\n      name = \"ifDescr\"\n      is_tag = true\n```\n\n### Configure SNMP Requests\n\nThis plugin provides two methods for configuring the SNMP requests: `fields`\nand `tables`. Use the `field` option to gather single ad-hoc variables.\nTo collect SNMP tables, use the `table` option.\n\n#### Field\n\nUse a `field` to collect a variable by OID. Requests specified with this\noption operate similar to the `snmpget` utility.\n\n```toml\n[[inputs.snmp]]\n  # ... snip ...\n\n  [[inputs.snmp.field]]\n    ## Object identifier of the variable as a numeric or textual OID.\n    oid = \"RFC1213-MIB::sysName.0\"\n\n    ## Name of the field or tag to create.  If not specified, it defaults to\n    ## the value of 'oid'. If 'oid' is numeric, an attempt to translate the\n    ## numeric OID into a textual OID will be made.\n    # name = \"\"\n\n    ## If true the variable will be added as a tag, otherwise a field will be\n    ## created.\n    # is_tag = false\n\n    ## Apply one of the following conversions to the variable value:\n    ##   float(X) Convert the input value into a float and divides by the\n    ##            Xth power of 10. Effectively just moves the decimal left\n    ##            X places. For example a value of `123` with `float(2)`\n    ##            will result in `1.23`.\n    ##   float:   Convert the value into a float with no adjustment. Same\n    ##            as `float(0)`.\n    ##   int:     Convert the value into an integer.\n    ##   hwaddr:  Convert the value to a MAC address.\n    ##   ipaddr:  Convert the value to an IP address.\n    # conversion = \"\"\n```\n\n#### Table\n\nUse a `table` to configure the collection of a SNMP table. SNMP requests\nformed with this option operate similarly way to the `snmptable` command.\n\nControl the handling of specific table columns using a nested `field`. These\nnested fields are specified similarly to a top-level `field`.\n\nBy default all columns of the SNMP table will be collected - it is not required\nto add a nested field for each column, only those which you wish to modify. To\n_only_ collect certain columns, omit the `oid` from the `table` section and only\ninclude `oid` settings in `field` sections. For more complex include/exclude\ncases for columns use [metric filtering](https://github.com/circonus-labs/circonus-unified-agent/blob/master/docs/CONFIGURATION.md#metric-filtering).\n\nOne [metric](https://github.com/circonus-labs/circonus-unified-agent/blob/master/docs/METRICS.md) is created for each row of the SNMP table.\n\n```toml\n[[inputs.snmp]]\n  # ... snip ...\n\n  [[inputs.snmp.table]]\n    ## Object identifier of the SNMP table as a numeric or textual OID.\n    oid = \"IF-MIB::ifTable\"\n\n    ## Name of the field or tag to create.  If not specified, it defaults to\n    ## the value of 'oid'.  If 'oid' is numeric an attempt to translate the\n    ## numeric OID into a textual OID will be made.\n    # name = \"\"\n\n    ## Which tags to inherit from the top-level config and to use in the output\n    ## of this table's measurement.\n    ## example: inherit_tags = [\"source\"]\n    # inherit_tags = []\n\n    ## Add an 'index' tag with the table row number.  Use this if the table has\n    ## no indexes or if you are excluding them.  This option is normally not\n    ## required as any index columns are automatically added as tags.\n    # index_as_tag = false\n\n    [[inputs.snmp.table.field]]\n      ## OID to get. May be a numeric or textual module-qualified OID.\n      oid = \"IF-MIB::ifDescr\"\n\n      ## Name of the field or tag to create.  If not specified, it defaults to\n      ## the value of 'oid'. If 'oid' is numeric an attempt to translate the\n      ## numeric OID into a textual OID will be made.\n      # name = \"\"\n\n      ## Output this field as a tag.\n      # is_tag = false\n\n      ## The OID sub-identifier to strip off so that the index can be matched\n      ## against other fields in the table.\n      # oid_index_suffix = \"\"\n\n      ## Specifies the length of the index after the supplied table OID (in OID\n      ## path segments). Truncates the index after this point to remove non-fixed\n      ## value or length index suffixes.\n      # oid_index_length = 0\n\n      ## Specifies if the value of given field should be snmptranslated\n      ## by default no field values are translated\n      # translate = true\n```\n\n## Troubleshooting\n\nCheck that a numeric field can be translated to a textual field:\n\n```shell\nsnmptranslate .1.3.6.1.2.1.1.3.0\nDISMAN-EVENT-MIB::sysUpTimeInstance\n```\n\nRequest a top-level field:\n\n```shell\nsnmpget -v2c -c public 127.0.0.1 sysUpTime.0\n```\n\nRequest a table:\n\n```shell\nsnmptable -v2c -c public 127.0.0.1 ifTable\n```\n\nTo collect a packet capture, run this command in the background while running\nagent or one of the above commands. Adjust the interface, host and port as\nneeded:\n\n```shell\nsudo tcpdump -s 0 -i eth0 -w circonus-unified-agent-snmp.pcap host 127.0.0.1 and port 161\n```\n","logo":{"light":"/img/library/snmp-trap.svg"},"attributes":{"implementation":"cua"},"tags":["network","management","protocol"],"module":"httptrap:cua:snmp_trap"},{"name":"snmp","title":"SNMP","content":"# SNMP\n\n## Overview\n\nThe `snmp` input plugin uses polling to gather metrics from SNMP agents.\nSupport for gathering individual OIDs as well as complete SNMP tables is\nincluded.\n\nThis plugin sends metrics directly to Circonus - it bypasses aggregators, processors, parsers, and outputs.\nThe intended use-case is for sitations where a large number of SNMP collector instances is required. In such\ncases optimizations and efficiencies can be gained by sending metrics directly.\n\nThe `snmp` input plugin introduces a new capability to send metrics directly to Circonus `direct_metrics`. Useful when\nthere are a large number of snmp plugin instances.\n\n### Prerequisites\n\nThis plugin uses the `snmptable` and `snmptranslate` programs from the\n[net-snmp](http://www.net-snmp.org/) project. These tools will need to be installed into the `PATH` in\norder to be located. Other utilities from the net-snmp project may be useful\nfor troubleshooting, but are not directly used by the plugin.\n\nThese programs will load available MIBs on the system. Typically the default\ndirectory for MIBs is `/usr/share/snmp/mibs`, but if your MIBs are in a\ndifferent location you may need to make the paths known to net-snmp. The\nlocation of these files can be configured in the `snmp.conf` or via the\n`MIBDIRS` environment variable. See [`man 1 snmpcmd`](http://net-snmp.sourceforge.net/docs/man/snmpcmd.html#lbAK) for more\ninformation.\n\n## Configuration\n\n```toml\n[[inputs.snmp]]\n  ## Instance ID is required\n  instance_id = \"\"\n  ## Direct metrics\n  # direct_metrics = false\n\n  ## example of collecting hundreds of devices on a 5m cadence\n  # collection interval\n  interval = \"5m\"\n  # flush interval, so cua does not log warning about input\n  # taking too long w/flush delay\n  flush_interval = \"5m\"\n  # spread flushing submissions over a two minute window\n  flush_delay = \"2m\"\n\n  ## Agent addresses to retrieve values from.\n  ##   example: agents = [\"udp://127.0.0.1:161\"]\n  ##            agents = [\"tcp://127.0.0.1:161\"]\n  agents = [\"udp://127.0.0.1:161\"]\n\n  ## Timeout for each request.\n  # timeout = \"5s\"\n\n  ## SNMP version; can be 1, 2, or 3.\n  # version = 2\n\n  ## SNMP community string.\n  # community = \"public\"\n\n  ## Agent host tag\n  # agent_host_tag = \"agent_host\"\n\n  ## Number of retries to attempt.\n  # retries = 3\n\n  ## The GETBULK max-repetitions parameter.\n  # max_repetitions = 10\n\n  ## SNMPv3 authentication and encryption options.\n  ##\n  ## Security Name.\n  # sec_name = \"myuser\"\n  ## Authentication protocol; one of \"MD5\", \"SHA\", or \"\".\n  # auth_protocol = \"MD5\"\n  ## Authentication password.\n  # auth_password = \"pass\"\n  ## Security Level; one of \"noAuthNoPriv\", \"authNoPriv\", or \"authPriv\".\n  # sec_level = \"authNoPriv\"\n  ## Context Name.\n  # context_name = \"\"\n  ## Privacy protocol used for encrypted messages; one of \"DES\", \"AES\" or \"\".\n  # priv_protocol = \"\"\n  ## Privacy password used for encrypted messages.\n  # priv_password = \"\"\n\n  ## Add fields and tables defining the variables you wish to collect.  This\n  ## example collects the system uptime and interface variables.  Reference the\n  ## full plugin documentation for configuration details.\n  [[inputs.snmp.field]]\n    oid = \"RFC1213-MIB::sysUpTime.0\"\n    name = \"uptime\"\n\n  [[inputs.snmp.field]]\n    oid = \"RFC1213-MIB::sysName.0\"\n    name = \"source\"\n    is_tag = true\n\n  [[inputs.snmp.table]]\n    oid = \"IF-MIB::ifTable\"\n    name = \"interface\"\n    inherit_tags = [\"source\"]\n\n    [[inputs.snmp.table.field]]\n      oid = \"IF-MIB::ifDescr\"\n      name = \"ifDescr\"\n      is_tag = true\n```\n\n### Configure SNMP Requests\n\nThis plugin provides two methods for configuring the SNMP requests: `fields`\nand `tables`. Use the `field` option to gather single ad-hoc variables.\nTo collect SNMP tables, use the `table` option.\n\n#### Field\n\nUse a `field` to collect a variable by OID. Requests specified with this\noption operate similar to the `snmpget` utility.\n\n```toml\n[[inputs.snmp]]\n  # ... snip ...\n\n  [[inputs.snmp.field]]\n    ## Object identifier of the variable as a numeric or textual OID.\n    oid = \"RFC1213-MIB::sysName.0\"\n\n    ## Name of the field or tag to create.  If not specified, it defaults to\n    ## the value of 'oid'. If 'oid' is numeric, an attempt to translate the\n    ## numeric OID into a textual OID will be made.\n    # name = \"\"\n\n    ## If true the variable will be added as a tag, otherwise a field will be\n    ## created.\n    # is_tag = false\n\n    ## Apply one of the following conversions to the variable value:\n    ##   float(X) Convert the input value into a float and divides by the\n    ##            Xth power of 10. Effectively just moves the decimal left\n    ##            X places. For example a value of `123` with `float(2)`\n    ##            will result in `1.23`.\n    ##   float:   Convert the value into a float with no adjustment. Same\n    ##            as `float(0)`.\n    ##   int:     Convert the value into an integer.\n    ##   hwaddr:  Convert the value to a MAC address.\n    ##   ipaddr:  Convert the value to an IP address.\n    # conversion = \"\"\n```\n\n#### Table\n\nUse a `table` to configure the collection of a SNMP table. SNMP requests\nformed with this option operate similarly way to the `snmptable` command.\n\nControl the handling of specific table columns using a nested `field`. These\nnested fields are specified similarly to a top-level `field`.\n\nBy default all columns of the SNMP table will be collected - it is not required\nto add a nested field for each column, only those which you wish to modify. To\n_only_ collect certain columns, omit the `oid` from the `table` section and only\ninclude `oid` settings in `field` sections. For more complex include/exclude\ncases for columns use [metric filtering](https://github.com/circonus-labs/circonus-unified-agent/blob/master/docs/CONFIGURATION.md#metric-filtering).\n\nOne [metric](https://github.com/circonus-labs/circonus-unified-agent/blob/master/docs/METRICS.md) is created for each row of the SNMP table.\n\n```toml\n[[inputs.snmp]]\n  # ... snip ...\n\n  [[inputs.snmp.table]]\n    ## Object identifier of the SNMP table as a numeric or textual OID.\n    oid = \"IF-MIB::ifTable\"\n\n    ## Name of the field or tag to create.  If not specified, it defaults to\n    ## the value of 'oid'.  If 'oid' is numeric an attempt to translate the\n    ## numeric OID into a textual OID will be made.\n    # name = \"\"\n\n    ## Which tags to inherit from the top-level config and to use in the output\n    ## of this table's measurement.\n    ## example: inherit_tags = [\"source\"]\n    # inherit_tags = []\n\n    ## Add an 'index' tag with the table row number.  Use this if the table has\n    ## no indexes or if you are excluding them.  This option is normally not\n    ## required as any index columns are automatically added as tags.\n    # index_as_tag = false\n\n    [[inputs.snmp.table.field]]\n      ## OID to get. May be a numeric or textual module-qualified OID.\n      oid = \"IF-MIB::ifDescr\"\n\n      ## Name of the field or tag to create.  If not specified, it defaults to\n      ## the value of 'oid'. If 'oid' is numeric an attempt to translate the\n      ## numeric OID into a textual OID will be made.\n      # name = \"\"\n\n      ## Output this field as a tag.\n      # is_tag = false\n\n      ## The OID sub-identifier to strip off so that the index can be matched\n      ## against other fields in the table.\n      # oid_index_suffix = \"\"\n\n      ## Specifies the length of the index after the supplied table OID (in OID\n      ## path segments). Truncates the index after this point to remove non-fixed\n      ## value or length index suffixes.\n      # oid_index_length = 0\n\n      ## Specifies if the value of given field should be snmptranslated\n      ## by default no field values are translated\n      # translate = true\n```\n\n## Troubleshooting\n\nCheck that a numeric field can be translated to a textual field:\n\n```shell\nsnmptranslate .1.3.6.1.2.1.1.3.0\nDISMAN-EVENT-MIB::sysUpTimeInstance\n```\n\nRequest a top-level field:\n\n```shell\nsnmpget -v2c -c public 127.0.0.1 sysUpTime.0\n```\n\nRequest a table:\n\n```shell\nsnmptable -v2c -c public 127.0.0.1 ifTable\n```\n\nTo collect a packet capture, run this command in the background while running\nagent or one of the above commands. Adjust the interface, host and port as\nneeded:\n\n```shell\nsudo tcpdump -s 0 -i eth0 -w circonus-unified-agent-snmp.pcap host 127.0.0.1 and port 161\n```\n","logo":{"light":"/img/library/snmp.svg"},"attributes":{"implementation":"cua"},"tags":["network","management","protocol"],"module":"httptrap:cua:snmp"},{"name":"solr","title":"Solr","content":"# Solr\n\n## Overview\n\nThe [solr](http://lucene.apache.org/solr/) plugin collects stats via the\n[MBean Request Handler](https://cwiki.apache.org/confluence/display/solr/MBean+Request+Handler)\n\nMore about [performance statistics](https://cwiki.apache.org/confluence/display/solr/Performance+Statistics+Reference)\n\nTested from 3.5 to 7.\\*\n\n## Configuration\n\n```toml\n[[inputs.solr]]\n  ## specify a list of one or more Solr servers\n  servers = [\"http://localhost:8983\"]\n  ##\n  ## specify a list of one or more Solr cores (default - all)\n  # cores = [\"main\"]\n  ##\n  ## Optional HTTP Basic Auth Credentials\n  # username = \"username\"\n  # password = \"pa$$word\"\n```\n","logo":{"light":"/img/library/solr.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:solr"},{"name":"ssh2","title":"SSH2","content":"# SSH2\n\n## Overview\n\nMonitor your SSH server's banner and host key fingerprint. The SSH2 check does not actually perform a login on the target. It proceeds only as far as key exchange, to obtain the server's host-key fingerprint, and then closes the connection.\n\n## Configuration\n\nExcept for the target host/IP, there are no required parameters. However, the Circonus UI will pre-fill many of the Advanced Configuration fields with values that are broadly compatible with various SSH server implementations.\n\nOptional parameters:\n|Name|Description|\n|----|-----------|\n|port|The TCP port on which the remote server's ssh service is running (default: 22).\nmethod_hostkey|The host key algorithm supported. Choices are ssh-dss, ssh-rsa, ecdsa-sha2-nistp, or ssh-ed25519 (default: \"ssh-rsa\").|\n|method_kex|The key exchange method to use. Choices are diffie-hellman-group1-sha1, diffie-hellman-group14-sha1, diffie-hellman-group16-sha512, or diffie-hellman-group18-sha512 (default: \"diffie-hellman-group14-sha1\").|\n|method_crypt_cs|The encryption algorithm used from client to server. Choices are chacha20-poly1305@openssh.com, aes256-gcm@openssh.com, aes128-gcm@openssh.com, aes256-ctr, aes192-ctr, aes128-ctr, aes256-cbc, aes192-cbc, aes128-cbc, rijndael128-cbc, rijndael192-cbc, rijndael256-cbc, blowfish-cbc, blowfish-ecb, blowfish-cfb, blowfish-ofb, blowfish-ctr, twofish128-ctr, twofish128-cbc, twofish192-ctr, twofish192-cbc, twofish256-ctr, twofish256-cbc, twofish-cbc, twofish-ecb, twofish-cfb, twofish-ofb, arcfour256, arcfour128, arcfour, cast128-cbc, cast128-ecb, cast128-cfb, cast128-ofb, idea-cbc, idea-ecb, idea-cfb, idea-ofb, 3des-cbc, 3des-ecb, 3des-cfb, 3des-ofb, 3des-ctr, or none.|\n|method_crypt_sc|The encryption algorithm used from server to client. Choices are the same as for method_crypt_cs.|\n|method_mac_cs|The message authentication code algorithm used from client to server. Choices are hmac-sha2-512-etm@openssh.com, hmac-sha2-256-etm@openssh.com, umac-128-etm@openssh.com, umac-64-etm@openssh.com, hmac-sha2-512, hmac-sha2-256, hmac-sha1, hmac-sha1-96, hmac-md5, hmac-md5-96, hmac-ripemd160, hmac-ripemd160@openssh.com, or none.|\n|method_mac_sc|The message authentication code algorithm used from server to client. Choices are the same as for method_mac_cs.|\n|method_comp_cs|The compress algorithm used from client to server. Choices are zlib or none (default: \"none\").|\n|method_comp_sc|The compress algorithm used from server to client. Choices are the same as for method_comp_cs (default: \"none\").|\n\n## Metrics\n\nTypical metrics include:\n|Name|Type|Description|\n|----|----|-----------|\n|banner|text|The banner string received from the target server.|\n|error|text|If a connection error occurred, this metric is emitted and contains a brief reason for the error. If no error occurred, this metric is suppressed.|\n|key-type|text|The type of host key presented by the target server, such as RSA.|\n|bits|numeric|The size of the host key, in bits.|\n|fingerprint|text|MD5 checksum of the host key, in hexadecimal format.|\n|fingerprint_sha1|text|SHA1 checksum of the host key, in hexadecimal format.|\n|fingerprint_sha256|text|SHA256 checksum of the host key, in hexadecimal format.|\n|fingerprint_sha1_base64|text|SHA1 digest of the host key, base64-encoded.|\n|fingerprint_sha256_base64|text|SHA256 digest of the host key, base64-encoded.|\n|duration|numeric|Elapsed time from the start of the connection attempt to the end of the check operation, in milliseconds.|\n\nAdditional options allow you to specify the encryption method for client-to-server data encryption, the encryption method for server-to-client data encryption, and the hostkey type.\n","logo":{"light":"/img/library/ssh2.svg"},"attributes":{"implementation":"broker"},"tags":["protocol","network","system","authentication"],"module":"ssh2"},{"name":"statsd-legacy","title":"StatsD","content":"# StatsD\n\n## Overview\n\nPush count and timer metrics to your Circonus Enterprise Broker using Statsd over UDP (the standard way) or over TCP. This check does not pull data; you must push the data to the broker.\n\n## Configuration\n\nCreating a statsd check requires choosing the statsd check and selecting a target host.\n\n![Image: 'statsd_target3.png'](../../img/statsd_target3.png)\n\n> The target host must be an IP address and the packets that arrive at the broker must have that IP address as the source address.\n\nThe next step is optional. If you know the metrics you expect to see via statsd, you may enter them by clicking \"add new metric\".\n\n![Image: 'statsd_test_check3.png'](../../img/statsd_test_check3.png)\n\nEnter the name of the metric and its check type. You may repeat this process for multiple metrics.\n\n![Image: 'statsd_metric_entry3.png'](../../img/statsd_metric_entry3.png)\n\n> You may elect to collect no metrics at this time. Under normal operating conditions where data is being collected, you can view the check and see what metrics are arriving and available for collection (this could take a few minutes). Using the \"Update Metrics\" feature on the \"view check\" page, you can enable any metric you see. This is the preferred method if you have more than a few metrics to collect.\n\nWhen submitting data, send to port 8125/8126 of the system you are using to collect data (port 8125 via UDP or port 8126 via TCP).\n\n## Metrics\n\nMetrics are reported from Circonus in the following format:\n\n```\n<Metric Name>`<Metric Type>\n```\n\nCounting metric types:\n|Metric Type|Description|\n|-----------|-----------|\n|count|The number of events received for the metric.|\n|counter|The most recent counter value.|\n|rate|The sampling rate, if enabled.|\n\nTiming metric types:\n|Metric Type|Description|\n|-----------|-----------|\n|count|The number of events received for the metric.|\n|timing|The most recent timing value.|\n\nGauge metric types:\n|Metric Type|Description|\n|-----------|-----------|\n|count|The number of events received for the metric.|\n|gauge|The most recent gauge value.|\n\nSo, for example, if you have a counting metric named blah and wanted to pull both the count and counter types, you would look for the following metrics:\n\nblah\\`count\n\nblah\\`counter\n\n### Stream Tags\n\nTo submit stream tagged StatsD metrics into Circonus, you must pass in the StatsD metric name and append the tags to the series name. The format for a tagged StatsD metric is as follows:\n\nmetric_name:value|type|@sample_rate|#tag1:value,tag2,#tag3:value\n\n### Treatment of Counters\n\nStatsd counters in Circonus are converted into a histogram. They are implemented as a count of samples in the 0-bucket. CAQL provides a way to count these samples using `histogram:count()`. The 0-bucket here was chosen so that attempting to `histogram:sum()` the counter will result in a clearly incorrect `0` value. It is important to remember that histogram buckets are a range of values, so attempting to use the bucket that contains `1` would cause `histogram:sum()` to return a close, but incorrect value (within the guaranteed 5% error margin). The 0 bucket was chosen to return the most obviously incorrect value to minimize chances of this occurring.\n","logo":{"light":"../../img/library/statsd.svg"},"attributes":{"implementation":"broker","legacy":true},"tags":["push"],"module":"statsd_all"},{"name":"statsd","title":"StatsD","content":"# StatsD\n\n## Overview\n\nThe statsd plugin is a special type of plugin which runs a backgrounded statsd\nlistener service while agent is running.\n\nThe format of the statsd messages was based on the format described in the\noriginal [etsy statsd](https://github.com/etsy/statsd/blob/master/docs/metric_types.md)\nimplementation. In short, the agent statsd listener will accept:\n\n- Gauges\n\n  - `users.current.den001.myapp:32|g` <- standard\n  - `users.current.den001.myapp:+10|g` <- additive\n  - `users.current.den001.myapp:-10|g`\n\n- Counters\n\n  - `deploys.test.myservice:1|c` <- increments by 1\n  - `deploys.test.myservice:101|c` <- increments by 101\n  - `deploys.test.myservice:1|c|@0.1` <- with sample rate, increments by 10\n\n- Sets\n  - `users.unique:101|s`\n  - `users.unique:101|s`\n  - `users.unique:102|s` <- would result in a count of 2 for `users.unique`\n- Timings & Histograms\n  - `load.time:320|ms`\n  - `load.time.nanoseconds:1|h`\n  - `load.time:200|ms|@0.1` <- sampled 1/10 of the time\n\nIt is possible to omit repetitive names and merge individual stats into a\nsingle line by separating them with additional colons:\n\n- `users.current.den001.myapp:32|g:+10|g:-10|g`\n- `deploys.test.myservice:1|c:101|c:1|c|@0.1`\n- `users.unique:101|s:101|s:102|s`\n- `load.time:320|ms:200|ms|@0.1`\n\nThis also allows for mixed types in a single line:\n\n- `foo:1|c:200|ms`\n\nThe string `foo:1|c:200|ms` is internally split into two individual metrics\n`foo:1|c` and `foo:200|ms` which are added to the aggregator separately.\n\n## Configuration\n\n```toml\n# Statsd Server\n[[inputs.statsd]]\n  ## Instance ID -- required\n  instance_id = \"\"\n\n  ## Protocol, must be \"tcp\", \"udp4\", \"udp6\" or \"udp\" (default=udp)\n  protocol = \"udp\"\n\n  ## MaxTCPConnection - applicable when protocol is set to tcp (default=250)\n  max_tcp_connections = 250\n\n  ## Enable TCP keep alive probes (default=false)\n  tcp_keep_alive = false\n\n  ## Specifies the keep-alive period for an active network connection.\n  ## Only applies to TCP sockets and will be ignored if tcp_keep_alive is false.\n  ## Defaults to the OS configuration.\n  # tcp_keep_alive_period = \"2h\"\n\n  ## Address and port to host UDP listener on\n  service_address = \":8125\"\n\n  ## separator to use between elements of a statsd metric\n  metric_separator = \"_\"\n\n  ## Parses extensions to statsd in the datadog statsd format\n  ## currently supports metrics and datadog tags.\n  ## http://docs.datadoghq.com/guides/dogstatsd/\n  datadog_extensions = true\n\n  ## Statsd data translation templates, more info can be read here:\n  ## https://github.com/circonus-labs/circonus-unified-agent/blob/master/docs/TEMPLATE_PATTERN.md\n  # templates = [\n  #     \"cpu.* measurement*\"\n  # ]\n\n  ## Number of UDP messages allowed to queue up, once filled,\n  ## the statsd server will start dropping packets\n  allowed_pending_messages = 10000\n\n  ## Maximum socket buffer size in bytes, once the buffer fills up, metrics\n  ## will start dropping.  Defaults to the OS default.\n  # read_buffer_size = 65535\n```\n\n### Plugin arguments\n\n- **protocol** string: Protocol used in listener - tcp or udp options\n\n- **max_tcp_connections** []int: Maximum number of concurrent TCP connections to allow. Used when protocol is set to tcp.\n\n- **tcp_keep_alive** boolean: Enable TCP keep alive probes\n\n- **tcp_keep_alive_period** internal.Duration: Specifies the keep-alive period for an active network connection\n\n- **service_address** string: Address to listen for statsd UDP packets on\n\n<!--\n\n* **delete_gauges** boolean: Delete gauges on every collection interval\n\n* **delete_counters** boolean: Delete counters on every collection interval\n\n* **delete_sets** boolean: Delete set counters on every collection interval\n\n* **delete_timings** boolean: Delete timings on every collection interval\n\n* **percentiles** []int: Percentiles to calculate for timing & histogram stats\n-->\n\n- **allowed_pending_messages** integer: Number of messages allowed to queue up waiting to be processed. When this fills, messages will be dropped and logged.\n\n<!--\n* **percentile_limit** integer: Number of timing/histogram values to track per-measurement in the calculation of percentiles. Raising this limit increases the accuracy of percentiles but also increases the memory usage and cpu time.\n-->\n\n- **templates** []string: Templates for transforming statsd buckets into Circonus measurements and tags.\n\n- **datadog_extensions** boolean: Enable parsing of DataDog's extensions to [dogstatsd format](http://docs.datadoghq.com/guides/dogstatsd/) Note: events are ignored at this time.\n\n#### Templates for Statsd bucket --> measurement name and tags\n\nThe plugin supports specifying templates for transforming statsd buckets into\nmeasurement names and tags. The templates have a _measurement_ keyword,\nwhich can be used to specify parts of the bucket that are to be used in the\nmeasurement name. Other words in the template are used as tag names. For example,\nthe following template:\n\n```toml\ntemplates = [\n    \"measurement.measurement.region\"\n]\n```\n\nwould result in the following transformation:\n\n```text\ncpu.load.us-west:100|g\n=> cpu_load,region=us-west 100\n```\n\nUsers can also filter the template to use based on the name of the bucket,\nusing glob matching, like so:\n\n```toml\ntemplates = [\n    \"cpu.* measurement.measurement.region\",\n    \"mem.* measurement.measurement.host\"\n]\n```\n\nwhich would result in the following transformation:\n\n```text\ncpu.load.us-west:100|g\n=> cpu_load,region=us-west 100\n\nmem.cached.localhost:256|g\n=> mem_cached,host=localhost 256\n```\n\nConsult the [Template Patterns](https://github.com/circonus-labs/circonus-unified-agent/blob/master/docs/TEMPLATE_PATTERN.md) documentation for\nadditional details.\n","logo":{},"attributes":{"implementation":"cua"},"module":"httptrap:cua:statsd"},{"name":"suricata","title":"Suricata","content":"# Suricata\n\n## Overview\n\nThis plugin reports internal performance counters of the Suricata IDS/IPS\nengine, such as captured traffic volume, memory usage, uptime, flow counters,\nand much more. It provides a socket for the Suricata log output to write JSON\nstats output to, and processes the incoming data to fit agent's format.\n\n## Configuration\n\n```toml\n[[inputs.suricata]]\n  ## Data sink for Suricata stats log.\n  # This is expected to be a filename of a\n  # unix socket to be created for listening.\n  source = \"/var/run/suricata-stats.sock\"\n\n  # Delimiter for flattening field keys, e.g. subitem \"alert\" of \"detect\"\n  # becomes \"detect_alert\" when delimiter is \"_\".\n  delimiter = \"_\"\n```\n\n### Suricata configuration\n\nSuricata needs to deliver the 'stats' event type to a given unix socket for\nthis plugin to pick up. This can be done, for example, by creating an additional\noutput in the Suricata configuration file:\n\n```yaml\n- eve-log:\n    enabled: yes\n    filetype: unix_stream\n    filename: /tmp/suricata-stats.sock\n    types:\n      - stats:\n          threads: yes\n```\n\n### FreeBSD tuning\n\nUnder FreeBSD it is necessary to increase the localhost buffer space to at least 16384, default is 8192\notherwise messages from Suricata are truncated as they exceed the default available buffer space,\nconsequently no statistics are processed by the plugin.\n\n```text\nsysctl -w net.local.stream.recvspace=16384\nsysctl -w net.local.stream.sendspace=16384\n```\n","logo":{"light":"/img/library/suricata.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:suricata"},{"name":"synproxy","title":"Synproxy","content":"# Synproxy\n\n## Overview\n\nThe synproxy plugin gathers the synproxy counters. Synproxy is a Linux netfilter module used for SYN attack mitigation.\nThe use of synproxy is documented in `man iptables-extensions` under the SYNPROXY section.\n\n## Configuration\n\nThe synproxy plugin does not need any configuration\n\n```toml\n[[inputs.synproxy]]\n  # no configuration\n```\n\n## Troubleshooting\n\nExecute the following CLI command in Linux to test the synproxy counters:\n\n```sh\ncat /proc/net/stat/synproxy\n```\n","logo":{"light":"/img/library/synproxy.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:synproxy"},{"name":"sysstat","title":"sysstat","content":"# sysstat\n\n## Overview\n\nCollect [sysstat](https://github.com/sysstat/sysstat) metrics - requires the sysstat\npackage installed.\n\nThis plugin collects system metrics with the sysstat collector utility `sadc` and parses\nthe created binary data file with the `sadf` utility.\n\n## Configuration\n\n```toml\n# Sysstat metrics collector\n[[inputs.sysstat]]\n  ## Path to the sadc command.\n  #\n  ## On Debian and Arch Linux the default path is /usr/lib/sa/sadc whereas\n  ## on RHEL and CentOS the default path is /usr/lib64/sa/sadc\n  sadc_path = \"/usr/lib/sa/sadc\" # required\n\n  ## Path to the sadf command, if it is not in PATH\n  # sadf_path = \"/usr/bin/sadf\"\n\n  ## Activities is a list of activities, that are passed as argument to the\n  ## sadc collector utility (e.g: DISK, SNMP etc...)\n  ## The more activities that are added, the more data is collected.\n  # activities = [\"DISK\"]\n\n  ## Group metrics to measurements.\n  ##\n  ## If group is false each metric will be prefixed with a description\n  ## and represents itself a measurement.\n  ##\n  ## If Group is true, corresponding metrics are grouped to a single measurement.\n  # group = true\n\n  ## Options for the sadf command. The values on the left represent the sadf options and\n  ## the values on the right their description (wich are used for grouping and prefixing metrics).\n  ##\n  ## Run 'sar -h' or 'man sar' to find out the supported options for your sysstat version.\n  [inputs.sysstat.options]\n\t-C = \"cpu\"\n\t-B = \"paging\"\n\t-b = \"io\"\n\t-d = \"disk\"             # requires DISK activity\n\t\"-n ALL\" = \"network\"\n\t\"-P ALL\" = \"per_cpu\"\n\t-q = \"queue\"\n\t-R = \"mem\"\n\t-r = \"mem_util\"\n\t-S = \"swap_util\"\n\t-u = \"cpu_util\"\n\t-v = \"inode\"\n\t-W = \"swap\"\n\t-w = \"task\"\n  #\t-H = \"hugepages\"        # only available for newer linux distributions\n  #\t\"-I ALL\" = \"interrupts\" # requires INT activity\n\n  ## Device tags can be used to add additional tags for devices. For example the configuration below\n  ## adds a tag vg with value rootvg for all metrics with sda devices.\n  # [[inputs.sysstat.device_tags.sda]]\n  #  vg = \"rootvg\"\n```\n","logo":{"light":"/img/library/sysstat.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:sysstat"},{"name":"systemd-units","title":"systemd Units","content":"# systemd Units\n\n## Overview\n\nThe systemd_units plugin gathers systemd unit status on Linux. It relies on\n`systemctl list-units --all --plain --type=service` to collect data on service status.\n\nThe results are tagged with the unit name and provide enumerated fields for\nloaded, active and running fields, indicating the unit health.\n\nThis plugin is related to the [win_services module](https://github.com/circonus-labs/circonus-unified-agent/blob/master/plugins/inputs/win_services), which\nfulfills the same purpose on windows.\n\nIn addition to services, this plugin can gather other unit types as well,\nsee `systemctl list-units --all --type help` for possible options.\n\n## Configuration\n\n```toml\n[[inputs.systemd_units]]\n  ## Set timeout for systemctl execution\n  # timeout = \"1s\"\n  #\n  ## Filter for a specific unit type, default is \"service\", other possible\n  ## values are \"socket\", \"target\", \"device\", \"mount\", \"automount\", \"swap\",\n  ## \"timer\", \"path\", \"slice\" and \"scope \":\n  # unittype = \"service\"\n```\n","logo":{"light":"/img/library/systemd.svg","dark":"/img/library/systemd-dark.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:systemd_units"},{"name":"tail","title":"Tail","content":"# Tail\n\n## Overview\n\nThe tail plugin \"tails\" a logfile and parses each log message.\n\nBy default, the tail plugin acts like the following unix tail command:\n\n```\ntail -F --lines=0 myfile.log\n```\n\n- `-F` means that it will follow the _name_ of the given file, so\n  that it will be compatible with log-rotated files, and that it will retry on\n  inaccessible files.\n- `--lines=0` means that it will start at the end of the file (unless\n  the `from_beginning` option is set).\n\nsee http://man7.org/linux/man-pages/man1/tail.1.html for more details.\n\nThe plugin expects messages in one of the\n[Input Data Formats](https://github.com/circonus-labs/circonus-unified-agent/blob/master/docs/DATA_FORMATS_INPUT.md).\n\n## Configuration\n\n```toml\n[[inputs.tail]]\n  ## File names or a pattern to tail.\n  ## These accept standard unix glob matching rules, but with the addition of\n  ## ** as a \"super asterisk\". ie:\n  ##   \"/var/log/**.log\"  -> recursively find all .log files in /var/log\n  ##   \"/var/log/*/*.log\" -> find all .log files with a parent dir in /var/log\n  ##   \"/var/log/apache.log\" -> just tail the apache log file\n  ##\n  ## See https://github.com/gobwas/glob for more examples\n  ##\n  files = [\"/var/mymetrics.out\"]\n\n  ## Read file from beginning.\n  # from_beginning = false\n\n  ## Whether file is a named pipe\n  # pipe = false\n\n  ## Method used to watch for file updates.  Can be either \"inotify\" or \"poll\".\n  # watch_method = \"inotify\"\n\n  ## Maximum lines of the file to process that have not yet be written by the\n  ## output.  For best throughput set based on the number of metrics on each\n  ## line and the size of the output's metric_batch_size.\n  # max_undelivered_lines = 1000\n\n  ## Character encoding to use when interpreting the file contents.  Invalid\n  ## characters are replaced using the unicode replacement character.  When set\n  ## to the empty string the data is not decoded to text.\n  ##   ex: character_encoding = \"utf-8\"\n  ##       character_encoding = \"utf-16le\"\n  ##       character_encoding = \"utf-16be\"\n  ##       character_encoding = \"\"\n  # character_encoding = \"\"\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/circonus-labs/circonus-unified-agent/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"json\"\n\n  ## multiline parser/codec\n  ## https://www.elastic.co/guide/en/logstash/2.4/plugins-filters-multiline.html\n  #[inputs.tail.multiline]\n    ## The pattern should be a regexp which matches what you believe to be an indicator that the field is part of an event consisting of multiple lines of log data.\n    #pattern = \"^\\s\"\n\n    ## The field's value must be previous or next and indicates the relation to the\n    ## multi-line event.\n    #match_which_line = \"previous\"\n\n    ## The invert_match can be true or false (defaults to false).\n    ## If true, a message not matching the pattern will constitute a match of the multiline filter and the what will be applied. (vice-versa is also true)\n    #invert_match = false\n\n    #After the specified timeout, this plugin sends the multiline event even if no new pattern is found to start a new event. The default is 5s.\n    #timeout = 5s\n```\n","logo":{"light":"/img/library/tail.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:tail"},{"name":"tcp","title":"TCP","content":"# TCP\n\n## Overview\n\nCheck TCP service availability and responsiveness.\n\n## Configuration\n\nRequired parameters:\n|Name|Description|\n|----|-----------|\n|port|The TCP port on which the target service can be reached.|\n\nOptional parameters:\n|Name|Description|\n|----|-----------|\n|banner_match|This regular expression is matched against the response banner. If a match is not found, the check will be marked as bad.|\n|use_ssl|Upgrade the connection to use SSL/TLS.|\n\n## Metrics\n\nTypical metrics include:\n|Name|Type|Description|\n|----|----|-----------|\n|banner_match|text|If the banner_match parameter is supplied, this metric's value will be the string that resulted from the match.|\n|tt_connect|numeric|Elapsed time from the start of the check to completing the TCP connection, in milliseconds.|\n|duration|numeric|Elapsed time from the start of the check to its completion, in milliseconds.|\\\n","logo":{"light":"/img/library/tcp.svg"},"attributes":{"implementation":"broker"},"tags":["protocol","network","availability"],"module":"tcp"},{"name":"teamspeak","title":"Teamspeak","content":"# Teamspeak\n\n## Overview\n\nThis plugin uses the Teamspeak 3 ServerQuery interface of the Teamspeak server to collect statistics of one or more\nvirtual servers. If you are querying an external Teamspeak server, make sure to add the host which is running the agent\nto query_ip_whitelist.txt in the Teamspeak Server directory.\n\n## Configuration\n\n```toml\n# Reads metrics from a Teamspeak 3 Server via ServerQuery\n[[inputs.teamspeak]]\n  ## Server address for Teamspeak 3 ServerQuery\n  # server = \"127.0.0.1:10011\"\n  ## Username for ServerQuery\n  username = \"serverqueryuser\"\n  ## Password for ServerQuery\n  password = \"secret\"\n  ## Array of virtual servers\n  # virtual_servers = [1]\n```\n","logo":{"light":"/img/library/teamspeak.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:teamspeak"},{"name":"tengine","title":"Tengine","content":"# Tengine\n\n## Overview\n\nThe tengine plugin gathers metrics from the\n[Tengine Web Server](http://tengine.taobao.org/) via the\n[reqstat](http://tengine.taobao.org/document/http_reqstat.html) module.\n\n## Configuration\n\n```toml\n# Read Tengine's basic status information (ngx_http_reqstat_module)\n[[inputs.tengine]]\n  ## An array of Tengine reqstat module URI to gather stats.\n  urls = [\"http://127.0.0.1/us\"]\n\n  ## HTTP response timeout (default: 5s)\n  # response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/circonus-unified-agent/ca.pem\"\n  # tls_cert = \"/etc/circonus-unified-agent/cert.pem\"\n  # tls_key = \"/etc/circonus-unified-agent/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n```\n","logo":{"light":"/img/library/tengine.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:tengine"},{"name":"unbound","title":"Unbound","content":"# Unbound\n\n## Overview\n\nThis plugin gathers stats from [Unbound](https://www.unbound.net/) -\na validating, recursive, and caching DNS resolver.\n\n## Configuration\n\n```toml\n# A plugin to collect stats from the Unbound DNS resolver\n[[inputs.unbound]]\n  ## Address of server to connect to, read from unbound conf default, optionally ':port'\n  ## Will lookup IP if given a hostname\n  server = \"127.0.0.1:8953\"\n\n  ## If running as a restricted user you can prepend sudo for additional access:\n  # use_sudo = false\n\n  ## The default location of the unbound-control binary can be overridden with:\n  # binary = \"/usr/sbin/unbound-control\"\n\n  ## The default location of the unbound config file can be overridden with:\n  # config_file = \"/etc/unbound/unbound.conf\"\n\n  ## The default timeout of 1s can be overridden with:\n  # timeout = \"1s\"\n\n  ## When set to true, thread metrics are tagged with the thread id.\n  ##\n  ## The default is false for backwards compatibility, and will be changed to\n  ## true in a future version.  It is recommended to set to true on new\n  ## deployments.\n  thread_as_tag = false\n```\n\n### Permissions\n\nIt's important to note that this plugin references unbound-control, which may require additional permissions to execute successfully.\nDepending on the user/group permissions of the user executing this plugin, you may need to alter the group membership, set facls, or use sudo.\n\n**Group membership (Recommended)**:\n\n```bash\n$ groups cua\ncua : cua\n\n$ usermod -a -G unbound cua\n\n$ groups cua\ncua : cua unbound\n```\n\n**Sudo privileges**:\n\nIf you use this method, you will need the following in your config:\n\n```toml\n[[inputs.unbound]]\n  use_sudo = true\n```\n\nYou will also need to update your sudoers file:\n\n```bash\n$ visudo\n# Add the following line:\nCmnd_Alias UNBOUNDCTL = /usr/sbin/unbound-control\ncua  ALL=(ALL) NOPASSWD: UNBOUNDCTL\nDefaults!UNBOUNDCTL !logfile, !syslog, !pam_session\n```\n\nPlease use the solution you see as most appropriate.\n","logo":{"light":"/img/library/unbound.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:unbound"},{"name":"uwsgi","title":"uWSGI","content":"# uWSGI\n\n## Overview\n\nThe uWSGI input plugin gathers metrics about uWSGI using its [Stats Server](https://uwsgi-docs.readthedocs.io/en/latest/StatsServer.html).\n\n## Configuration\n\n```toml\n[[inputs.uwsgi]]\n  ## List with urls of uWSGI Stats servers. Url must match pattern:\n  ## scheme://address[:port]\n  ##\n  ## For example:\n  ## servers = [\"tcp://localhost:5050\", \"http://localhost:1717\", \"unix:///tmp/statsock\"]\n  servers = [\"tcp://127.0.0.1:1717\"]\n\n  ## General connection timeout\n  # timeout = \"5s\"\n```\n","logo":{"light":"/img/library/uwsgi.svg","dark":"/img/library/uwsgi-dark.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:uwsgi"},{"name":"varnish-legacy","title":"Varnish","content":"# Varnish\n\nThis check type gathers Varnish statistics.\n\n## Overview\n\nGather Varnish statistics. The check will connect to the Varnish server and issue the stats command. Metrics will be created for each key seen in the server's response, and each metric's value will be the value of that key.\n\nFor more info about Varnish, please see the Varnish site.\n\n## Configuration\n\nThis check is only compatible with versions of Varnish prior to 3.0 (in Varnish 3.0, the stats command was removed).\n\nExcept for the target host/IP, there are no required parameters.\n\nOptional parameters:\n|Name|Description|\n|----|-----------|\n|port|The TCP port on which to contact the Varnish server (default: 8081).|\n\n## Metrics\n\nTypical metrics include:\n|Name|Type|\n|----|----|\n|bytes_free|numeric|\n|Client_uptime|numeric|\n|N_backends|numeric|\n|N_large_free_smf|numeric|\n|N_new_purges_added|numeric|\n|N_struct_sess_mem|numeric|\n|N_struct_smf|numeric|\n|N_total_active_purges|numeric|\n|N_vcl_available|numeric|\n|N_vcl_total|numeric|\n|N_worker_threads|numeric|\n|N_worker_threads_created|numeric|\n|SHM_records|numeric|\n|SHM_writes|numeric|\n","logo":{"light":"/img/library/varnish.svg"},"attributes":{"implementation":"broker","legacy":true},"tags":["proxy","accelerator","load-balancer","cache","server"],"module":"varnish"},{"name":"varnish","title":"Varnish","content":"# Varnish\n\n## Overview\n\nThis plugin gathers stats from [Varnish HTTP Cache](https://varnish-cache.org/)\n\n## Configuration\n\n```toml\n[[inputs.varnish]]\n  ## If running as a restricted user you can prepend sudo for additional access:\n  #use_sudo = false\n\n  ## The default location of the varnishstat binary can be overridden with:\n  binary = \"/usr/bin/varnishstat\"\n\n  ## By default, agent gathers stats for 3 metric points.\n  ## Setting stats will override the defaults shown below.\n  ## Glob matching can be used, ie, stats = [\"MAIN.*\"]\n  ## stats may also be set to [\"*\"], which will collect all stats\n  stats = [\"MAIN.cache_hit\", \"MAIN.cache_miss\", \"MAIN.uptime\"]\n\n  ## Optional name for the varnish instance (or working directory) to query\n  ## Usually append after -n in varnish cli\n  # instance_name = instanceName\n\n  ## Timeout for varnishstat command\n  # timeout = \"1s\"\n```\n\n### Permissions\n\nIt's important to note that this plugin references varnishstat, which may require additional permissions to execute successfully.\nDepending on the user/group permissions of the agent user executing this plugin, you may need to alter the group membership, set facls, or use sudo.\n\n**Group membership (Recommended)**:\n\n```bash\n$ groups cua\ncua : cua\n\n$ usermod -a -G varnish cua\n\n$ groups cua\ncua : cua varnish\n```\n\n**Extended filesystem ACL's**:\n\n```bash\n$ getfacl /var/lib/varnish/<hostname>/_.vsm\n# file: var/lib/varnish/<hostname>/_.vsm\n# owner: root\n# group: root\nuser::rw-\ngroup::r--\nother::---\n\n$ setfacl -m u:cua:r /var/lib/varnish/<hostname>/_.vsm\n\n$ getfacl /var/lib/varnish/<hostname>/_.vsm\n# file: var/lib/varnish/<hostname>/_.vsm\n# owner: root\n# group: root\nuser::rw-\nuser:cua:r--\ngroup::r--\nmask::r--\nother::---\n```\n\n**Sudo privileges**:\n\nIf you use this method, you will need the following in your agent config:\n\n```toml\n[[inputs.varnish]]\n  use_sudo = true\n```\n\nYou will also need to update your sudoers file:\n\n```bash\n$ visudo\n# Add the following line:\nCmnd_Alias VARNISHSTAT = /usr/bin/varnishstat\ncua  ALL=(ALL) NOPASSWD: VARNISHSTAT\nDefaults!VARNISHSTAT !logfile, !syslog, !pam_session\n```\n\nPlease use the solution you see as most appropriate.\n","logo":{"light":"/img/library/varnish.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:varnish"},{"name":"vmware-vsphere","title":"VMware vSphere","content":"# VMware vSphere\n\n## Overview\n\nThe VMware vSphere plugin uses the vSphere API to gather metrics from multiple vCenter servers.\n\n- Clusters\n- Hosts\n- VMs\n- Datastores\n\n### Supported versions of vSphere\n\nThis plugin supports vSphere version 5.5 through 6.7.\n\n## Configuration\n\nNOTE: To disable collection of a specific resource type, simply exclude all metrics using the XX_metric_exclude.\nFor example, to disable collection of VMs, add this:\n\n```\nvm_metric_exclude = [ \"*\" ]\n```\n\n```toml\n# Read metrics from one or many vCenters\n[[inputs.vsphere]]\n    ## List of vCenter URLs to be monitored. These three lines must be uncommented\n  ## and edited for the plugin to work.\n  vcenters = [ \"https://vcenter.local/sdk\" ]\n  username = \"user@corp.local\"\n  password = \"secret\"\n\n  ## VMs\n  ## Typical VM metrics (if omitted or empty, all metrics are collected)\n  # vm_include = [ \"/*/vm/**\"] # Inventory path to VMs to collect (by default all are collected)\n  # vm_exclude = [] # Inventory paths to exclude\n  vm_metric_include = [\n    \"cpu.demand.average\",\n    \"cpu.idle.summation\",\n    \"cpu.latency.average\",\n    \"cpu.readiness.average\",\n    \"cpu.ready.summation\",\n    \"cpu.run.summation\",\n    \"cpu.usagemhz.average\",\n    \"cpu.used.summation\",\n    \"cpu.wait.summation\",\n    \"mem.active.average\",\n    \"mem.granted.average\",\n    \"mem.latency.average\",\n    \"mem.swapin.average\",\n    \"mem.swapinRate.average\",\n    \"mem.swapout.average\",\n    \"mem.swapoutRate.average\",\n    \"mem.usage.average\",\n    \"mem.vmmemctl.average\",\n    \"net.bytesRx.average\",\n    \"net.bytesTx.average\",\n    \"net.droppedRx.summation\",\n    \"net.droppedTx.summation\",\n    \"net.usage.average\",\n    \"power.power.average\",\n    \"virtualDisk.numberReadAveraged.average\",\n    \"virtualDisk.numberWriteAveraged.average\",\n    \"virtualDisk.read.average\",\n    \"virtualDisk.readOIO.latest\",\n    \"virtualDisk.throughput.usage.average\",\n    \"virtualDisk.totalReadLatency.average\",\n    \"virtualDisk.totalWriteLatency.average\",\n    \"virtualDisk.write.average\",\n    \"virtualDisk.writeOIO.latest\",\n    \"sys.uptime.latest\",\n  ]\n  # vm_metric_exclude = [] ## Nothing is excluded by default\n  # vm_instances = true ## true by default\n\n  ## Hosts\n  ## Typical host metrics (if omitted or empty, all metrics are collected)\n  # host_include = [ \"/*/host/**\"] # Inventory path to hosts to collect (by default all are collected)\n  # host_exclude [] # Inventory paths to exclude\n  host_metric_include = [\n    \"cpu.coreUtilization.average\",\n    \"cpu.costop.summation\",\n    \"cpu.demand.average\",\n    \"cpu.idle.summation\",\n    \"cpu.latency.average\",\n    \"cpu.readiness.average\",\n    \"cpu.ready.summation\",\n    \"cpu.swapwait.summation\",\n    \"cpu.usage.average\",\n    \"cpu.usagemhz.average\",\n    \"cpu.used.summation\",\n    \"cpu.utilization.average\",\n    \"cpu.wait.summation\",\n    \"disk.deviceReadLatency.average\",\n    \"disk.deviceWriteLatency.average\",\n    \"disk.kernelReadLatency.average\",\n    \"disk.kernelWriteLatency.average\",\n    \"disk.numberReadAveraged.average\",\n    \"disk.numberWriteAveraged.average\",\n    \"disk.read.average\",\n    \"disk.totalReadLatency.average\",\n    \"disk.totalWriteLatency.average\",\n    \"disk.write.average\",\n    \"mem.active.average\",\n    \"mem.latency.average\",\n    \"mem.state.latest\",\n    \"mem.swapin.average\",\n    \"mem.swapinRate.average\",\n    \"mem.swapout.average\",\n    \"mem.swapoutRate.average\",\n    \"mem.totalCapacity.average\",\n    \"mem.usage.average\",\n    \"mem.vmmemctl.average\",\n    \"net.bytesRx.average\",\n    \"net.bytesTx.average\",\n    \"net.droppedRx.summation\",\n    \"net.droppedTx.summation\",\n    \"net.errorsRx.summation\",\n    \"net.errorsTx.summation\",\n    \"net.usage.average\",\n    \"power.power.average\",\n    \"storageAdapter.numberReadAveraged.average\",\n    \"storageAdapter.numberWriteAveraged.average\",\n    \"storageAdapter.read.average\",\n    \"storageAdapter.write.average\",\n    \"sys.uptime.latest\",\n  ]\n    ## Collect IP addresses? Valid values are \"ipv4\" and \"ipv6\"\n  # ip_addresses = [\"ipv6\", \"ipv4\" ]\n\n  # host_metric_exclude = [] ## Nothing excluded by default\n  # host_instances = true ## true by default\n\n\n  ## Clusters\n  # cluster_include = [ \"/*/host/**\"] # Inventory path to clusters to collect (by default all are collected)\n  # cluster_exclude = [] # Inventory paths to exclude\n  # cluster_metric_include = [] ## if omitted or empty, all metrics are collected\n  # cluster_metric_exclude = [] ## Nothing excluded by default\n  # cluster_instances = false ## false by default\n\n  ## Datastores\n  # datastore_include = [ \"/*/datastore/**\"] # Inventory path to datastores to collect (by default all are collected)\n  # datastore_exclude = [] # Inventory paths to exclude\n  # datastore_metric_include = [] ## if omitted or empty, all metrics are collected\n  # datastore_metric_exclude = [] ## Nothing excluded by default\n  # datastore_instances = false ## false by default\n\n  ## Datacenters\n  # datacenter_include = [ \"/*/host/**\"] # Inventory path to clusters to collect (by default all are collected)\n  # datacenter_exclude = [] # Inventory paths to exclude\n  datacenter_metric_include = [] ## if omitted or empty, all metrics are collected\n  datacenter_metric_exclude = [ \"*\" ] ## Datacenters are not collected by default.\n  # datacenter_instances = false ## false by default\n\n  ## Plugin Settings\n  ## separator character to use for measurement and field names (default: \"_\")\n  # separator = \"_\"\n\n  ## number of objects to retrieve per query for realtime resources (vms and hosts)\n  ## set to 64 for vCenter 5.5 and 6.0 (default: 256)\n  # max_query_objects = 256\n\n  ## number of metrics to retrieve per query for non-realtime resources (clusters and datastores)\n  ## set to 64 for vCenter 5.5 and 6.0 (default: 256)\n  # max_query_metrics = 256\n\n  ## number of go routines to use for collection and discovery of objects and metrics\n  # collect_concurrency = 1\n  # discover_concurrency = 1\n\n  ## the interval before (re)discovering objects subject to metrics collection (default: 300s)\n  # object_discovery_interval = \"300s\"\n\n  ## timeout applies to any of the api request made to vcenter\n  # timeout = \"60s\"\n\n  ## When set to true, all samples are sent as integers.  Normally all\n  ## samples from vCenter, with the exception of percentages, are integer\n  ## values, but under some conditions, some averaging takes place internally in\n  ## the plugin. Setting this flag to \"false\" will send values as floats to\n  ## preserve the full precision when averaging takes place.\n  # use_int_samples = true\n\n  ## Custom attributes from vCenter can be very useful for queries in order to slice the\n  ## metrics along different dimension and for forming ad-hoc relationships. They are disabled\n  ## by default, since they can add a considerable amount of tags to the resulting metrics. To\n  ## enable, simply set custom_attribute_exclude to [] (empty set) and use custom_attribute_include\n  ## to select the attributes you want to include.\n  ## By default, since they can add a considerable amount of tags to the resulting metrics. To\n  ## enable, simply set custom_attribute_exclude to [] (empty set) and use custom_attribute_include\n  ## to select the attributes you want to include.\n  # custom_attribute_include = []\n  # custom_attribute_exclude = [\"*\"]\n\n  ## Optional SSL Config\n  # ssl_ca = \"/path/to/cafile\"\n  # ssl_cert = \"/path/to/certfile\"\n  # ssl_key = \"/path/to/keyfile\"\n  ## Use SSL but skip chain & host verification\n  # insecure_skip_verify = false\n```\n\n### Objects and Metrics Per Query\n\nBy default, in vCenter's configuration a limit is set to the number of entities that are included in a performance chart query. Default settings for vCenter 6.5 and above is 256. Prior versions of vCenter have this set to 64.\nA vCenter administrator can change this setting, see this [VMware KB article](https://kb.vmware.com/s/article/2107096) for more information.\n\nAny modification should be reflected in this plugin by modifying the parameter `max_query_objects`\n\n```\n  ## number of objects to retrieve per query for realtime resources (vms and hosts)\n  ## set to 64 for vCenter 5.5 and 6.0 (default: 256)\n  # max_query_objects = 256\n```\n\n### Collection and Discovery concurrency\n\nOn large vCenter setups it may be prudent to have multiple concurrent go routines collect performance metrics\nin order to avoid potential errors for time elapsed during a collection cycle. This should never be greater than 8,\nthough the default of 1 (no concurrency) should be sufficient for most configurations.\n\nFor setting up concurrency, modify `collect_concurrency` and `discover_concurrency` parameters.\n\n```\n  ## number of go routines to use for collection and discovery of objects and metrics\n  # collect_concurrency = 1\n  # discover_concurrency = 1\n```\n\n### Inventory Paths\n\nResources to be monitored can be selected using Inventory Paths. This treats the vSphere inventory as a tree structure similar\nto a file system. A vSphere inventory has a structure similar to this:\n\n```\n<root>\n+-DC0 # Virtual datacenter\n   +-datastore # Datastore folder (created by system)\n   | +-Datastore1\n   +-host # Host folder (created by system)\n   | +-Cluster1\n   | | +-Host1\n   | | | +-VM1\n   | | | +-VM2\n   | | | +-hadoop1\n   | +-Host2 # Dummy cluster created for non-clustered host\n   | | +-Host2\n   | | | +-VM3\n   | | | +-VM4\n   +-vm # VM folder (created by system)\n   | +-VM1\n   | +-VM2\n   | +-Folder1\n   | | +-hadoop1\n   | | +-NestedFolder1\n   | | | +-VM3\n   | | | +-VM4\n```\n\n#### Using Inventory Paths\n\nUsing familiar UNIX-style paths, one could select e.g. VM2 with the path `/DC0/vm/VM2`.\n\nOften, we want to select a group of resource, such as all the VMs in a folder. We could use the path `/DC0/vm/Folder1/*` for that.\n\nAnother possibility is to select objects using a partial name, such as `/DC0/vm/Folder1/hadoop*` yielding all vms in Folder1 with a name starting with \"hadoop\".\n\nFinally, due to the arbitrary nesting of the folder structure, we need a \"recursive wildcard\" for traversing multiple folders. We use the \"**\" symbol for that. If we want to look for a VM with a name starting with \"hadoop\" in any folder, we could use the following path: ```/DC0/vm/**/hadoop\\*```\n\n#### Multiple paths to VMs\n\nAs we can see from the example tree above, VMs appear both in its on folder under the datacenter, as well as under the hosts. This is useful when you like to select VMs on a specific host. For example, `/DC0/host/Cluster1/Host1/hadoop*` selects all VMs with a name starting with \"hadoop\" that are running on Host1.\n\nWe can extend this to looking at a cluster level: `/DC0/host/Cluster1/*/hadoop*`. This selects any VM matching \"hadoop\\*\" on any host in Cluster1.\n\n### Performance Considerations\n\n#### Realtime vs. historical metrics\n\nvCenter keeps two different kinds of metrics, known as realtime and historical metrics.\n\n- Realtime metrics: Available at a 20 second granularity. These metrics are stored in memory and are very fast and cheap to query. Our tests have shown that a complete set of realtime metrics for 7000 virtual machines can be obtained in less than 20 seconds. Realtime metrics are only available on **ESXi hosts** and **virtual machine** resources. Realtime metrics are only stored for 1 hour in vCenter.\n- Historical metrics: Available at a 5 minute, 30 minutes, 2 hours and 24 hours rollup levels. The vSphere agent plugin only uses the 5 minute rollup. These metrics are stored in the vCenter database and can be expensive and slow to query. Historical metrics are the only type of metrics available for **clusters**, **datastores** and **datacenters**.\n\nFor more information, refer to the vSphere documentation here: https://pubs.vmware.com/vsphere-50/index.jsp?topic=%2Fcom.vmware.wssdk.pg.doc_50%2FPG_Ch16_Performance.18.2.html\n\nThis distinction has an impact on how agent collects metrics. A single instance of an input plugin can have one and only one collection interval, which means that you typically set the collection interval based on the most frequently collected metric. Let's assume you set the collection interval to 1 minute. All realtime metrics will be collected every minute. Since the historical metrics are only available on a 5 minute interval, the vSphere agent plugin automatically skips four out of five collection cycles for these metrics. This works fine in many cases. Problems arise when the collection of historical metrics takes longer than the collection interval. This will cause error messages similar to this to appear in the agent logs:\n\n`2019-01-16T13:41:10Z W! [agent] input \"inputs.vsphere\" did not complete within its interval`\n\nThis will disrupt the metric collection and can result in missed samples. The best practice workaround is to specify two instances of the vSphere plugin, one for the realtime metrics with a short collection interval and one for the historical metrics with a longer interval. You can use the `*_metric_exclude` to turn off the resources you don't want to collect metrics for in each instance. For example:\n\n```toml\n## Realtime instance\n[[inputs.vsphere]]\n  interval = \"60s\"\n  vcenters = [ \"https://someaddress/sdk\" ]\n  username = \"someuser@vsphere.local\"\n  password = \"secret\"\n\n  insecure_skip_verify = true\n  force_discover_on_init = true\n\n  # Exclude all historical metrics\n  datastore_metric_exclude = [\"*\"]\n  cluster_metric_exclude = [\"*\"]\n  datacenter_metric_exclude = [\"*\"]\n\n  collect_concurrency = 5\n  discover_concurrency = 5\n\n# Historical instance\n[[inputs.vsphere]]\n\n  interval = \"300s\"\n\n  vcenters = [ \"https://someaddress/sdk\" ]\n  username = \"someuser@vsphere.local\"\n  password = \"secret\"\n\n  insecure_skip_verify = true\n  force_discover_on_init = true\n  host_metric_exclude = [\"*\"] # Exclude realtime metrics\n  vm_metric_exclude = [\"*\"] # Exclude realtime metrics\n\n  max_query_metrics = 256\n  collect_concurrency = 3\n```\n\n#### Configuring max_query_metrics setting\n\nThe `max_query_metrics` determines the maximum number of metrics to attempt to retrieve in one call to vCenter. Generally speaking, a higher number means faster and more efficient queries. However, the number of allowed metrics in a query is typically limited in vCenter by the `config.vpxd.stats.maxQueryMetrics` setting in vCenter. The value defaults to 64 on vSphere 5.5 and older and 256 on newver versions of vCenter. The vSphere plugin always checks this setting and will automatically reduce the number if the limit configured in vCenter is lower than max_query_metrics in the plugin. This will result in a log message similar to this:\n\n`2019-01-21T03:24:18Z W! [input.vsphere] Configured max_query_metrics is 256, but server limits it to 64. Reducing.`\n\nYou may ask a vCenter administrator to increase this limit to help boost performance.\n\n#### Cluster metrics and the max_query_metrics setting\n\nCluster metrics are handled a bit differently by vCenter. They are aggregated from ESXi and virtual machine metrics and may not be available when you query their most recent values. When this happens, vCenter will attempt to perform that aggregation on the fly. Unfortunately, all the subqueries needed internally in vCenter to perform this aggregation will count towards `config.vpxd.stats.maxQueryMetrics`. This means that even a very small query may result in an error message similar to this:\n\n`2018-11-02T13:37:11Z E! Error in plugin [inputs.vsphere]: ServerFaultCode: This operation is restricted by the administrator - 'vpxd.stats.maxQueryMetrics'. Contact your system administrator`\n\nThere are two ways of addressing this:\n\n- Ask your vCenter administrator to set `config.vpxd.stats.maxQueryMetrics` to a number that's higher than the total number of virtual machines managed by a vCenter instance.\n- Exclude the cluster metrics and use either the basicstats aggregator to calculate sums and averages per cluster or use queries in the visualization tool to obtain the same result.\n\n#### Concurrency settings\n\nThe vSphere plugin allows you to specify two concurrency settings:\n\n- `collect_concurrency`: The maximum number of simultaneous queries for performance metrics allowed per resource type.\n- `discover_concurrency`: The maximum number of simultaneous queries for resource discovery allowed.\n\nWhile a higher level of concurrency typically has a positive impact on performance, increasing these numbers too much can cause performance issues at the vCenter server. A rule of thumb is to set these parameters to the number of virtual machines divided by 1500 and rounded up to the nearest integer.\n","logo":{"light":"/img/library/vmware.svg","dark":"/img/library/vmware-dark.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:vsphere"},{"name":"webhooks","title":"Webhooks","content":"# Webhooks\n\n## Overview\n\nThis is a service plugin that start an http server and register multiple webhook listeners.\n\n```sh\n$ circonus-unified-agent config -input-filter webhooks -output-filter circonus > config.conf.new\n```\n\nChange the config file to point to add Circonus API token key you are using and adjust the settings to match your environment. Once that is complete:\n\n```sh\n$ cp config.conf.new /etc/circonus-unified-agent/circonus-unified-agent.conf\n$ sudo service circonus-unified-agent start\n```\n\n## Configuration\n\n```toml\n[[inputs.webhooks]]\n  ## Address and port to host Webhook listener on\n  service_address = \":1619\"\n\n  [inputs.webhooks.filestack]\n    path = \"/filestack\"\n\n  [inputs.webhooks.github]\n    path = \"/github\"\n    # secret = \"\"\n\n  [inputs.webhooks.mandrill]\n    path = \"/mandrill\"\n\n  [inputs.webhooks.rollbar]\n    path = \"/rollbar\"\n\n  [inputs.webhooks.papertrail]\n    path = \"/papertrail\"\n\n  [inputs.webhooks.particle]\n    path = \"/particle\"\n```\n\n### Available webhooks\n\n- [Filestack](https://github.com/circonus-labs/circonus-unified-agent/blob/master/plugins/inputs/webhooks/filestack)\n- [Github](https://github.com/circonus-labs/circonus-unified-agent/blob/master/plugins/inputs/webhooks/github)\n- [Mandrill](https://github.com/circonus-labs/circonus-unified-agent/blob/master/plugins/inputs/webhooks/mandrill)\n- [Rollbar](https://github.com/circonus-labs/circonus-unified-agent/blob/master/plugins/inputs/webhooks/rollbar)\n- [Papertrail](https://github.com/circonus-labs/circonus-unified-agent/blob/master/plugins/inputs/webhooks/papertrail)\n- [Particle](https://github.com/circonus-labs/circonus-unified-agent/blob/master/plugins/inputs/webhooks/particle)\n\n### Adding new webhooks plugin\n\n1. Add your webhook plugin inside the `webhooks` folder\n1. Your plugin must implement the `Webhook` interface\n1. Import your plugin in the `webhooks.go` file and add it to the `Webhooks` struct\n\nBoth [Github](https://github.com/circonus-labs/circonus-unified-agent/blob/master/plugins/inputs/webhooks/github) and [Rollbar](https://github.com/circonus-labs/circonus-unified-agent/blob/master/plugins/inputs/webhooks/rollbar) are good example to follow.\n","logo":{"light":"/img/library/webhooks.svg","dark":"/img/library/webhooks-dark.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:webhooks"},{"name":"windows-eventlog","title":"Windows Eventlog","content":"# Windows Eventlog\n\n## Overview\n\nCollect Windows Event Log messages\n\nSupports Windows Vista and higher.\n\nAgent should have Administrator permissions to subscribe for some of the Windows Events Channels, like System Log.\n\n## Configuration\n\n```toml\n[[inputs.win_eventlog]]\n  ## Agent should have Administrator permissions to subscribe for some Windows Events channels\n  ## (System log, for example)\n\n  ## LCID (Locale ID) for event rendering\n  ## 1033 to force English language\n  ## 0 to use default Windows locale\n  # locale = 0\n\n  ## Name of eventlog, used only if xpath_query is empty\n  ## Example: \"Application\"\n  # eventlog_name = \"\"\n\n  ## xpath_query can be in defined short form like \"Event/System[EventID=999]\"\n  ## or you can form a XML Query. Refer to the Consuming Events article:\n  ## https://docs.microsoft.com/en-us/windows/win32/wes/consuming-events\n  ## XML query is the recommended form, because it is most flexible\n  ## You can create or debug XML Query by creating Custom View in Windows Event Viewer\n  ## and then copying resulting XML here\n  xpath_query = '''\n  <QueryList>\n    <Query Id=\"0\" Path=\"Security\">\n      <Select Path=\"Security\">*</Select>\n      <Suppress Path=\"Security\">*[System[( (EventID &gt;= 5152 and EventID &lt;= 5158) or EventID=5379 or EventID=4672)]]</Suppress>\n    </Query>\n    <Query Id=\"1\" Path=\"Application\">\n      <Select Path=\"Application\">*[System[(Level &lt; 4)]]</Select>\n    </Query>\n    <Query Id=\"2\" Path=\"Windows PowerShell\">\n      <Select Path=\"Windows PowerShell\">*[System[(Level &lt; 4)]]</Select>\n    </Query>\n    <Query Id=\"3\" Path=\"System\">\n      <Select Path=\"System\">*</Select>\n    </Query>\n    <Query Id=\"4\" Path=\"Setup\">\n      <Select Path=\"Setup\">*</Select>\n    </Query>\n  </QueryList>\n  '''\n\n  ## System field names:\n  ##   \"Source\", \"EventID\", \"Version\", \"Level\", \"Task\", \"Opcode\", \"Keywords\", \"TimeCreated\",\n  ##   \"EventRecordID\", \"ActivityID\", \"RelatedActivityID\", \"ProcessID\", \"ThreadID\", \"ProcessName\",\n  ##   \"Channel\", \"Computer\", \"UserID\", \"UserName\", \"Message\", \"LevelText\", \"TaskText\", \"OpcodeText\"\n\n  ## In addition to System, Data fields can be unrolled from additional XML nodes in event.\n  ## Human-readable representation of those nodes is formatted into event Message field,\n  ## but XML is more machine-parsable\n\n  # Process UserData XML to fields, if this node exists in Event XML\n  process_userdata = true\n\n  # Process EventData XML to fields, if this node exists in Event XML\n  process_eventdata = true\n\n  ## Separator character to use for unrolled XML Data field names\n  separator = \"_\"\n\n  ## Get only first line of Message field. For most events first line is usually more than enough\n  only_first_line_of_message = true\n\n  ## Parse timestamp from TimeCreated.SystemTime event field.\n  ## Will default to current time of processing on parsing error or if set to false\n  timestamp_from_event = true\n\n  ## Fields to include as tags. Globbing supported (\"Level*\" for both \"Level\" and \"LevelText\")\n  event_tags = [\"Source\", \"EventID\", \"Level\", \"LevelText\", \"Task\", \"TaskText\", \"Opcode\", \"OpcodeText\", \"Keywords\", \"Channel\", \"Computer\"]\n\n  ## Default list of fields to send. All fields are sent by default. Globbing supported\n  event_fields = [\"*\"]\n\n  ## Fields to exclude. Also applied to data fields. Globbing supported\n  exclude_fields = [\"TimeCreated\", \"Binary\", \"Data_Address*\"]\n\n  ## Skip those tags or fields if their value is empty or equals to zero. Globbing supported\n  exclude_empty = [\"*ActivityID\", \"UserID\"]\n```\n\n### Filtering\n\nThere are three types of filtering: **Event Log** name, **XPath Query** and **XML Query**.\n\n**Event Log** name filtering is simple:\n\n```toml\n  eventlog_name = \"Application\"\n  xpath_query = '''\n```\n\nFor **XPath Query** filtering set the `xpath_query` value, and `eventlog_name` will be ignored:\n\n```toml\n  eventlog_name = \"\"\n  xpath_query = \"Event/System[EventID=999]\"\n```\n\n**XML Query** is the most flexible: you can Select or Suppress any values, and give ranges for other values. XML query is the recommended form, because it is most flexible. You can create or debug XML Query by creating Custom View in Windows Event Viewer and then copying resulting XML in config file.\n\nXML Query documentation:\n\n<https://docs.microsoft.com/en-us/windows/win32/wes/consuming-events>\n","logo":{"light":"/img/library/windows.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:win_eventlog"},{"name":"windows-services","title":"Windows Services","content":"# Windows Services\n\n## Overview\n\nReports information about Windows service status.\n\nMonitoring some services may require running agent with administrator privileges.\n\n## Configuration\n\n```toml\n[[inputs.win_services]]\n  ## Names of the services to monitor. Leave empty to monitor all the available services on the host\n  service_names = [\n    \"LanmanServer\",\n    \"TermService\",\n  ]\n```\n","logo":{"light":"/img/library/windows.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:win_services"},{"name":"wireguard","title":"Wireguard","content":"# Wireguard\n\n## Overview\n\nThe Wireguard input plugin collects statistics on the local Wireguard server\nusing the [`wgctrl`](https://github.com/WireGuard/wgctrl-go) library. It\nreports gauge metrics for Wireguard interface device(s) and its peers.\n\n## Configuration\n\n```toml\n# Collect Wireguard server interface and peer statistics\n[[inputs.wireguard]]\n  ## Optional list of Wireguard device/interface names to query.\n  ## If omitted, all Wireguard interfaces are queried.\n  # devices = [\"wg0\"]\n```\n\n## Troubleshooting\n\n### Error: `operation not permitted`\n\nWhen the kernelspace implementation of Wireguard is in use (as opposed to its\nuserspace implementations), agent communicates with the module over netlink.\nThis requires agent to either run as root, or for the agent binary to\nhave the `CAP_NET_ADMIN` capability.\n\nTo add this capability to the agent binary (to allow this communication under\nthe default user `cua`):\n\n```bash\n$ sudo setcap CAP_NET_ADMIN+epi $(which circonus-unified-agent)\n```\n\nN.B.: This capability is a filesystem attribute on the binary itself. The\nattribute needs to be re-applied if the agent binary is rotated (e.g.\non installation of new a agent version from the system package manager).\n\n### Error: `error enumerating Wireguard devices`\n\nThis usually happens when the device names specified in config are invalid.\nEnsure that `sudo wg show` succeeds, and that the device names in config match\nthose printed by this command.\n","logo":{"light":"/img/library/wireguard.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:wireguard"},{"name":"wireless","title":"Wireless","content":"# Wireless\n\n## Overview\n\nThe wireless plugin gathers metrics about wireless link quality by reading the `/proc/net/wireless` file. This plugin currently supports linux only.\n\n## Configuration\n\n```toml\n# Monitor wifi signal strength and quality\n[[inputs.wireless]]\n  ## Sets 'proc' directory path\n  ## If not specified, then default is /proc\n  # host_proc = \"/proc\"\n```\n","logo":{"light":"/img/library/wireless.svg","dark":"/img/library/wireless-dark.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:wireless"},{"name":"x509-certificate","title":"x509 Certificate","content":"# x509 Certificate\n\n## Overview\n\nThis plugin provides information about X509 certificate accessible via local\nfile or network connection.\n\n## Configuration\n\n```toml\n# Reads metrics from a SSL certificate\n[[inputs.x509_cert]]\n  ## List certificate sources\n  sources = [\"/etc/ssl/certs/ssl-cert-snakeoil.pem\", \"https://example.org:443\"]\n\n  ## Timeout for SSL connection\n  # timeout = \"5s\"\n\n  ## Pass a different name into the TLS request (Server Name Indication)\n  ##   example: server_name = \"myhost.example.org\"\n  # server_name = \"myhost.example.org\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/circonus-unified-agent/ca.pem\"\n  # tls_cert = \"/etc/circonus-unified-agent/cert.pem\"\n  # tls_key = \"/etc/circonus-unified-agent/key.pem\"\n```\n","logo":{"light":"/img/library/x509-certificate.svg"},"attributes":{"implementation":"cua"},"module":"httptrap:cua:x509_cert"},{"name":"zfs","title":"ZFS","content":"# ZFS\n\n## Overview\n\nThis ZFS plugin provides metrics from your ZFS filesystems. It supports ZFS on\nLinux and FreeBSD. It gets ZFS stat from `/proc/spl/kstat/zfs` on Linux and\nfrom `sysctl`, `zfs`, and `zpool` on FreeBSD.\n\n## Configuration\n\n```toml\n[[inputs.zfs]]\n  ## an instance id is required\n  instance_id  \"\"\n  ## By default, gather zpool stats\n  poolMetrics = true\n\n  # ATTENTION LINUX USERS:\n  # Because circonus-unified-agent normally runs as an unprivileged user, it may not be\n  # able to run \"zpool {status,list}\" without root privileges, due to the\n  # permissions on /dev/zfs.\n  # This was addressed in ZFSonLinux 0.7.0 and later.\n  # See https://github.com/zfsonlinux/zfs/issues/362 for a potential workaround\n  # if your distribution does not support unprivileged access to /dev/zfs.\n\n  ## ZFS kstat path. Ignored on FreeBSD\n  ## If not specified, then default is:\n  # kstatPath = \"/proc/spl/kstat/zfs\"\n\n  ## By default, agent gathers all zfs stats\n  ## Override the stats list using the kstatMetrics array:\n  ## For FreeBSD, the default is:\n  # kstatMetrics = [\"arcstats\", \"zfetchstats\", \"vdev_cache_stats\"]\n  ## For Linux, the default is:\n  # kstatMetrics = [\"abdstats\", \"arcstats\", \"dnodestats\", \"dbufcachestats\",\n  #     \"dmu_tx\", \"fm\", \"vdev_mirror_stats\", \"zfetchstats\", \"zil\"]\n\n  ## By default, don't gather dataset stats\n  # datasetMetrics = false\n```\n","logo":{"light":"/img/library/zfs.svg"},"attributes":{"implementation":"cua"},"tags":["system","filesystem","performance"],"module":"httptrap:cua:zfs"}]